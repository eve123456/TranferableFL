{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e4e19-39e2-4593-85de-00e94803f6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaopan2/miniconda3/envs/graph_unlearn/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import datetime\n",
    "import argparse\n",
    "import warnings\n",
    "import importlib\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from data.finetune.ft_loader import get_loader\n",
    "from src.utils.worker_utils import read_data\n",
    "from src.models.model import choose_model\n",
    "from src.trainers.finetune import ft_train, eval\n",
    "from config import OPTIMIZERS, DATASETS, MODEL_PARAMS, TRAINERS\n",
    "from src.utils.torch_utils import get_flat_grad, get_state_dict, get_flat_params_from, set_flat_params_to\n",
    "from pyhessian import hessian\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c96043-3542-42fc-8901-c42463696794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hessian(model, data_loader, criterion, device, compute_alpha):\n",
    "    # only compute alpha (spectral norm of hessian) when the estimate is unknown\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "    alpha_list = []\n",
    "    for inputs, targets in data_loader:\n",
    "        # we use cuda to make the computation fast\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, targets)\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        total_num += targets.size(0)\n",
    "        \n",
    "        if compute_alpha:\n",
    "            # create the hessian computation module\n",
    "            hessian_comp = hessian(model, criterion, data=(inputs, targets), cuda=True)\n",
    "            top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues()\n",
    "            alpha_list.append(top_eigenvalues[-1])\n",
    "    \n",
    "    total_loss /= total_num\n",
    "    if len(alpha_list) == 0:\n",
    "        alpha = 0.0\n",
    "    else:\n",
    "        alpha = max(alpha_list)\n",
    "    return total_loss, alpha\n",
    "\n",
    "\n",
    "def freeze(model, k):\n",
    "    # only fine-tune the last k fc layers (if there are more than k fc layers)\n",
    "    num_layer = 0\n",
    "    for mod in model.children():\n",
    "        for params in mod.parameters():\n",
    "            params.requires_grad = False\n",
    "        num_layer += 1\n",
    "\n",
    "    for mod in model.children():\n",
    "        num_layer -= 1\n",
    "        if num_layer <= k and isinstance(mod, torch.nn.Linear):\n",
    "            for params in mod.parameters():\n",
    "                params.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087e3519-5c59-42a6-8822-827011ad50e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      ">>> Arguments:\n",
      "\t             algo : fedavgtl\n",
      "\t            alpha : 13.2316427\n",
      "\t       batch_size : 64\n",
      "\tclients_per_round : 100\n",
      "\t          dataset : mnist_all_data_1_equal_niid\n",
      "\t           device : cuda:0\n",
      "\t              dis : \n",
      "\t   early_stopping : 20\n",
      "\t       eval_every : 5\n",
      "\t    ft_batch_size : 256\n",
      "\t       ft_dataset : mnist-m\n",
      "\t        ft_epochs : 30\n",
      "\t            ft_lr : 0.01\n",
      "\t            ft_wd : 0\n",
      "\t              gpu : True\n",
      "\t      input_shape : (1, 28, 28)\n",
      "\t           last_k : 1\n",
      "\t               lr : 0.01\n",
      "\t            model : lenet\n",
      "\t           n_init : 10\n",
      "\t        noaverage : False\n",
      "\t             noft : False\n",
      "\t          noprint : False\n",
      "\t        num_class : 10\n",
      "\t        num_epoch : 1\n",
      "\t        num_round : 200\n",
      "\t           opt_lr : False\n",
      "\t            reg_J : False\n",
      "\t       reg_J_coef : 0\n",
      "\t             seed : 0\n",
      "\t               wd : 0.0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--algo',\n",
    "                    help='name of trainer;',\n",
    "                    type=str,\n",
    "                    choices=OPTIMIZERS,\n",
    "                    default='fedavgtl')\n",
    "parser.add_argument('--dataset',\n",
    "                    help='name of dataset;',\n",
    "                    type=str,\n",
    "                    default='mnist_all_data_1_equal_niid')\n",
    "parser.add_argument('--model',\n",
    "                    help='name of model;',\n",
    "                    type=str,\n",
    "                    default='lenet')\n",
    "parser.add_argument('--wd',\n",
    "                    help='weight decay parameter;',\n",
    "                    type=float,\n",
    "                    default=0.0)\n",
    "parser.add_argument('--lr',\n",
    "                    help='learning rate for inner solver;',\n",
    "                    type=float,\n",
    "                    default=0.01)\n",
    "parser.add_argument('--gpu',\n",
    "                    action='store_true',\n",
    "                    default=True,\n",
    "                    help='use gpu (default: False)')\n",
    "parser.add_argument('--noprint',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='whether to print inner result (default: False)')\n",
    "parser.add_argument('--noaverage',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='whether to only average local solutions (default: False)')\n",
    "parser.add_argument('--device',\n",
    "                    help='selected CUDA device',\n",
    "                    default='0',\n",
    "                    type=str)\n",
    "parser.add_argument('--num_round',\n",
    "                    help='number of rounds to simulate;',\n",
    "                    type=int,\n",
    "                    default=200)\n",
    "parser.add_argument('--eval_every',\n",
    "                    help='evaluate every ____ rounds;',\n",
    "                    type=int,\n",
    "                    default=5)\n",
    "parser.add_argument('--clients_per_round',\n",
    "                    help='number of clients trained per round;',\n",
    "                    type=int,\n",
    "                    default=100)\n",
    "parser.add_argument('--batch_size',\n",
    "                    help='batch size when clients train on data;',\n",
    "                    type=int,\n",
    "                    default=64)\n",
    "parser.add_argument('--num_epoch',\n",
    "                    help='number of epochs when clients train on data;',\n",
    "                    type=int,\n",
    "                    default=1)\n",
    "parser.add_argument('--seed',\n",
    "                    help='seed for randomness;',\n",
    "                    type=int,\n",
    "                    default=0)\n",
    "parser.add_argument('--dis',\n",
    "                    help='add more information;',\n",
    "                    type=str,\n",
    "                    default='')\n",
    "parser.add_argument('--opt_lr',\n",
    "                    help='flag for optimizing local learning rate at each round (default: False);',\n",
    "                    action='store_true',\n",
    "                    default=False)\n",
    "parser.add_argument('--reg_J',\n",
    "                    help='flag for regularizing Jacobian (default: False);',\n",
    "                    action='store_true',\n",
    "                    default=False)\n",
    "parser.add_argument('--reg_J_coef',\n",
    "                    help='coefficient for regularization on Jacobian;',\n",
    "                    type=float,\n",
    "                    default=0)\n",
    "parser.add_argument('--ft_dataset',\n",
    "                    help='dataset for fine-tuning;',\n",
    "                    type=str,\n",
    "                    default='mnist-m')\n",
    "parser.add_argument('--ft_epochs',\n",
    "                    help='epochs for fine-tuning;',\n",
    "                    type=int,\n",
    "                    default=30)\n",
    "parser.add_argument('--ft_batch_size',\n",
    "                    help='batch size for fine-tuning;',\n",
    "                    type=int,\n",
    "                    default=256)\n",
    "parser.add_argument('--ft_lr',\n",
    "                    help='learning rate for fine-tuning;',\n",
    "                    type=float,\n",
    "                    default=0.01)\n",
    "parser.add_argument('--ft_wd',\n",
    "                    help='weight decay for fine-tuning;',\n",
    "                    type=float,\n",
    "                    default=0)\n",
    "parser.add_argument('--n_init',\n",
    "                    help='number of initial models to consider;',\n",
    "                    type=int,\n",
    "                    default=10)\n",
    "parser.add_argument('--alpha',\n",
    "                    help='estimate of lipschitz continuous gradient constant (0 means no estimate yet);',\n",
    "                    type=float,\n",
    "                    default=13.2316427)\n",
    "parser.add_argument('--last_k',\n",
    "                    help='number of fc layers to fine-tune;',\n",
    "                    type=int,\n",
    "                    default=1)\n",
    "parser.add_argument('--early_stopping',\n",
    "                    help='number of epochs for early stopping during training (0 means no early stopping);',\n",
    "                    type=int,\n",
    "                    default=20)\n",
    "parser.add_argument('--noft',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='not performing fine-tuning (default: False);')\n",
    "\n",
    "parsed = parser.parse_args([])\n",
    "options = parsed.__dict__\n",
    "options['gpu'] = options['gpu'] and torch.cuda.is_available()\n",
    "\n",
    "if not options['gpu']:\n",
    "    options['device'] = 'cpu'\n",
    "else:\n",
    "    options['device'] = 'cuda:' + options['device']\n",
    "print('Using device: ' + options['device'])\n",
    "\n",
    "# read data\n",
    "idx = options['dataset'].find(\"_\")\n",
    "if idx != -1:\n",
    "    dataset_name, sub_data = options['dataset'][:idx], options['dataset'][idx+1:]\n",
    "else:\n",
    "    dataset_name, sub_data = options['dataset'], None\n",
    "assert dataset_name in DATASETS, \"{} not in dataset {}!\".format(dataset_name, DATASETS)\n",
    "\n",
    "# Add model arguments\n",
    "options.update(MODEL_PARAMS(dataset_name, options['model']))\n",
    "\n",
    "# Load selected trainer\n",
    "trainer_path = 'src.trainers.%s' % options['algo']\n",
    "mod = importlib.import_module(trainer_path)\n",
    "trainer_class = getattr(mod, TRAINERS[options['algo']])\n",
    "\n",
    "# Print arguments and return\n",
    "max_length = max([len(key) for key in options.keys()])\n",
    "fmt_string = '\\t%' + str(max_length) + 's : %s'\n",
    "print('>>> Arguments:')\n",
    "for keyPair in sorted(options.items()):\n",
    "    print(fmt_string % keyPair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a75c4d-2113-4a25-9f01-3844e450a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid: 20230914223349\n",
      ">>> Read data from:\n",
      "     ./data/mnist/data/all_data_1.pkl\n",
      ">>> The estimate of constant alpha is 13.2316427.\n"
     ]
    }
   ],
   "source": [
    "uid = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "print(f'uid: {uid}')\n",
    "\n",
    "# first estimate the value for alpha\n",
    "data_all_path = os.path.join('./data', dataset_name, 'data')\n",
    "_, _, data_all, _ = read_data(train_data_dir=data_all_path, key='all_data_1.pkl')\n",
    "data_all = data_all[0]\n",
    "data_all_loader = DataLoader(data_all, batch_size=1024, shuffle=False)\n",
    "\n",
    "# initialize several models to compute the lowest initial loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "best_alpha = 0 if options['alpha'] == 0 else options['alpha']\n",
    "compute_alpha = True if options['alpha'] == 0 else False\n",
    "start = time.time()\n",
    "\n",
    "for i_init in range(options['n_init']):\n",
    "    # randomly initialize a new model\n",
    "    random_model = choose_model(options)\n",
    "    i_loss, i_alpha = eval_hessian(random_model, data_all_loader, criterion, options['device'], compute_alpha)\n",
    "    # save the best model so far\n",
    "    if i_loss < best_loss:\n",
    "        best_loss = i_loss\n",
    "        best_model = random_model\n",
    "\n",
    "    if compute_alpha:\n",
    "        best_alpha = max(best_alpha, i_alpha)\n",
    "\n",
    "best_model_init = get_flat_params_from(best_model).detach()\n",
    "options['alpha'] = best_alpha\n",
    "options['model_init'] = best_model_init\n",
    "print(f'>>> The estimate of constant alpha is {best_alpha}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42633aa8-2e52-4f89-8cb6-5a331323c40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Read data from:\n",
      "     ./data/mnist/data/train/all_data_1_equal_niid.pkl\n",
      "     ./data/mnist/data/test/all_data_1_equal_niid.pkl\n",
      "FL pretrained model will be saved at ./models/lenet_mnist_20230914223349.pt\n",
      ">>> Use gpu on device cuda:0\n",
      ">>> Model statistic per layer\n",
      "LeNet_MNIST(\n",
      "  286.12 KMac, 100.000% MACs, \n",
      "  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)\n",
      ")\n",
      ">>> Activate a worker for training\n",
      ">>> Initialize 100 clients in total\n",
      ">>> Weigh updates by simple average\n",
      ">>> Select 100 clients per round \n",
      "\n",
      "\n",
      ">>> Round:    0 / Acc: 11.836% / Loss: 2.3005 / Grad Norm: 0.1447 / Grad Diff: 1.2042 / Time: 7.73s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 0 / acc: 11.080% / loss: 2.3020 / Time: 0.62s\n",
      "======================================================================================================\n",
      "\n",
      "round 0 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    1 / Acc: 12.482% / Loss: 2.2982 / Grad Norm: 0.1542 / Grad Diff: 1.3587 / Time: 7.74s\n",
      "======================================================================================================\n",
      "\n",
      "round 1 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    2 / Acc: 13.341% / Loss: 2.2955 / Grad Norm: 0.1664 / Grad Diff: 1.5848 / Time: 7.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 2 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    3 / Acc: 14.244% / Loss: 2.2920 / Grad Norm: 0.1809 / Grad Diff: 1.9314 / Time: 7.58s\n",
      "======================================================================================================\n",
      "\n",
      "round 3 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    4 / Acc: 15.319% / Loss: 2.2876 / Grad Norm: 0.1985 / Grad Diff: 2.4949 / Time: 7.57s\n",
      "======================================================================================================\n",
      "\n",
      "round 4 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    5 / Acc: 17.863% / Loss: 2.2820 / Grad Norm: 0.2231 / Grad Diff: 3.4898 / Time: 7.55s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 5 / acc: 17.720% / loss: 2.2837 / Time: 0.65s\n",
      "======================================================================================================\n",
      "\n",
      "round 5 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    6 / Acc: 23.218% / Loss: 2.2743 / Grad Norm: 0.2618 / Grad Diff: 5.5135 / Time: 7.00s\n",
      "======================================================================================================\n",
      "\n",
      "round 6 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    7 / Acc: 22.620% / Loss: 2.2635 / Grad Norm: 0.3271 / Grad Diff: 10.6025 / Time: 7.43s\n",
      "======================================================================================================\n",
      "\n",
      "round 7 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    8 / Acc: 19.120% / Loss: 2.2482 / Grad Norm: 0.4608 / Grad Diff: 25.8991 / Time: 7.27s\n",
      "======================================================================================================\n",
      "\n",
      "round 8 local learning rate = 0.01\n",
      "\n",
      ">>> Round:    9 / Acc: 24.886% / Loss: 2.2234 / Grad Norm: 0.6504 / Grad Diff: 65.0786 / Time: 8.62s\n",
      "======================================================================================================\n",
      "\n",
      "round 9 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   10 / Acc: 28.989% / Loss: 2.1874 / Grad Norm: 0.7712 / Grad Diff: 126.9677 / Time: 7.47s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 10 / acc: 28.410% / loss: 2.1895 / Time: 0.65s\n",
      "======================================================================================================\n",
      "\n",
      "round 10 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   11 / Acc: 40.085% / Loss: 2.1511 / Grad Norm: 0.9913 / Grad Diff: 179.3085 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 11 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   12 / Acc: 47.365% / Loss: 2.1162 / Grad Norm: 1.0441 / Grad Diff: 183.8620 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 12 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   13 / Acc: 50.172% / Loss: 2.0763 / Grad Norm: 1.0288 / Grad Diff: 187.0170 / Time: 7.21s\n",
      "======================================================================================================\n",
      "\n",
      "round 13 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   14 / Acc: 52.332% / Loss: 2.0364 / Grad Norm: 1.1603 / Grad Diff: 185.3930 / Time: 7.38s\n",
      "======================================================================================================\n",
      "\n",
      "round 14 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   15 / Acc: 55.397% / Loss: 1.9877 / Grad Norm: 1.1593 / Grad Diff: 188.6294 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 15 / acc: 55.870% / loss: 1.9786 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 15 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   16 / Acc: 57.004% / Loss: 1.9398 / Grad Norm: 1.2340 / Grad Diff: 187.0304 / Time: 8.69s\n",
      "======================================================================================================\n",
      "\n",
      "round 16 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   17 / Acc: 55.788% / Loss: 1.8861 / Grad Norm: 1.2540 / Grad Diff: 185.7068 / Time: 7.53s\n",
      "======================================================================================================\n",
      "\n",
      "round 17 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   18 / Acc: 57.749% / Loss: 1.8276 / Grad Norm: 1.2434 / Grad Diff: 187.5544 / Time: 7.77s\n",
      "======================================================================================================\n",
      "\n",
      "round 18 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   19 / Acc: 58.559% / Loss: 1.7698 / Grad Norm: 1.2967 / Grad Diff: 187.6111 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "round 19 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   20 / Acc: 59.268% / Loss: 1.7107 / Grad Norm: 1.2985 / Grad Diff: 184.6902 / Time: 6.93s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 20 / acc: 60.380% / loss: 1.6909 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 20 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   21 / Acc: 60.708% / Loss: 1.6518 / Grad Norm: 1.3433 / Grad Diff: 182.1886 / Time: 7.56s\n",
      "======================================================================================================\n",
      "\n",
      "round 21 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   22 / Acc: 61.321% / Loss: 1.5932 / Grad Norm: 1.3667 / Grad Diff: 179.0168 / Time: 6.90s\n",
      "======================================================================================================\n",
      "\n",
      "round 22 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   23 / Acc: 62.900% / Loss: 1.5318 / Grad Norm: 1.3704 / Grad Diff: 181.1075 / Time: 6.92s\n",
      "======================================================================================================\n",
      "\n",
      "round 23 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   24 / Acc: 64.478% / Loss: 1.4769 / Grad Norm: 1.4526 / Grad Diff: 177.0381 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 24 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   25 / Acc: 64.780% / Loss: 1.4200 / Grad Norm: 1.3968 / Grad Diff: 176.5995 / Time: 7.58s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 25 / acc: 66.070% / loss: 1.3910 / Time: 0.62s\n",
      "======================================================================================================\n",
      "\n",
      "round 25 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   26 / Acc: 66.266% / Loss: 1.3670 / Grad Norm: 1.3995 / Grad Diff: 175.7060 / Time: 7.56s\n",
      "======================================================================================================\n",
      "\n",
      "round 26 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   27 / Acc: 66.635% / Loss: 1.3157 / Grad Norm: 1.3897 / Grad Diff: 174.7910 / Time: 7.47s\n",
      "======================================================================================================\n",
      "\n",
      "round 27 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   28 / Acc: 67.766% / Loss: 1.2667 / Grad Norm: 1.3881 / Grad Diff: 174.2437 / Time: 7.49s\n",
      "======================================================================================================\n",
      "\n",
      "round 28 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   29 / Acc: 67.378% / Loss: 1.2248 / Grad Norm: 1.4827 / Grad Diff: 171.2674 / Time: 7.40s\n",
      "======================================================================================================\n",
      "\n",
      "round 29 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   30 / Acc: 68.793% / Loss: 1.1813 / Grad Norm: 1.4218 / Grad Diff: 169.6839 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 30 / acc: 70.100% / loss: 1.1489 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 30 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   31 / Acc: 69.742% / Loss: 1.1408 / Grad Norm: 1.3779 / Grad Diff: 167.3465 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 31 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   32 / Acc: 70.664% / Loss: 1.1004 / Grad Norm: 1.2832 / Grad Diff: 167.4952 / Time: 7.39s\n",
      "======================================================================================================\n",
      "\n",
      "round 32 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   33 / Acc: 71.506% / Loss: 1.0661 / Grad Norm: 1.3093 / Grad Diff: 166.6160 / Time: 7.52s\n",
      "======================================================================================================\n",
      "\n",
      "round 33 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   34 / Acc: 72.101% / Loss: 1.0346 / Grad Norm: 1.2757 / Grad Diff: 162.9997 / Time: 7.59s\n",
      "======================================================================================================\n",
      "\n",
      "round 34 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   35 / Acc: 72.494% / Loss: 1.0046 / Grad Norm: 1.2816 / Grad Diff: 161.0064 / Time: 7.50s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 35 / acc: 73.760% / loss: 0.9718 / Time: 0.63s\n",
      "======================================================================================================\n",
      "\n",
      "round 35 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   36 / Acc: 72.677% / Loss: 0.9769 / Grad Norm: 1.2973 / Grad Diff: 158.9353 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 36 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   37 / Acc: 73.393% / Loss: 0.9509 / Grad Norm: 1.2252 / Grad Diff: 155.4278 / Time: 7.65s\n",
      "======================================================================================================\n",
      "\n",
      "round 37 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   38 / Acc: 74.011% / Loss: 0.9262 / Grad Norm: 1.1658 / Grad Diff: 153.0391 / Time: 8.63s\n",
      "======================================================================================================\n",
      "\n",
      "round 38 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   39 / Acc: 74.629% / Loss: 0.9030 / Grad Norm: 1.1494 / Grad Diff: 151.2698 / Time: 7.50s\n",
      "======================================================================================================\n",
      "\n",
      "round 39 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   40 / Acc: 74.974% / Loss: 0.8812 / Grad Norm: 1.1761 / Grad Diff: 150.1635 / Time: 7.49s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 40 / acc: 76.150% / loss: 0.8476 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 40 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   41 / Acc: 75.685% / Loss: 0.8629 / Grad Norm: 1.1206 / Grad Diff: 144.6888 / Time: 7.78s\n",
      "======================================================================================================\n",
      "\n",
      "round 41 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   42 / Acc: 76.072% / Loss: 0.8426 / Grad Norm: 1.0558 / Grad Diff: 143.0262 / Time: 8.00s\n",
      "======================================================================================================\n",
      "\n",
      "round 42 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   43 / Acc: 76.232% / Loss: 0.8257 / Grad Norm: 1.1049 / Grad Diff: 140.8700 / Time: 7.51s\n",
      "======================================================================================================\n",
      "\n",
      "round 43 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   44 / Acc: 76.849% / Loss: 0.8073 / Grad Norm: 1.0039 / Grad Diff: 139.0099 / Time: 7.79s\n",
      "======================================================================================================\n",
      "\n",
      "round 44 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   45 / Acc: 77.052% / Loss: 0.7928 / Grad Norm: 1.1054 / Grad Diff: 137.0099 / Time: 7.66s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 45 / acc: 78.330% / loss: 0.7592 / Time: 0.64s\n",
      "======================================================================================================\n",
      "\n",
      "round 45 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   46 / Acc: 77.367% / Loss: 0.7783 / Grad Norm: 1.0840 / Grad Diff: 134.9142 / Time: 7.29s\n",
      "======================================================================================================\n",
      "\n",
      "round 46 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   47 / Acc: 77.662% / Loss: 0.7643 / Grad Norm: 1.0557 / Grad Diff: 132.1761 / Time: 7.59s\n",
      "======================================================================================================\n",
      "\n",
      "round 47 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   48 / Acc: 77.834% / Loss: 0.7505 / Grad Norm: 1.0109 / Grad Diff: 129.8933 / Time: 7.49s\n",
      "======================================================================================================\n",
      "\n",
      "round 48 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   49 / Acc: 78.188% / Loss: 0.7366 / Grad Norm: 0.9320 / Grad Diff: 127.7726 / Time: 7.36s\n",
      "======================================================================================================\n",
      "\n",
      "round 49 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   50 / Acc: 78.677% / Loss: 0.7235 / Grad Norm: 0.9531 / Grad Diff: 127.5221 / Time: 7.55s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 50 / acc: 80.010% / loss: 0.6901 / Time: 0.65s\n",
      "======================================================================================================\n",
      "\n",
      "round 50 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   51 / Acc: 79.002% / Loss: 0.7123 / Grad Norm: 0.9707 / Grad Diff: 125.4203 / Time: 7.52s\n",
      "======================================================================================================\n",
      "\n",
      "round 51 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   52 / Acc: 79.129% / Loss: 0.7022 / Grad Norm: 0.9303 / Grad Diff: 121.8712 / Time: 7.90s\n",
      "======================================================================================================\n",
      "\n",
      "round 52 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   53 / Acc: 79.487% / Loss: 0.6910 / Grad Norm: 0.8779 / Grad Diff: 119.5717 / Time: 7.63s\n",
      "======================================================================================================\n",
      "\n",
      "round 53 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   54 / Acc: 80.092% / Loss: 0.6795 / Grad Norm: 0.8032 / Grad Diff: 118.5877 / Time: 7.63s\n",
      "======================================================================================================\n",
      "\n",
      "round 54 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   55 / Acc: 79.987% / Loss: 0.6729 / Grad Norm: 0.8610 / Grad Diff: 114.0861 / Time: 8.90s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 55 / acc: 81.490% / loss: 0.6390 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 55 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   56 / Acc: 80.288% / Loss: 0.6629 / Grad Norm: 0.8429 / Grad Diff: 112.7873 / Time: 7.51s\n",
      "======================================================================================================\n",
      "\n",
      "round 56 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   57 / Acc: 80.611% / Loss: 0.6532 / Grad Norm: 0.8390 / Grad Diff: 111.5965 / Time: 7.55s\n",
      "======================================================================================================\n",
      "\n",
      "round 57 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   58 / Acc: 80.838% / Loss: 0.6442 / Grad Norm: 0.8221 / Grad Diff: 110.6576 / Time: 7.52s\n",
      "======================================================================================================\n",
      "\n",
      "round 58 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   59 / Acc: 80.941% / Loss: 0.6349 / Grad Norm: 0.7975 / Grad Diff: 109.8786 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 59 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   60 / Acc: 81.218% / Loss: 0.6275 / Grad Norm: 0.7504 / Grad Diff: 107.1485 / Time: 7.56s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 60 / acc: 82.640% / loss: 0.5935 / Time: 0.59s\n",
      "======================================================================================================\n",
      "\n",
      "round 60 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   61 / Acc: 81.380% / Loss: 0.6197 / Grad Norm: 0.7552 / Grad Diff: 105.8246 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "round 61 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   62 / Acc: 81.797% / Loss: 0.6114 / Grad Norm: 0.7058 / Grad Diff: 104.8026 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "round 62 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   63 / Acc: 81.906% / Loss: 0.6037 / Grad Norm: 0.7317 / Grad Diff: 104.3648 / Time: 7.39s\n",
      "======================================================================================================\n",
      "\n",
      "round 63 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   64 / Acc: 82.000% / Loss: 0.5976 / Grad Norm: 0.7720 / Grad Diff: 102.6408 / Time: 7.47s\n",
      "======================================================================================================\n",
      "\n",
      "round 64 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   65 / Acc: 82.037% / Loss: 0.5908 / Grad Norm: 0.8576 / Grad Diff: 102.6141 / Time: 7.54s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 65 / acc: 83.530% / loss: 0.5575 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 65 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   66 / Acc: 82.227% / Loss: 0.5848 / Grad Norm: 0.8331 / Grad Diff: 100.5143 / Time: 7.38s\n",
      "======================================================================================================\n",
      "\n",
      "round 66 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   67 / Acc: 82.790% / Loss: 0.5785 / Grad Norm: 0.7162 / Grad Diff: 98.1445 / Time: 7.56s\n",
      "======================================================================================================\n",
      "\n",
      "round 67 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   68 / Acc: 82.900% / Loss: 0.5717 / Grad Norm: 0.7050 / Grad Diff: 97.8165 / Time: 7.32s\n",
      "======================================================================================================\n",
      "\n",
      "round 68 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   69 / Acc: 83.124% / Loss: 0.5659 / Grad Norm: 0.7152 / Grad Diff: 96.8562 / Time: 7.52s\n",
      "======================================================================================================\n",
      "\n",
      "round 69 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   70 / Acc: 83.207% / Loss: 0.5599 / Grad Norm: 0.7217 / Grad Diff: 96.1143 / Time: 6.81s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 70 / acc: 84.470% / loss: 0.5272 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 70 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   71 / Acc: 83.406% / Loss: 0.5551 / Grad Norm: 0.7101 / Grad Diff: 94.3206 / Time: 7.31s\n",
      "======================================================================================================\n",
      "\n",
      "round 71 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   72 / Acc: 83.541% / Loss: 0.5506 / Grad Norm: 0.7151 / Grad Diff: 92.1352 / Time: 7.18s\n",
      "======================================================================================================\n",
      "\n",
      "round 72 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   73 / Acc: 83.681% / Loss: 0.5450 / Grad Norm: 0.6793 / Grad Diff: 90.6182 / Time: 7.65s\n",
      "======================================================================================================\n",
      "\n",
      "round 73 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   74 / Acc: 83.941% / Loss: 0.5390 / Grad Norm: 0.6440 / Grad Diff: 90.1911 / Time: 7.22s\n",
      "======================================================================================================\n",
      "\n",
      "round 74 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   75 / Acc: 84.205% / Loss: 0.5339 / Grad Norm: 0.6037 / Grad Diff: 89.2572 / Time: 7.58s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 75 / acc: 85.330% / loss: 0.5014 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 75 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   76 / Acc: 84.236% / Loss: 0.5307 / Grad Norm: 0.6396 / Grad Diff: 86.6230 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 76 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   77 / Acc: 84.277% / Loss: 0.5264 / Grad Norm: 0.6782 / Grad Diff: 85.7074 / Time: 7.47s\n",
      "======================================================================================================\n",
      "\n",
      "round 77 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   78 / Acc: 84.454% / Loss: 0.5213 / Grad Norm: 0.6429 / Grad Diff: 85.0949 / Time: 7.56s\n",
      "======================================================================================================\n",
      "\n",
      "round 78 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   79 / Acc: 84.723% / Loss: 0.5153 / Grad Norm: 0.5719 / Grad Diff: 85.3807 / Time: 7.49s\n",
      "======================================================================================================\n",
      "\n",
      "round 79 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   80 / Acc: 84.795% / Loss: 0.5119 / Grad Norm: 0.6431 / Grad Diff: 84.3777 / Time: 7.59s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 80 / acc: 85.920% / loss: 0.4799 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 80 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   81 / Acc: 84.923% / Loss: 0.5066 / Grad Norm: 0.5822 / Grad Diff: 84.1175 / Time: 7.49s\n",
      "======================================================================================================\n",
      "\n",
      "round 81 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   82 / Acc: 85.096% / Loss: 0.5025 / Grad Norm: 0.5563 / Grad Diff: 82.9925 / Time: 7.16s\n",
      "======================================================================================================\n",
      "\n",
      "round 82 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   83 / Acc: 85.144% / Loss: 0.4985 / Grad Norm: 0.5939 / Grad Diff: 82.5719 / Time: 7.35s\n",
      "======================================================================================================\n",
      "\n",
      "round 83 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   84 / Acc: 85.240% / Loss: 0.4962 / Grad Norm: 0.7420 / Grad Diff: 81.8927 / Time: 7.49s\n",
      "======================================================================================================\n",
      "\n",
      "round 84 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   85 / Acc: 85.371% / Loss: 0.4923 / Grad Norm: 0.6652 / Grad Diff: 79.9742 / Time: 7.52s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 85 / acc: 86.500% / loss: 0.4607 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 85 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   86 / Acc: 85.504% / Loss: 0.4882 / Grad Norm: 0.6512 / Grad Diff: 79.2931 / Time: 8.00s\n",
      "======================================================================================================\n",
      "\n",
      "round 86 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   87 / Acc: 85.568% / Loss: 0.4851 / Grad Norm: 0.6629 / Grad Diff: 77.9904 / Time: 7.43s\n",
      "======================================================================================================\n",
      "\n",
      "round 87 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   88 / Acc: 85.705% / Loss: 0.4805 / Grad Norm: 0.5890 / Grad Diff: 77.3599 / Time: 7.46s\n",
      "======================================================================================================\n",
      "\n",
      "round 88 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   89 / Acc: 85.897% / Loss: 0.4768 / Grad Norm: 0.5928 / Grad Diff: 77.1110 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 89 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   90 / Acc: 85.969% / Loss: 0.4730 / Grad Norm: 0.6119 / Grad Diff: 77.0766 / Time: 7.51s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 90 / acc: 87.190% / loss: 0.4418 / Time: 0.63s\n",
      "======================================================================================================\n",
      "\n",
      "round 90 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   91 / Acc: 86.057% / Loss: 0.4695 / Grad Norm: 0.5987 / Grad Diff: 76.2496 / Time: 7.57s\n",
      "======================================================================================================\n",
      "\n",
      "round 91 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   92 / Acc: 86.192% / Loss: 0.4652 / Grad Norm: 0.5751 / Grad Diff: 76.4063 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "round 92 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   93 / Acc: 86.275% / Loss: 0.4614 / Grad Norm: 0.5379 / Grad Diff: 76.0183 / Time: 7.43s\n",
      "======================================================================================================\n",
      "\n",
      "round 93 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   94 / Acc: 86.369% / Loss: 0.4591 / Grad Norm: 0.5896 / Grad Diff: 75.0162 / Time: 7.55s\n",
      "======================================================================================================\n",
      "\n",
      "round 94 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   95 / Acc: 86.441% / Loss: 0.4559 / Grad Norm: 0.5329 / Grad Diff: 73.8076 / Time: 8.62s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 95 / acc: 87.700% / loss: 0.4244 / Time: 0.59s\n",
      "======================================================================================================\n",
      "\n",
      "round 95 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   96 / Acc: 86.504% / Loss: 0.4531 / Grad Norm: 0.5660 / Grad Diff: 73.3537 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 96 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   97 / Acc: 86.611% / Loss: 0.4500 / Grad Norm: 0.5307 / Grad Diff: 72.1469 / Time: 7.56s\n",
      "======================================================================================================\n",
      "\n",
      "round 97 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   98 / Acc: 86.721% / Loss: 0.4469 / Grad Norm: 0.5585 / Grad Diff: 72.0094 / Time: 8.98s\n",
      "======================================================================================================\n",
      "\n",
      "round 98 local learning rate = 0.01\n",
      "\n",
      ">>> Round:   99 / Acc: 86.777% / Loss: 0.4442 / Grad Norm: 0.5189 / Grad Diff: 70.5989 / Time: 7.78s\n",
      "======================================================================================================\n",
      "\n",
      "round 99 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  100 / Acc: 86.849% / Loss: 0.4425 / Grad Norm: 0.5842 / Grad Diff: 69.3984 / Time: 7.55s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 100 / acc: 87.990% / loss: 0.4116 / Time: 0.62s\n",
      "======================================================================================================\n",
      "\n",
      "round 100 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  101 / Acc: 86.939% / Loss: 0.4388 / Grad Norm: 0.5481 / Grad Diff: 69.2415 / Time: 8.03s\n",
      "======================================================================================================\n",
      "\n",
      "round 101 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  102 / Acc: 87.035% / Loss: 0.4360 / Grad Norm: 0.5414 / Grad Diff: 68.6416 / Time: 7.59s\n",
      "======================================================================================================\n",
      "\n",
      "round 102 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  103 / Acc: 87.135% / Loss: 0.4326 / Grad Norm: 0.5096 / Grad Diff: 68.5302 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "round 103 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  104 / Acc: 87.234% / Loss: 0.4293 / Grad Norm: 0.5057 / Grad Diff: 68.7044 / Time: 7.46s\n",
      "======================================================================================================\n",
      "\n",
      "round 104 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  105 / Acc: 87.244% / Loss: 0.4274 / Grad Norm: 0.5177 / Grad Diff: 67.4257 / Time: 7.27s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 105 / acc: 88.440% / loss: 0.3971 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 105 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  106 / Acc: 87.395% / Loss: 0.4241 / Grad Norm: 0.4827 / Grad Diff: 67.4244 / Time: 7.98s\n",
      "======================================================================================================\n",
      "\n",
      "round 106 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  107 / Acc: 87.413% / Loss: 0.4222 / Grad Norm: 0.5103 / Grad Diff: 66.3388 / Time: 7.51s\n",
      "======================================================================================================\n",
      "\n",
      "round 107 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  108 / Acc: 87.430% / Loss: 0.4208 / Grad Norm: 0.5189 / Grad Diff: 64.3791 / Time: 7.51s\n",
      "======================================================================================================\n",
      "\n",
      "round 108 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  109 / Acc: 87.487% / Loss: 0.4182 / Grad Norm: 0.5340 / Grad Diff: 63.7764 / Time: 8.90s\n",
      "======================================================================================================\n",
      "\n",
      "round 109 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  110 / Acc: 87.625% / Loss: 0.4150 / Grad Norm: 0.4405 / Grad Diff: 63.2120 / Time: 7.53s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 110 / acc: 88.930% / loss: 0.3849 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 110 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  111 / Acc: 87.701% / Loss: 0.4125 / Grad Norm: 0.4833 / Grad Diff: 63.2727 / Time: 7.64s\n",
      "======================================================================================================\n",
      "\n",
      "round 111 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  112 / Acc: 87.792% / Loss: 0.4098 / Grad Norm: 0.4676 / Grad Diff: 63.0096 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 112 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  113 / Acc: 87.836% / Loss: 0.4078 / Grad Norm: 0.5210 / Grad Diff: 62.6595 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 113 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  114 / Acc: 87.926% / Loss: 0.4054 / Grad Norm: 0.4730 / Grad Diff: 61.8490 / Time: 7.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 114 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  115 / Acc: 87.998% / Loss: 0.4034 / Grad Norm: 0.4721 / Grad Diff: 61.0638 / Time: 7.75s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 115 / acc: 89.160% / loss: 0.3739 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 115 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  116 / Acc: 87.991% / Loss: 0.4017 / Grad Norm: 0.5496 / Grad Diff: 60.5602 / Time: 7.88s\n",
      "======================================================================================================\n",
      "\n",
      "round 116 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  117 / Acc: 88.116% / Loss: 0.3989 / Grad Norm: 0.4334 / Grad Diff: 59.9745 / Time: 7.47s\n",
      "======================================================================================================\n",
      "\n",
      "round 117 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  118 / Acc: 88.188% / Loss: 0.3974 / Grad Norm: 0.4596 / Grad Diff: 59.1072 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 118 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  119 / Acc: 88.220% / Loss: 0.3958 / Grad Norm: 0.5060 / Grad Diff: 58.5155 / Time: 7.32s\n",
      "======================================================================================================\n",
      "\n",
      "round 119 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  120 / Acc: 88.306% / Loss: 0.3932 / Grad Norm: 0.4367 / Grad Diff: 57.9506 / Time: 7.39s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 120 / acc: 89.390% / loss: 0.3641 / Time: 0.60s\n",
      "======================================================================================================\n",
      "\n",
      "round 120 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  121 / Acc: 88.303% / Loss: 0.3913 / Grad Norm: 0.5216 / Grad Diff: 57.8820 / Time: 7.32s\n",
      "======================================================================================================\n",
      "\n",
      "round 121 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  122 / Acc: 88.373% / Loss: 0.3887 / Grad Norm: 0.5215 / Grad Diff: 58.0981 / Time: 7.71s\n",
      "======================================================================================================\n",
      "\n",
      "round 122 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  123 / Acc: 88.443% / Loss: 0.3867 / Grad Norm: 0.4803 / Grad Diff: 57.0809 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "round 123 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  124 / Acc: 88.533% / Loss: 0.3841 / Grad Norm: 0.4369 / Grad Diff: 56.7475 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 124 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  125 / Acc: 88.633% / Loss: 0.3819 / Grad Norm: 0.4280 / Grad Diff: 56.3896 / Time: 7.30s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 125 / acc: 89.680% / loss: 0.3532 / Time: 0.62s\n",
      "======================================================================================================\n",
      "\n",
      "round 125 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  126 / Acc: 88.642% / Loss: 0.3797 / Grad Norm: 0.4592 / Grad Diff: 56.4747 / Time: 8.05s\n",
      "======================================================================================================\n",
      "\n",
      "round 126 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  127 / Acc: 88.685% / Loss: 0.3774 / Grad Norm: 0.4422 / Grad Diff: 56.2908 / Time: 7.50s\n",
      "======================================================================================================\n",
      "\n",
      "round 127 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  128 / Acc: 88.766% / Loss: 0.3761 / Grad Norm: 0.4465 / Grad Diff: 55.1282 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 128 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  129 / Acc: 88.819% / Loss: 0.3737 / Grad Norm: 0.4273 / Grad Diff: 55.0546 / Time: 8.35s\n",
      "======================================================================================================\n",
      "\n",
      "round 129 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  130 / Acc: 88.887% / Loss: 0.3711 / Grad Norm: 0.4191 / Grad Diff: 55.3053 / Time: 7.78s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 130 / acc: 89.970% / loss: 0.3428 / Time: 0.59s\n",
      "======================================================================================================\n",
      "\n",
      "round 130 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  131 / Acc: 88.913% / Loss: 0.3694 / Grad Norm: 0.4298 / Grad Diff: 55.0536 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "round 131 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  132 / Acc: 88.983% / Loss: 0.3675 / Grad Norm: 0.4301 / Grad Diff: 54.5590 / Time: 7.94s\n",
      "======================================================================================================\n",
      "\n",
      "round 132 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  133 / Acc: 89.006% / Loss: 0.3664 / Grad Norm: 0.4349 / Grad Diff: 53.5774 / Time: 7.84s\n",
      "======================================================================================================\n",
      "\n",
      "round 133 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  134 / Acc: 89.063% / Loss: 0.3646 / Grad Norm: 0.4302 / Grad Diff: 53.2239 / Time: 7.65s\n",
      "======================================================================================================\n",
      "\n",
      "round 134 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  135 / Acc: 89.105% / Loss: 0.3631 / Grad Norm: 0.4179 / Grad Diff: 52.4392 / Time: 7.67s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 135 / acc: 90.260% / loss: 0.3351 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 135 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  136 / Acc: 89.153% / Loss: 0.3611 / Grad Norm: 0.4257 / Grad Diff: 52.3647 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 136 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  137 / Acc: 89.212% / Loss: 0.3598 / Grad Norm: 0.4161 / Grad Diff: 51.4863 / Time: 7.49s\n",
      "======================================================================================================\n",
      "\n",
      "round 137 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  138 / Acc: 89.256% / Loss: 0.3578 / Grad Norm: 0.4325 / Grad Diff: 51.4498 / Time: 7.46s\n",
      "======================================================================================================\n",
      "\n",
      "round 138 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  139 / Acc: 89.295% / Loss: 0.3562 / Grad Norm: 0.4286 / Grad Diff: 51.1514 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 139 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  140 / Acc: 89.299% / Loss: 0.3551 / Grad Norm: 0.4667 / Grad Diff: 50.4626 / Time: 7.54s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 140 / acc: 90.500% / loss: 0.3274 / Time: 0.80s\n",
      "======================================================================================================\n",
      "\n",
      "round 140 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  141 / Acc: 89.371% / Loss: 0.3534 / Grad Norm: 0.5026 / Grad Diff: 50.5531 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 141 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  142 / Acc: 89.446% / Loss: 0.3508 / Grad Norm: 0.4246 / Grad Diff: 50.3837 / Time: 8.33s\n",
      "======================================================================================================\n",
      "\n",
      "round 142 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  143 / Acc: 89.522% / Loss: 0.3487 / Grad Norm: 0.3887 / Grad Diff: 50.1496 / Time: 7.40s\n",
      "======================================================================================================\n",
      "\n",
      "round 143 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  144 / Acc: 89.520% / Loss: 0.3481 / Grad Norm: 0.4784 / Grad Diff: 49.4978 / Time: 7.09s\n",
      "======================================================================================================\n",
      "\n",
      "round 144 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  145 / Acc: 89.577% / Loss: 0.3459 / Grad Norm: 0.4248 / Grad Diff: 49.3336 / Time: 7.46s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 145 / acc: 90.870% / loss: 0.3191 / Time: 0.63s\n",
      "======================================================================================================\n",
      "\n",
      "round 145 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  146 / Acc: 89.635% / Loss: 0.3438 / Grad Norm: 0.4050 / Grad Diff: 49.2745 / Time: 7.65s\n",
      "======================================================================================================\n",
      "\n",
      "round 146 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  147 / Acc: 89.707% / Loss: 0.3416 / Grad Norm: 0.3522 / Grad Diff: 49.1160 / Time: 7.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 147 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  148 / Acc: 89.749% / Loss: 0.3400 / Grad Norm: 0.3500 / Grad Diff: 48.9927 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 148 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  149 / Acc: 89.773% / Loss: 0.3390 / Grad Norm: 0.4330 / Grad Diff: 48.6376 / Time: 8.06s\n",
      "======================================================================================================\n",
      "\n",
      "round 149 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  150 / Acc: 89.832% / Loss: 0.3371 / Grad Norm: 0.3650 / Grad Diff: 48.4131 / Time: 9.19s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 150 / acc: 91.090% / loss: 0.3100 / Time: 0.64s\n",
      "======================================================================================================\n",
      "\n",
      "round 150 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  151 / Acc: 89.808% / Loss: 0.3362 / Grad Norm: 0.4242 / Grad Diff: 47.9584 / Time: 7.52s\n",
      "======================================================================================================\n",
      "\n",
      "round 151 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  152 / Acc: 89.900% / Loss: 0.3353 / Grad Norm: 0.3948 / Grad Diff: 47.0796 / Time: 7.56s\n",
      "======================================================================================================\n",
      "\n",
      "round 152 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  153 / Acc: 89.965% / Loss: 0.3337 / Grad Norm: 0.3856 / Grad Diff: 46.9595 / Time: 8.05s\n",
      "======================================================================================================\n",
      "\n",
      "round 153 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  154 / Acc: 89.985% / Loss: 0.3322 / Grad Norm: 0.3784 / Grad Diff: 46.7573 / Time: 7.58s\n",
      "======================================================================================================\n",
      "\n",
      "round 154 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  155 / Acc: 90.039% / Loss: 0.3314 / Grad Norm: 0.3745 / Grad Diff: 45.6138 / Time: 7.54s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 155 / acc: 91.240% / loss: 0.3045 / Time: 0.62s\n",
      "======================================================================================================\n",
      "\n",
      "round 155 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  156 / Acc: 90.057% / Loss: 0.3306 / Grad Norm: 0.4233 / Grad Diff: 45.1077 / Time: 7.66s\n",
      "======================================================================================================\n",
      "\n",
      "round 156 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  157 / Acc: 90.127% / Loss: 0.3288 / Grad Norm: 0.3883 / Grad Diff: 44.9195 / Time: 7.42s\n",
      "======================================================================================================\n",
      "\n",
      "round 157 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  158 / Acc: 90.208% / Loss: 0.3276 / Grad Norm: 0.4096 / Grad Diff: 44.7224 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 158 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  159 / Acc: 90.258% / Loss: 0.3255 / Grad Norm: 0.3568 / Grad Diff: 44.6929 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 159 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  160 / Acc: 90.310% / Loss: 0.3240 / Grad Norm: 0.3649 / Grad Diff: 44.6852 / Time: 6.77s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 160 / acc: 91.400% / loss: 0.2978 / Time: 0.68s\n",
      "======================================================================================================\n",
      "\n",
      "round 160 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  161 / Acc: 90.258% / Loss: 0.3236 / Grad Norm: 0.4565 / Grad Diff: 44.2487 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 161 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  162 / Acc: 90.382% / Loss: 0.3215 / Grad Norm: 0.3572 / Grad Diff: 43.9034 / Time: 7.38s\n",
      "======================================================================================================\n",
      "\n",
      "round 162 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  163 / Acc: 90.421% / Loss: 0.3198 / Grad Norm: 0.3592 / Grad Diff: 43.8716 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "round 163 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  164 / Acc: 90.452% / Loss: 0.3187 / Grad Norm: 0.4273 / Grad Diff: 43.7860 / Time: 7.54s\n",
      "======================================================================================================\n",
      "\n",
      "round 164 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  165 / Acc: 90.476% / Loss: 0.3166 / Grad Norm: 0.3288 / Grad Diff: 43.8357 / Time: 7.39s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 165 / acc: 91.640% / loss: 0.2904 / Time: 0.78s\n",
      "======================================================================================================\n",
      "\n",
      "round 165 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  166 / Acc: 90.541% / Loss: 0.3151 / Grad Norm: 0.3375 / Grad Diff: 43.9800 / Time: 7.47s\n",
      "======================================================================================================\n",
      "\n",
      "round 166 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  167 / Acc: 90.555% / Loss: 0.3138 / Grad Norm: 0.3547 / Grad Diff: 43.8952 / Time: 7.00s\n",
      "======================================================================================================\n",
      "\n",
      "round 167 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  168 / Acc: 90.609% / Loss: 0.3130 / Grad Norm: 0.3779 / Grad Diff: 43.1885 / Time: 6.75s\n",
      "======================================================================================================\n",
      "\n",
      "round 168 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  169 / Acc: 90.640% / Loss: 0.3117 / Grad Norm: 0.3608 / Grad Diff: 42.9131 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "round 169 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  170 / Acc: 90.642% / Loss: 0.3103 / Grad Norm: 0.3404 / Grad Diff: 42.8023 / Time: 7.87s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 170 / acc: 91.690% / loss: 0.2843 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 170 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  171 / Acc: 90.666% / Loss: 0.3096 / Grad Norm: 0.3699 / Grad Diff: 42.1420 / Time: 7.05s\n",
      "======================================================================================================\n",
      "\n",
      "round 171 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  172 / Acc: 90.742% / Loss: 0.3082 / Grad Norm: 0.3629 / Grad Diff: 42.2823 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "round 172 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  173 / Acc: 90.755% / Loss: 0.3072 / Grad Norm: 0.3988 / Grad Diff: 41.9222 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 173 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  174 / Acc: 90.771% / Loss: 0.3057 / Grad Norm: 0.3374 / Grad Diff: 41.6649 / Time: 7.37s\n",
      "======================================================================================================\n",
      "\n",
      "round 174 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  175 / Acc: 90.832% / Loss: 0.3045 / Grad Norm: 0.3434 / Grad Diff: 41.5405 / Time: 7.36s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 175 / acc: 91.770% / loss: 0.2787 / Time: 0.59s\n",
      "======================================================================================================\n",
      "\n",
      "round 175 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  176 / Acc: 90.867% / Loss: 0.3030 / Grad Norm: 0.3342 / Grad Diff: 41.6760 / Time: 7.41s\n",
      "======================================================================================================\n",
      "\n",
      "round 176 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  177 / Acc: 90.913% / Loss: 0.3025 / Grad Norm: 0.3723 / Grad Diff: 41.0865 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "round 177 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  178 / Acc: 90.961% / Loss: 0.3011 / Grad Norm: 0.3385 / Grad Diff: 40.6579 / Time: 7.44s\n",
      "======================================================================================================\n",
      "\n",
      "round 178 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  179 / Acc: 90.970% / Loss: 0.3000 / Grad Norm: 0.3418 / Grad Diff: 40.4388 / Time: 7.39s\n",
      "======================================================================================================\n",
      "\n",
      "round 179 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  180 / Acc: 91.035% / Loss: 0.2988 / Grad Norm: 0.3216 / Grad Diff: 40.0799 / Time: 7.39s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 180 / acc: 91.950% / loss: 0.2738 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 180 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  181 / Acc: 91.055% / Loss: 0.2980 / Grad Norm: 0.3375 / Grad Diff: 39.8218 / Time: 7.50s\n",
      "======================================================================================================\n",
      "\n",
      "round 181 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  182 / Acc: 91.039% / Loss: 0.2968 / Grad Norm: 0.3426 / Grad Diff: 39.7094 / Time: 7.43s\n",
      "======================================================================================================\n",
      "\n",
      "round 182 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  183 / Acc: 91.070% / Loss: 0.2962 / Grad Norm: 0.3683 / Grad Diff: 39.2015 / Time: 9.46s\n",
      "======================================================================================================\n",
      "\n",
      "round 183 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  184 / Acc: 91.133% / Loss: 0.2949 / Grad Norm: 0.3437 / Grad Diff: 39.0264 / Time: 7.40s\n",
      "======================================================================================================\n",
      "\n",
      "round 184 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  185 / Acc: 91.164% / Loss: 0.2940 / Grad Norm: 0.3663 / Grad Diff: 38.9576 / Time: 7.76s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 185 / acc: 92.040% / loss: 0.2695 / Time: 0.67s\n",
      "======================================================================================================\n",
      "\n",
      "round 185 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  186 / Acc: 91.170% / Loss: 0.2929 / Grad Norm: 0.3695 / Grad Diff: 38.7132 / Time: 7.50s\n",
      "======================================================================================================\n",
      "\n",
      "round 186 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  187 / Acc: 91.192% / Loss: 0.2922 / Grad Norm: 0.3717 / Grad Diff: 38.1885 / Time: 7.38s\n",
      "======================================================================================================\n",
      "\n",
      "round 187 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  188 / Acc: 91.251% / Loss: 0.2907 / Grad Norm: 0.3692 / Grad Diff: 38.2483 / Time: 7.40s\n",
      "======================================================================================================\n",
      "\n",
      "round 188 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  189 / Acc: 91.286% / Loss: 0.2891 / Grad Norm: 0.3395 / Grad Diff: 38.2566 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 189 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  190 / Acc: 91.306% / Loss: 0.2876 / Grad Norm: 0.2950 / Grad Diff: 38.2966 / Time: 8.63s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 190 / acc: 92.250% / loss: 0.2630 / Time: 0.62s\n",
      "======================================================================================================\n",
      "\n",
      "round 190 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  191 / Acc: 91.323% / Loss: 0.2871 / Grad Norm: 0.3257 / Grad Diff: 37.6941 / Time: 7.46s\n",
      "======================================================================================================\n",
      "\n",
      "round 191 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  192 / Acc: 91.328% / Loss: 0.2863 / Grad Norm: 0.3592 / Grad Diff: 37.6031 / Time: 7.46s\n",
      "======================================================================================================\n",
      "\n",
      "round 192 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  193 / Acc: 91.351% / Loss: 0.2851 / Grad Norm: 0.3316 / Grad Diff: 37.4104 / Time: 7.45s\n",
      "======================================================================================================\n",
      "\n",
      "round 193 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  194 / Acc: 91.443% / Loss: 0.2838 / Grad Norm: 0.2963 / Grad Diff: 37.3060 / Time: 7.67s\n",
      "======================================================================================================\n",
      "\n",
      "round 194 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  195 / Acc: 91.452% / Loss: 0.2831 / Grad Norm: 0.3147 / Grad Diff: 36.9787 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 195 / acc: 92.430% / loss: 0.2587 / Time: 0.61s\n",
      "======================================================================================================\n",
      "\n",
      "round 195 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  196 / Acc: 91.472% / Loss: 0.2825 / Grad Norm: 0.3555 / Grad Diff: 36.6097 / Time: 7.05s\n",
      "======================================================================================================\n",
      "\n",
      "round 196 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  197 / Acc: 91.472% / Loss: 0.2820 / Grad Norm: 0.3558 / Grad Diff: 36.0627 / Time: 7.48s\n",
      "======================================================================================================\n",
      "\n",
      "round 197 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  198 / Acc: 91.542% / Loss: 0.2809 / Grad Norm: 0.3536 / Grad Diff: 36.0573 / Time: 7.99s\n",
      "======================================================================================================\n",
      "\n",
      "round 198 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  199 / Acc: 91.550% / Loss: 0.2798 / Grad Norm: 0.3697 / Grad Diff: 35.9115 / Time: 7.51s\n",
      "======================================================================================================\n",
      "\n",
      "round 199 local learning rate = 0.01\n",
      "\n",
      ">>> Round:  200 / Acc: 91.589% / Loss: 0.2783 / Grad Norm: 0.3326 / Grad Diff: 36.0121 / Time: 7.70s\n",
      "======================================================================================================\n",
      "\n",
      "= Test = round: 200 / acc: 92.630% / loss: 0.2542 / Time: 0.65s\n",
      "======================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seeds\n",
    "np.random.seed(1 + options['seed'])\n",
    "torch.manual_seed(12 + options['seed'])\n",
    "if options['gpu']:\n",
    "    torch.cuda.manual_seed_all(123 + options['seed'])\n",
    "\n",
    "train_path = os.path.join('./data', dataset_name, 'data', 'train')\n",
    "test_path = os.path.join('./data', dataset_name, 'data', 'test')\n",
    "\n",
    "# `dataset` is a tuple like (cids, groups, train_data, test_data)\n",
    "all_data_info = read_data(train_path, test_path, sub_data)\n",
    "\n",
    "# Call appropriate trainer\n",
    "model_path = f\"./models/{options['model']}_{dataset_name}_{uid}.pt\"\n",
    "print(f'FL pretrained model will be saved at {model_path}')\n",
    "trainer = trainer_class(options, all_data_info, model_path)\n",
    "trainer.train()\n",
    "\n",
    "# # FL training finish here, save the latest server model\n",
    "# flat_model_params = trainer.latest_model\n",
    "# model_path = f\"./models/{options['model']}_{dataset_name}_{options['algo']}.pt\"\n",
    "# torch.save(flat_model_params, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e8b6c-52e8-4300-93b6-ab3312622bdc",
   "metadata": {},
   "source": [
    "# Old version fine-tuning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35f4951-8598-466e-8cd7-573acd2810e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training model_target_only\n",
      "Epoch: 001, Train_loss: 1.2227, Train_acc: 0.6084, Test_loss: 1.2163, Test_acc: 0.6108\n",
      "Epoch: 002, Train_loss: 1.0485, Train_acc: 0.6744, Test_loss: 1.0463, Test_acc: 0.6768\n",
      "Epoch: 003, Train_loss: 0.9168, Train_acc: 0.7105, Test_loss: 0.9224, Test_acc: 0.7096\n",
      "Epoch: 004, Train_loss: 1.0087, Train_acc: 0.6950, Test_loss: 1.0185, Test_acc: 0.6921\n",
      "Epoch: 005, Train_loss: 0.9355, Train_acc: 0.7056, Test_loss: 0.9493, Test_acc: 0.7057\n",
      "Epoch: 006, Train_loss: 0.8694, Train_acc: 0.7296, Test_loss: 0.8747, Test_acc: 0.7228\n",
      "Epoch: 007, Train_loss: 0.8733, Train_acc: 0.7270, Test_loss: 0.8871, Test_acc: 0.7248\n",
      "Epoch: 008, Train_loss: 0.8907, Train_acc: 0.7196, Test_loss: 0.8908, Test_acc: 0.7213\n",
      "Epoch: 009, Train_loss: 0.8679, Train_acc: 0.7301, Test_loss: 0.8816, Test_acc: 0.7291\n",
      "Epoch: 010, Train_loss: 0.8503, Train_acc: 0.7410, Test_loss: 0.8832, Test_acc: 0.7343\n",
      ">>> Fine-tuning done!\n",
      ">>> Training model_ft\n",
      "Epoch: 001, Train_loss: 1.5132, Train_acc: 0.4746, Test_loss: 1.5383, Test_acc: 0.4697\n",
      "Epoch: 002, Train_loss: 1.4389, Train_acc: 0.4990, Test_loss: 1.4927, Test_acc: 0.4845\n",
      "Epoch: 003, Train_loss: 1.4122, Train_acc: 0.5084, Test_loss: 1.5039, Test_acc: 0.4873\n",
      "Epoch: 004, Train_loss: 1.4026, Train_acc: 0.5140, Test_loss: 1.5071, Test_acc: 0.4887\n",
      "Epoch: 005, Train_loss: 1.3782, Train_acc: 0.5122, Test_loss: 1.4992, Test_acc: 0.4774\n",
      "Epoch: 006, Train_loss: 1.3431, Train_acc: 0.5313, Test_loss: 1.4979, Test_acc: 0.4935\n",
      "Epoch: 007, Train_loss: 1.3532, Train_acc: 0.5299, Test_loss: 1.5148, Test_acc: 0.4965\n",
      "Epoch: 008, Train_loss: 1.3224, Train_acc: 0.5363, Test_loss: 1.5074, Test_acc: 0.4914\n",
      "Epoch: 009, Train_loss: 1.3171, Train_acc: 0.5402, Test_loss: 1.5300, Test_acc: 0.4931\n",
      "Epoch: 010, Train_loss: 1.3339, Train_acc: 0.5355, Test_loss: 1.5358, Test_acc: 0.4953\n",
      ">>> Fine-tuning done!\n",
      ">>> Training model_random\n",
      "Epoch: 001, Train_loss: 1.8656, Train_acc: 0.3066, Test_loss: 1.8585, Test_acc: 0.3135\n",
      "Epoch: 002, Train_loss: 1.7497, Train_acc: 0.3571, Test_loss: 1.7284, Test_acc: 0.3573\n",
      "Epoch: 003, Train_loss: 1.7537, Train_acc: 0.3700, Test_loss: 1.7366, Test_acc: 0.3754\n",
      "Epoch: 004, Train_loss: 1.6957, Train_acc: 0.3821, Test_loss: 1.6863, Test_acc: 0.3837\n",
      "Epoch: 005, Train_loss: 1.7571, Train_acc: 0.3498, Test_loss: 1.7479, Test_acc: 0.3567\n",
      "Epoch: 006, Train_loss: 1.7056, Train_acc: 0.3939, Test_loss: 1.6897, Test_acc: 0.3945\n",
      "Epoch: 007, Train_loss: 1.6931, Train_acc: 0.3950, Test_loss: 1.6825, Test_acc: 0.3985\n",
      "Epoch: 008, Train_loss: 1.7002, Train_acc: 0.3912, Test_loss: 1.6838, Test_acc: 0.3904\n",
      "Epoch: 009, Train_loss: 1.6409, Train_acc: 0.4152, Test_loss: 1.6294, Test_acc: 0.4190\n",
      "Epoch: 010, Train_loss: 1.6302, Train_acc: 0.4161, Test_loss: 1.6221, Test_acc: 0.4155\n",
      ">>> Fine-tuning done!\n",
      ">>> Evaluating model_source_only\n",
      "model_target_only: tensor([0.8503, 0.7410, 0.8832, 0.7343])\n",
      "model_ft: tensor([1.3339, 0.5355, 1.5358, 0.4953])\n",
      "model_random: tensor([1.6302, 0.4161, 1.6221, 0.4155])\n",
      "model_source_only: [2.2967622668251635, 0.12218436975644481, 2.2957382774289457, 0.12620819908899011]\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning part\n",
    "# load the previous model\n",
    "model_path = f\"./models/{options['model']}_{dataset_name}_{options['algo']}.pt\"\n",
    "flat_model_params = torch.load(model_path)\n",
    "\n",
    "# Initialize new models\n",
    "model_source_only = choose_model(options)  # baseline: lower bound (f on source, g on source)\n",
    "model_ft = choose_model(options)  # baseline: standard fine-tune (f on source, g on target)\n",
    "model_target_only = choose_model(options)  # baseline: upper bound (f on target, g on target)\n",
    "model_random = choose_model(options)  # baseline: lower bound (f random, g on target)\n",
    "\n",
    "# Assign model params\n",
    "set_flat_params_to(model_source_only, flat_model_params)\n",
    "set_flat_params_to(model_ft, flat_model_params)\n",
    "\n",
    "# Now model is set with flat_model_params\n",
    "# Start fine-tuning below\n",
    "# First, freeze all but last layer\n",
    "freeze(model_random, options['last_k'])\n",
    "freeze(model_ft, options['last_k'])\n",
    "\n",
    "# # load the fine-tuning dataset\n",
    "ft_train_loader, ft_test_loader = get_loader('./data/mnist_m', options['ft_dataset'], options['ft_batch_size'], num_workers=16)\n",
    "\n",
    "# Train model_target_only\n",
    "print('>>> Training model_target_only')\n",
    "model_target_only_results = ft_train(model_target_only, options, options['device'], ft_train_loader, ft_test_loader)\n",
    "\n",
    "# fine-tuning\n",
    "print('>>> Training model_ft')\n",
    "model_ft_results = ft_train(model_ft, options, options['device'], ft_train_loader, ft_test_loader)\n",
    "\n",
    "# fine-tuning random model\n",
    "print('>>> Training model_random')\n",
    "model_random_results = ft_train(model_random, options, options['device'], ft_train_loader, ft_test_loader)\n",
    "\n",
    "# evaluate model_source_only\n",
    "print('>>> Evaluating model_source_only')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model_source_only = model_source_only.to(options['device'])\n",
    "model_source_only_results = [0., 0., 0., 0.]\n",
    "model_source_only_results[0], model_source_only_results[1] = eval(model_source_only, options['device'],\n",
    "                                                                  ft_train_loader, criterion=criterion)\n",
    "model_source_only_results[2], model_source_only_results[3] = eval(model_source_only, options['device'],\n",
    "                                                                  ft_test_loader, criterion=criterion)\n",
    "\n",
    "print(f'model_target_only: {model_target_only_results[-1]}')\n",
    "print(f'model_ft: {model_ft_results[-1]}')\n",
    "print(f'model_random: {model_random_results[-1]}')\n",
    "print(f'model_source_only: {model_source_only_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8322c722-c815-4691-8675-b56f36edd463",
   "metadata": {},
   "source": [
    "# New fine-tuning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d21259-554e-46e2-a842-803b59342108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training model_target_only\n",
      "Epoch: 001, Train_loss: 0.6074, Train_acc: 0.8167, Test_loss: 0.6067, Test_acc: 0.8136\n",
      "Epoch: 002, Train_loss: 0.4909, Train_acc: 0.8480, Test_loss: 0.5082, Test_acc: 0.8409\n",
      "Epoch: 003, Train_loss: 0.4198, Train_acc: 0.8666, Test_loss: 0.4503, Test_acc: 0.8523\n",
      "Epoch: 004, Train_loss: 0.3872, Train_acc: 0.8768, Test_loss: 0.4312, Test_acc: 0.8598\n",
      "Epoch: 005, Train_loss: 0.3542, Train_acc: 0.8844, Test_loss: 0.4160, Test_acc: 0.8617\n",
      "Epoch: 006, Train_loss: 0.3236, Train_acc: 0.8928, Test_loss: 0.4022, Test_acc: 0.8680\n",
      "Epoch: 007, Train_loss: 0.3018, Train_acc: 0.8994, Test_loss: 0.4033, Test_acc: 0.8711\n",
      "Epoch: 008, Train_loss: 0.2796, Train_acc: 0.9078, Test_loss: 0.3887, Test_acc: 0.8682\n",
      "Epoch: 009, Train_loss: 0.2578, Train_acc: 0.9129, Test_loss: 0.3885, Test_acc: 0.8739\n",
      "Epoch: 010, Train_loss: 0.2531, Train_acc: 0.9143, Test_loss: 0.4094, Test_acc: 0.8731\n",
      "Epoch: 011, Train_loss: 0.2283, Train_acc: 0.9232, Test_loss: 0.3980, Test_acc: 0.8768\n",
      "Epoch: 012, Train_loss: 0.2142, Train_acc: 0.9283, Test_loss: 0.4082, Test_acc: 0.8745\n",
      "Epoch: 013, Train_loss: 0.2196, Train_acc: 0.9243, Test_loss: 0.4508, Test_acc: 0.8688\n",
      "Epoch: 014, Train_loss: 0.1911, Train_acc: 0.9367, Test_loss: 0.4215, Test_acc: 0.8760\n",
      "Epoch: 015, Train_loss: 0.1741, Train_acc: 0.9415, Test_loss: 0.4464, Test_acc: 0.8746\n",
      "Epoch: 016, Train_loss: 0.1614, Train_acc: 0.9468, Test_loss: 0.4451, Test_acc: 0.8781\n",
      "Epoch: 017, Train_loss: 0.1509, Train_acc: 0.9495, Test_loss: 0.4696, Test_acc: 0.8756\n",
      "Epoch: 018, Train_loss: 0.1395, Train_acc: 0.9531, Test_loss: 0.5065, Test_acc: 0.8743\n",
      "Epoch: 019, Train_loss: 0.1339, Train_acc: 0.9553, Test_loss: 0.5079, Test_acc: 0.8743\n",
      "Epoch: 020, Train_loss: 0.1433, Train_acc: 0.9502, Test_loss: 0.5104, Test_acc: 0.8723\n",
      "Epoch: 021, Train_loss: 0.1256, Train_acc: 0.9571, Test_loss: 0.5366, Test_acc: 0.8715\n",
      "Epoch: 022, Train_loss: 0.1143, Train_acc: 0.9611, Test_loss: 0.5590, Test_acc: 0.8718\n",
      "Epoch: 023, Train_loss: 0.1174, Train_acc: 0.9596, Test_loss: 0.5873, Test_acc: 0.8707\n",
      "Epoch: 024, Train_loss: 0.1006, Train_acc: 0.9659, Test_loss: 0.5984, Test_acc: 0.8718\n",
      "Epoch: 025, Train_loss: 0.1084, Train_acc: 0.9627, Test_loss: 0.6099, Test_acc: 0.8686\n",
      "Epoch: 026, Train_loss: 0.1016, Train_acc: 0.9652, Test_loss: 0.6419, Test_acc: 0.8680\n",
      "Epoch: 027, Train_loss: 0.0838, Train_acc: 0.9722, Test_loss: 0.6696, Test_acc: 0.8715\n",
      "Epoch: 028, Train_loss: 0.0849, Train_acc: 0.9715, Test_loss: 0.6901, Test_acc: 0.8697\n",
      "Epoch: 029, Train_loss: 0.0744, Train_acc: 0.9758, Test_loss: 0.6985, Test_acc: 0.8691\n",
      "Epoch: 030, Train_loss: 0.0811, Train_acc: 0.9720, Test_loss: 0.7732, Test_acc: 0.8703\n",
      "Epoch: 031, Train_loss: 0.0738, Train_acc: 0.9753, Test_loss: 0.7355, Test_acc: 0.8691\n",
      "Epoch: 032, Train_loss: 0.0650, Train_acc: 0.9789, Test_loss: 0.7719, Test_acc: 0.8686\n",
      "Epoch: 033, Train_loss: 0.0766, Train_acc: 0.9732, Test_loss: 0.8601, Test_acc: 0.8660\n",
      "Epoch: 034, Train_loss: 0.0696, Train_acc: 0.9769, Test_loss: 0.7919, Test_acc: 0.8657\n",
      "Epoch: 035, Train_loss: 0.0626, Train_acc: 0.9790, Test_loss: 0.8459, Test_acc: 0.8692\n",
      "Epoch: 036, Train_loss: 0.0655, Train_acc: 0.9772, Test_loss: 0.9154, Test_acc: 0.8687\n",
      "Epoch: 037, Train_loss: 0.0708, Train_acc: 0.9774, Test_loss: 0.9226, Test_acc: 0.8619\n",
      "Epoch: 038, Train_loss: 0.0680, Train_acc: 0.9761, Test_loss: 0.9438, Test_acc: 0.8656\n",
      "Epoch: 039, Train_loss: 0.0542, Train_acc: 0.9824, Test_loss: 0.9403, Test_acc: 0.8691\n",
      "Epoch: 040, Train_loss: 0.0502, Train_acc: 0.9838, Test_loss: 0.9110, Test_acc: 0.8647\n",
      "Epoch: 041, Train_loss: 0.0384, Train_acc: 0.9882, Test_loss: 0.9773, Test_acc: 0.8655\n",
      "Epoch: 042, Train_loss: 0.0626, Train_acc: 0.9795, Test_loss: 1.0194, Test_acc: 0.8682\n",
      "Epoch: 043, Train_loss: 0.0452, Train_acc: 0.9850, Test_loss: 1.0200, Test_acc: 0.8693\n",
      "Epoch: 044, Train_loss: 0.0614, Train_acc: 0.9788, Test_loss: 1.0475, Test_acc: 0.8680\n",
      "Epoch: 045, Train_loss: 0.0547, Train_acc: 0.9826, Test_loss: 1.0490, Test_acc: 0.8652\n",
      "Epoch: 046, Train_loss: 0.0460, Train_acc: 0.9851, Test_loss: 1.0660, Test_acc: 0.8635\n",
      "Epoch: 047, Train_loss: 0.0563, Train_acc: 0.9815, Test_loss: 1.0798, Test_acc: 0.8620\n",
      "Epoch: 048, Train_loss: 0.0369, Train_acc: 0.9875, Test_loss: 1.0921, Test_acc: 0.8685\n",
      "Epoch: 049, Train_loss: 0.0453, Train_acc: 0.9856, Test_loss: 1.1273, Test_acc: 0.8662\n",
      "Epoch: 050, Train_loss: 0.0519, Train_acc: 0.9830, Test_loss: 1.1842, Test_acc: 0.8630\n",
      "Epoch: 051, Train_loss: 0.0502, Train_acc: 0.9832, Test_loss: 1.2424, Test_acc: 0.8639\n",
      "Epoch: 052, Train_loss: 0.0376, Train_acc: 0.9880, Test_loss: 1.1558, Test_acc: 0.8660\n",
      "Epoch: 053, Train_loss: 0.0360, Train_acc: 0.9887, Test_loss: 1.1759, Test_acc: 0.8652\n",
      "Epoch: 054, Train_loss: 0.0466, Train_acc: 0.9832, Test_loss: 1.2342, Test_acc: 0.8632\n",
      "Epoch: 055, Train_loss: 0.0536, Train_acc: 0.9835, Test_loss: 1.2971, Test_acc: 0.8557\n",
      "Epoch: 056, Train_loss: 0.0302, Train_acc: 0.9908, Test_loss: 1.2410, Test_acc: 0.8661\n",
      "Epoch: 057, Train_loss: 0.0223, Train_acc: 0.9934, Test_loss: 1.2956, Test_acc: 0.8653\n",
      "Epoch: 058, Train_loss: 0.0424, Train_acc: 0.9866, Test_loss: 1.2872, Test_acc: 0.8658\n",
      "Epoch: 059, Train_loss: 0.0463, Train_acc: 0.9845, Test_loss: 1.2826, Test_acc: 0.8637\n",
      "Epoch: 060, Train_loss: 0.0255, Train_acc: 0.9923, Test_loss: 1.3401, Test_acc: 0.8648\n",
      "Epoch: 061, Train_loss: 0.0368, Train_acc: 0.9888, Test_loss: 1.3655, Test_acc: 0.8677\n",
      "Epoch: 062, Train_loss: 0.0327, Train_acc: 0.9893, Test_loss: 1.3770, Test_acc: 0.8623\n",
      "Epoch: 063, Train_loss: 0.0488, Train_acc: 0.9843, Test_loss: 1.3323, Test_acc: 0.8678\n",
      "Epoch: 064, Train_loss: 0.0295, Train_acc: 0.9908, Test_loss: 1.2830, Test_acc: 0.8651\n",
      "Epoch: 065, Train_loss: 0.0252, Train_acc: 0.9924, Test_loss: 1.3799, Test_acc: 0.8628\n",
      "Epoch: 066, Train_loss: 0.0253, Train_acc: 0.9922, Test_loss: 1.4337, Test_acc: 0.8625\n",
      "Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/model_target_only.pt.\n",
      "Model saved at ./models/ft_checkpoints/model_target_only.pt.\n",
      ">>> Fine-tuning done!\n",
      ">>> Training model_ft\n",
      "Epoch: 001, Train_loss: 1.1538, Train_acc: 0.6454, Test_loss: 1.1345, Test_acc: 0.6526\n",
      "Epoch: 002, Train_loss: 1.0401, Train_acc: 0.6775, Test_loss: 1.0240, Test_acc: 0.6794\n",
      "Epoch: 003, Train_loss: 0.9973, Train_acc: 0.6874, Test_loss: 0.9787, Test_acc: 0.6917\n",
      "Epoch: 004, Train_loss: 0.9534, Train_acc: 0.7042, Test_loss: 0.9453, Test_acc: 0.7034\n",
      "Epoch: 005, Train_loss: 0.9456, Train_acc: 0.7057, Test_loss: 0.9375, Test_acc: 0.7081\n",
      "Epoch: 006, Train_loss: 0.9060, Train_acc: 0.7185, Test_loss: 0.9003, Test_acc: 0.7196\n",
      "Epoch: 007, Train_loss: 0.8893, Train_acc: 0.7217, Test_loss: 0.8863, Test_acc: 0.7241\n",
      "Epoch: 008, Train_loss: 0.8800, Train_acc: 0.7256, Test_loss: 0.8843, Test_acc: 0.7250\n",
      "Epoch: 009, Train_loss: 0.8648, Train_acc: 0.7288, Test_loss: 0.8671, Test_acc: 0.7256\n",
      "Epoch: 010, Train_loss: 0.8576, Train_acc: 0.7322, Test_loss: 0.8597, Test_acc: 0.7303\n",
      "Epoch: 011, Train_loss: 0.8430, Train_acc: 0.7361, Test_loss: 0.8508, Test_acc: 0.7357\n",
      "Epoch: 012, Train_loss: 0.8388, Train_acc: 0.7375, Test_loss: 0.8452, Test_acc: 0.7378\n",
      "Epoch: 013, Train_loss: 0.8221, Train_acc: 0.7434, Test_loss: 0.8352, Test_acc: 0.7405\n",
      "Epoch: 014, Train_loss: 0.8187, Train_acc: 0.7442, Test_loss: 0.8347, Test_acc: 0.7401\n",
      "Epoch: 015, Train_loss: 0.8103, Train_acc: 0.7453, Test_loss: 0.8215, Test_acc: 0.7435\n",
      "Epoch: 016, Train_loss: 0.8178, Train_acc: 0.7450, Test_loss: 0.8294, Test_acc: 0.7437\n",
      "Epoch: 017, Train_loss: 0.7952, Train_acc: 0.7528, Test_loss: 0.8098, Test_acc: 0.7460\n",
      "Epoch: 018, Train_loss: 0.7965, Train_acc: 0.7510, Test_loss: 0.8143, Test_acc: 0.7444\n",
      "Epoch: 019, Train_loss: 0.7973, Train_acc: 0.7505, Test_loss: 0.8165, Test_acc: 0.7460\n",
      "Epoch: 020, Train_loss: 0.7915, Train_acc: 0.7523, Test_loss: 0.8135, Test_acc: 0.7478\n",
      "Epoch: 021, Train_loss: 0.7918, Train_acc: 0.7489, Test_loss: 0.8176, Test_acc: 0.7428\n",
      "Epoch: 022, Train_loss: 0.7698, Train_acc: 0.7582, Test_loss: 0.7969, Test_acc: 0.7486\n",
      "Epoch: 023, Train_loss: 0.7653, Train_acc: 0.7600, Test_loss: 0.7968, Test_acc: 0.7494\n",
      "Epoch: 024, Train_loss: 0.7627, Train_acc: 0.7599, Test_loss: 0.7921, Test_acc: 0.7520\n",
      "Epoch: 025, Train_loss: 0.7714, Train_acc: 0.7556, Test_loss: 0.8047, Test_acc: 0.7493\n",
      "Epoch: 026, Train_loss: 0.7647, Train_acc: 0.7603, Test_loss: 0.7978, Test_acc: 0.7531\n",
      "Epoch: 027, Train_loss: 0.7616, Train_acc: 0.7584, Test_loss: 0.7977, Test_acc: 0.7495\n",
      "Epoch: 028, Train_loss: 0.7471, Train_acc: 0.7650, Test_loss: 0.7902, Test_acc: 0.7551\n",
      "Epoch: 029, Train_loss: 0.7452, Train_acc: 0.7646, Test_loss: 0.7874, Test_acc: 0.7518\n",
      "Epoch: 030, Train_loss: 0.7468, Train_acc: 0.7633, Test_loss: 0.7939, Test_acc: 0.7565\n",
      "Epoch: 031, Train_loss: 0.7412, Train_acc: 0.7627, Test_loss: 0.7880, Test_acc: 0.7551\n",
      "Epoch: 032, Train_loss: 0.7520, Train_acc: 0.7610, Test_loss: 0.7971, Test_acc: 0.7554\n",
      "Epoch: 033, Train_loss: 0.7320, Train_acc: 0.7675, Test_loss: 0.7825, Test_acc: 0.7542\n",
      "Epoch: 034, Train_loss: 0.7289, Train_acc: 0.7685, Test_loss: 0.7819, Test_acc: 0.7551\n",
      "Epoch: 035, Train_loss: 0.7247, Train_acc: 0.7704, Test_loss: 0.7816, Test_acc: 0.7565\n",
      "Epoch: 036, Train_loss: 0.7257, Train_acc: 0.7696, Test_loss: 0.7832, Test_acc: 0.7535\n",
      "Epoch: 037, Train_loss: 0.7257, Train_acc: 0.7690, Test_loss: 0.7860, Test_acc: 0.7566\n",
      "Epoch: 038, Train_loss: 0.7237, Train_acc: 0.7681, Test_loss: 0.7889, Test_acc: 0.7497\n",
      "Epoch: 039, Train_loss: 0.7266, Train_acc: 0.7667, Test_loss: 0.7890, Test_acc: 0.7531\n",
      "Epoch: 040, Train_loss: 0.7146, Train_acc: 0.7722, Test_loss: 0.7766, Test_acc: 0.7567\n",
      "Epoch: 041, Train_loss: 0.7195, Train_acc: 0.7689, Test_loss: 0.7835, Test_acc: 0.7542\n",
      "Epoch: 042, Train_loss: 0.7200, Train_acc: 0.7709, Test_loss: 0.7862, Test_acc: 0.7574\n",
      "Epoch: 043, Train_loss: 0.7016, Train_acc: 0.7765, Test_loss: 0.7689, Test_acc: 0.7592\n",
      "Epoch: 044, Train_loss: 0.7182, Train_acc: 0.7692, Test_loss: 0.7827, Test_acc: 0.7567\n",
      "Epoch: 045, Train_loss: 0.7032, Train_acc: 0.7756, Test_loss: 0.7799, Test_acc: 0.7574\n",
      "Epoch: 046, Train_loss: 0.7115, Train_acc: 0.7712, Test_loss: 0.7773, Test_acc: 0.7555\n",
      "Epoch: 047, Train_loss: 0.7147, Train_acc: 0.7706, Test_loss: 0.7880, Test_acc: 0.7515\n",
      "Epoch: 048, Train_loss: 0.7016, Train_acc: 0.7771, Test_loss: 0.7790, Test_acc: 0.7592\n",
      "Epoch: 049, Train_loss: 0.6992, Train_acc: 0.7767, Test_loss: 0.7813, Test_acc: 0.7571\n",
      "Epoch: 050, Train_loss: 0.6939, Train_acc: 0.7773, Test_loss: 0.7759, Test_acc: 0.7565\n",
      "Epoch: 051, Train_loss: 0.6931, Train_acc: 0.7779, Test_loss: 0.7793, Test_acc: 0.7574\n",
      "Epoch: 052, Train_loss: 0.6972, Train_acc: 0.7759, Test_loss: 0.7835, Test_acc: 0.7535\n",
      "Epoch: 053, Train_loss: 0.6911, Train_acc: 0.7789, Test_loss: 0.7765, Test_acc: 0.7576\n",
      "Epoch: 054, Train_loss: 0.6889, Train_acc: 0.7791, Test_loss: 0.7751, Test_acc: 0.7552\n",
      "Epoch: 055, Train_loss: 0.6880, Train_acc: 0.7788, Test_loss: 0.7754, Test_acc: 0.7575\n",
      "Epoch: 056, Train_loss: 0.6821, Train_acc: 0.7814, Test_loss: 0.7727, Test_acc: 0.7586\n",
      "Epoch: 057, Train_loss: 0.7006, Train_acc: 0.7736, Test_loss: 0.7892, Test_acc: 0.7530\n",
      "Epoch: 058, Train_loss: 0.6850, Train_acc: 0.7795, Test_loss: 0.7795, Test_acc: 0.7558\n",
      "Epoch: 059, Train_loss: 0.6826, Train_acc: 0.7809, Test_loss: 0.7764, Test_acc: 0.7622\n",
      "Epoch: 060, Train_loss: 0.6827, Train_acc: 0.7794, Test_loss: 0.7770, Test_acc: 0.7586\n",
      "Epoch: 061, Train_loss: 0.6811, Train_acc: 0.7803, Test_loss: 0.7715, Test_acc: 0.7598\n",
      "Epoch: 062, Train_loss: 0.6831, Train_acc: 0.7788, Test_loss: 0.7777, Test_acc: 0.7591\n",
      "Epoch: 063, Train_loss: 0.6720, Train_acc: 0.7841, Test_loss: 0.7685, Test_acc: 0.7627\n",
      "Epoch: 064, Train_loss: 0.6788, Train_acc: 0.7815, Test_loss: 0.7760, Test_acc: 0.7585\n",
      "Epoch: 065, Train_loss: 0.6689, Train_acc: 0.7851, Test_loss: 0.7691, Test_acc: 0.7626\n",
      "Epoch: 066, Train_loss: 0.6741, Train_acc: 0.7819, Test_loss: 0.7760, Test_acc: 0.7569\n",
      "Epoch: 067, Train_loss: 0.6747, Train_acc: 0.7824, Test_loss: 0.7729, Test_acc: 0.7587\n",
      "Epoch: 068, Train_loss: 0.6791, Train_acc: 0.7783, Test_loss: 0.7774, Test_acc: 0.7550\n",
      "Epoch: 069, Train_loss: 0.6695, Train_acc: 0.7844, Test_loss: 0.7717, Test_acc: 0.7596\n",
      "Epoch: 070, Train_loss: 0.6749, Train_acc: 0.7815, Test_loss: 0.7766, Test_acc: 0.7570\n",
      "Epoch: 071, Train_loss: 0.6690, Train_acc: 0.7835, Test_loss: 0.7715, Test_acc: 0.7576\n",
      "Epoch: 072, Train_loss: 0.6621, Train_acc: 0.7872, Test_loss: 0.7727, Test_acc: 0.7588\n",
      "Epoch: 073, Train_loss: 0.6587, Train_acc: 0.7886, Test_loss: 0.7644, Test_acc: 0.7628\n",
      "Epoch: 074, Train_loss: 0.6710, Train_acc: 0.7823, Test_loss: 0.7711, Test_acc: 0.7572\n",
      "Epoch: 075, Train_loss: 0.6674, Train_acc: 0.7830, Test_loss: 0.7755, Test_acc: 0.7574\n",
      "Epoch: 076, Train_loss: 0.6617, Train_acc: 0.7849, Test_loss: 0.7728, Test_acc: 0.7579\n",
      "Epoch: 077, Train_loss: 0.6712, Train_acc: 0.7822, Test_loss: 0.7805, Test_acc: 0.7559\n",
      "Epoch: 078, Train_loss: 0.6664, Train_acc: 0.7851, Test_loss: 0.7767, Test_acc: 0.7587\n",
      "Epoch: 079, Train_loss: 0.6641, Train_acc: 0.7842, Test_loss: 0.7846, Test_acc: 0.7585\n",
      "Epoch: 080, Train_loss: 0.6692, Train_acc: 0.7830, Test_loss: 0.7784, Test_acc: 0.7552\n",
      "Epoch: 081, Train_loss: 0.6611, Train_acc: 0.7860, Test_loss: 0.7718, Test_acc: 0.7605\n",
      "Epoch: 082, Train_loss: 0.6596, Train_acc: 0.7885, Test_loss: 0.7793, Test_acc: 0.7579\n",
      "Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/model_ft.pt.\n",
      "Model saved at ./models/ft_checkpoints/model_ft.pt.\n",
      ">>> Fine-tuning done!\n",
      ">>> Training model_random\n",
      "Epoch: 001, Train_loss: 1.7664, Train_acc: 0.4327, Test_loss: 1.7581, Test_acc: 0.4395\n",
      "Epoch: 002, Train_loss: 1.5233, Train_acc: 0.5148, Test_loss: 1.5117, Test_acc: 0.5218\n",
      "Epoch: 003, Train_loss: 1.4172, Train_acc: 0.5491, Test_loss: 1.4064, Test_acc: 0.5505\n",
      "Epoch: 004, Train_loss: 1.3588, Train_acc: 0.5656, Test_loss: 1.3503, Test_acc: 0.5639\n",
      "Epoch: 005, Train_loss: 1.3211, Train_acc: 0.5828, Test_loss: 1.3143, Test_acc: 0.5826\n",
      "Epoch: 006, Train_loss: 1.2859, Train_acc: 0.5905, Test_loss: 1.2815, Test_acc: 0.5877\n",
      "Epoch: 007, Train_loss: 1.2574, Train_acc: 0.6006, Test_loss: 1.2543, Test_acc: 0.6018\n",
      "Epoch: 008, Train_loss: 1.2346, Train_acc: 0.6085, Test_loss: 1.2336, Test_acc: 0.6032\n",
      "Epoch: 009, Train_loss: 1.2135, Train_acc: 0.6125, Test_loss: 1.2133, Test_acc: 0.6118\n",
      "Epoch: 010, Train_loss: 1.1945, Train_acc: 0.6160, Test_loss: 1.1980, Test_acc: 0.6152\n",
      "Epoch: 011, Train_loss: 1.1778, Train_acc: 0.6222, Test_loss: 1.1813, Test_acc: 0.6220\n",
      "Epoch: 012, Train_loss: 1.1623, Train_acc: 0.6266, Test_loss: 1.1670, Test_acc: 0.6270\n",
      "Epoch: 013, Train_loss: 1.1509, Train_acc: 0.6301, Test_loss: 1.1584, Test_acc: 0.6293\n",
      "Epoch: 014, Train_loss: 1.1370, Train_acc: 0.6344, Test_loss: 1.1458, Test_acc: 0.6325\n",
      "Epoch: 015, Train_loss: 1.1268, Train_acc: 0.6380, Test_loss: 1.1365, Test_acc: 0.6364\n",
      "Epoch: 016, Train_loss: 1.1149, Train_acc: 0.6410, Test_loss: 1.1240, Test_acc: 0.6454\n",
      "Epoch: 017, Train_loss: 1.1102, Train_acc: 0.6429, Test_loss: 1.1221, Test_acc: 0.6416\n",
      "Epoch: 018, Train_loss: 1.0965, Train_acc: 0.6494, Test_loss: 1.1117, Test_acc: 0.6502\n",
      "Epoch: 019, Train_loss: 1.0857, Train_acc: 0.6501, Test_loss: 1.1024, Test_acc: 0.6484\n",
      "Epoch: 020, Train_loss: 1.0773, Train_acc: 0.6538, Test_loss: 1.0957, Test_acc: 0.6534\n",
      "Epoch: 021, Train_loss: 1.0684, Train_acc: 0.6572, Test_loss: 1.0862, Test_acc: 0.6547\n",
      "Epoch: 022, Train_loss: 1.0612, Train_acc: 0.6581, Test_loss: 1.0829, Test_acc: 0.6528\n",
      "Epoch: 023, Train_loss: 1.0550, Train_acc: 0.6595, Test_loss: 1.0768, Test_acc: 0.6574\n",
      "Epoch: 024, Train_loss: 1.0457, Train_acc: 0.6635, Test_loss: 1.0676, Test_acc: 0.6617\n",
      "Epoch: 025, Train_loss: 1.0394, Train_acc: 0.6657, Test_loss: 1.0656, Test_acc: 0.6621\n",
      "Epoch: 026, Train_loss: 1.0316, Train_acc: 0.6678, Test_loss: 1.0577, Test_acc: 0.6655\n",
      "Epoch: 027, Train_loss: 1.0259, Train_acc: 0.6703, Test_loss: 1.0544, Test_acc: 0.6654\n",
      "Epoch: 028, Train_loss: 1.0193, Train_acc: 0.6728, Test_loss: 1.0495, Test_acc: 0.6680\n",
      "Epoch: 029, Train_loss: 1.0124, Train_acc: 0.6726, Test_loss: 1.0418, Test_acc: 0.6677\n",
      "Epoch: 030, Train_loss: 1.0084, Train_acc: 0.6754, Test_loss: 1.0398, Test_acc: 0.6729\n",
      "Epoch: 031, Train_loss: 1.0051, Train_acc: 0.6755, Test_loss: 1.0397, Test_acc: 0.6714\n",
      "Epoch: 032, Train_loss: 1.0015, Train_acc: 0.6770, Test_loss: 1.0368, Test_acc: 0.6740\n",
      "Epoch: 033, Train_loss: 0.9957, Train_acc: 0.6792, Test_loss: 1.0305, Test_acc: 0.6734\n",
      "Epoch: 034, Train_loss: 0.9905, Train_acc: 0.6789, Test_loss: 1.0294, Test_acc: 0.6743\n",
      "Epoch: 035, Train_loss: 0.9848, Train_acc: 0.6818, Test_loss: 1.0233, Test_acc: 0.6789\n",
      "Epoch: 036, Train_loss: 0.9826, Train_acc: 0.6810, Test_loss: 1.0250, Test_acc: 0.6768\n",
      "Epoch: 037, Train_loss: 0.9768, Train_acc: 0.6825, Test_loss: 1.0183, Test_acc: 0.6781\n",
      "Epoch: 038, Train_loss: 0.9709, Train_acc: 0.6854, Test_loss: 1.0142, Test_acc: 0.6794\n",
      "Epoch: 039, Train_loss: 0.9676, Train_acc: 0.6888, Test_loss: 1.0129, Test_acc: 0.6796\n",
      "Epoch: 040, Train_loss: 0.9643, Train_acc: 0.6881, Test_loss: 1.0100, Test_acc: 0.6816\n",
      "Epoch: 041, Train_loss: 0.9677, Train_acc: 0.6862, Test_loss: 1.0176, Test_acc: 0.6804\n",
      "Epoch: 042, Train_loss: 0.9569, Train_acc: 0.6900, Test_loss: 1.0060, Test_acc: 0.6825\n",
      "Epoch: 043, Train_loss: 0.9553, Train_acc: 0.6916, Test_loss: 1.0058, Test_acc: 0.6839\n",
      "Epoch: 044, Train_loss: 0.9552, Train_acc: 0.6915, Test_loss: 1.0082, Test_acc: 0.6823\n",
      "Epoch: 045, Train_loss: 0.9473, Train_acc: 0.6922, Test_loss: 0.9995, Test_acc: 0.6864\n",
      "Epoch: 046, Train_loss: 0.9447, Train_acc: 0.6940, Test_loss: 0.9994, Test_acc: 0.6838\n",
      "Epoch: 047, Train_loss: 0.9409, Train_acc: 0.6954, Test_loss: 0.9969, Test_acc: 0.6876\n",
      "Epoch: 048, Train_loss: 0.9418, Train_acc: 0.6971, Test_loss: 0.9999, Test_acc: 0.6853\n",
      "Epoch: 049, Train_loss: 0.9360, Train_acc: 0.6979, Test_loss: 0.9944, Test_acc: 0.6861\n",
      "Epoch: 050, Train_loss: 0.9344, Train_acc: 0.6970, Test_loss: 0.9939, Test_acc: 0.6867\n",
      "Epoch: 051, Train_loss: 0.9317, Train_acc: 0.6991, Test_loss: 0.9930, Test_acc: 0.6868\n",
      "Epoch: 052, Train_loss: 0.9277, Train_acc: 0.6998, Test_loss: 0.9918, Test_acc: 0.6874\n",
      "Epoch: 053, Train_loss: 0.9252, Train_acc: 0.7009, Test_loss: 0.9909, Test_acc: 0.6878\n",
      "Epoch: 054, Train_loss: 0.9211, Train_acc: 0.7019, Test_loss: 0.9864, Test_acc: 0.6883\n",
      "Epoch: 055, Train_loss: 0.9199, Train_acc: 0.7020, Test_loss: 0.9864, Test_acc: 0.6888\n",
      "Epoch: 056, Train_loss: 0.9177, Train_acc: 0.7036, Test_loss: 0.9849, Test_acc: 0.6928\n",
      "Epoch: 057, Train_loss: 0.9154, Train_acc: 0.7034, Test_loss: 0.9839, Test_acc: 0.6868\n",
      "Epoch: 058, Train_loss: 0.9137, Train_acc: 0.7056, Test_loss: 0.9831, Test_acc: 0.6905\n",
      "Epoch: 059, Train_loss: 0.9096, Train_acc: 0.7059, Test_loss: 0.9815, Test_acc: 0.6913\n",
      "Epoch: 060, Train_loss: 0.9079, Train_acc: 0.7062, Test_loss: 0.9807, Test_acc: 0.6903\n",
      "Epoch: 061, Train_loss: 0.9034, Train_acc: 0.7074, Test_loss: 0.9782, Test_acc: 0.6931\n",
      "Epoch: 062, Train_loss: 0.9049, Train_acc: 0.7078, Test_loss: 0.9803, Test_acc: 0.6938\n",
      "Epoch: 063, Train_loss: 0.9004, Train_acc: 0.7081, Test_loss: 0.9768, Test_acc: 0.6943\n",
      "Epoch: 064, Train_loss: 0.8997, Train_acc: 0.7075, Test_loss: 0.9764, Test_acc: 0.6944\n",
      "Epoch: 065, Train_loss: 0.8982, Train_acc: 0.7095, Test_loss: 0.9786, Test_acc: 0.6941\n",
      "Epoch: 066, Train_loss: 0.8942, Train_acc: 0.7103, Test_loss: 0.9733, Test_acc: 0.6945\n",
      "Epoch: 067, Train_loss: 0.8952, Train_acc: 0.7097, Test_loss: 0.9768, Test_acc: 0.6934\n",
      "Epoch: 068, Train_loss: 0.8928, Train_acc: 0.7093, Test_loss: 0.9733, Test_acc: 0.6940\n",
      "Epoch: 069, Train_loss: 0.8936, Train_acc: 0.7113, Test_loss: 0.9770, Test_acc: 0.6926\n",
      "Epoch: 070, Train_loss: 0.8881, Train_acc: 0.7127, Test_loss: 0.9744, Test_acc: 0.6938\n",
      "Epoch: 071, Train_loss: 0.8879, Train_acc: 0.7139, Test_loss: 0.9752, Test_acc: 0.6960\n",
      "Epoch: 072, Train_loss: 0.8850, Train_acc: 0.7118, Test_loss: 0.9722, Test_acc: 0.6970\n",
      "Epoch: 073, Train_loss: 0.8832, Train_acc: 0.7145, Test_loss: 0.9719, Test_acc: 0.6944\n",
      "Epoch: 074, Train_loss: 0.8824, Train_acc: 0.7136, Test_loss: 0.9727, Test_acc: 0.6958\n",
      "Epoch: 075, Train_loss: 0.8804, Train_acc: 0.7142, Test_loss: 0.9722, Test_acc: 0.6957\n",
      "Epoch: 076, Train_loss: 0.8796, Train_acc: 0.7144, Test_loss: 0.9703, Test_acc: 0.6938\n",
      "Epoch: 077, Train_loss: 0.8789, Train_acc: 0.7149, Test_loss: 0.9727, Test_acc: 0.6959\n",
      "Epoch: 078, Train_loss: 0.8761, Train_acc: 0.7152, Test_loss: 0.9727, Test_acc: 0.6961\n",
      "Epoch: 079, Train_loss: 0.8757, Train_acc: 0.7167, Test_loss: 0.9731, Test_acc: 0.6915\n",
      "Epoch: 080, Train_loss: 0.8736, Train_acc: 0.7165, Test_loss: 0.9710, Test_acc: 0.6920\n",
      "Epoch: 081, Train_loss: 0.8727, Train_acc: 0.7174, Test_loss: 0.9706, Test_acc: 0.6935\n",
      "Epoch: 082, Train_loss: 0.8721, Train_acc: 0.7179, Test_loss: 0.9705, Test_acc: 0.6936\n",
      "Epoch: 083, Train_loss: 0.8675, Train_acc: 0.7188, Test_loss: 0.9678, Test_acc: 0.6929\n",
      "Epoch: 084, Train_loss: 0.8713, Train_acc: 0.7170, Test_loss: 0.9703, Test_acc: 0.6944\n",
      "Epoch: 085, Train_loss: 0.8661, Train_acc: 0.7196, Test_loss: 0.9694, Test_acc: 0.6954\n",
      "Epoch: 086, Train_loss: 0.8673, Train_acc: 0.7176, Test_loss: 0.9678, Test_acc: 0.6934\n",
      "Epoch: 087, Train_loss: 0.8626, Train_acc: 0.7210, Test_loss: 0.9677, Test_acc: 0.6949\n",
      "Epoch: 088, Train_loss: 0.8629, Train_acc: 0.7208, Test_loss: 0.9695, Test_acc: 0.6925\n",
      "Epoch: 089, Train_loss: 0.8605, Train_acc: 0.7204, Test_loss: 0.9688, Test_acc: 0.6941\n",
      "Epoch: 090, Train_loss: 0.8610, Train_acc: 0.7202, Test_loss: 0.9670, Test_acc: 0.6974\n",
      "Epoch: 091, Train_loss: 0.8609, Train_acc: 0.7215, Test_loss: 0.9706, Test_acc: 0.6916\n",
      "Epoch: 092, Train_loss: 0.8559, Train_acc: 0.7229, Test_loss: 0.9673, Test_acc: 0.6968\n",
      "Epoch: 093, Train_loss: 0.8560, Train_acc: 0.7222, Test_loss: 0.9676, Test_acc: 0.6958\n",
      "Epoch: 094, Train_loss: 0.8554, Train_acc: 0.7225, Test_loss: 0.9672, Test_acc: 0.6939\n",
      "Epoch: 095, Train_loss: 0.8584, Train_acc: 0.7215, Test_loss: 0.9711, Test_acc: 0.6940\n",
      "Epoch: 096, Train_loss: 0.8528, Train_acc: 0.7233, Test_loss: 0.9680, Test_acc: 0.6934\n",
      "Epoch: 097, Train_loss: 0.8520, Train_acc: 0.7218, Test_loss: 0.9670, Test_acc: 0.6961\n",
      "Epoch: 098, Train_loss: 0.8514, Train_acc: 0.7222, Test_loss: 0.9672, Test_acc: 0.6978\n",
      "Epoch: 099, Train_loss: 0.8527, Train_acc: 0.7228, Test_loss: 0.9706, Test_acc: 0.6945\n",
      "Epoch: 100, Train_loss: 0.8484, Train_acc: 0.7246, Test_loss: 0.9687, Test_acc: 0.6967\n",
      "Epoch: 101, Train_loss: 0.8493, Train_acc: 0.7230, Test_loss: 0.9677, Test_acc: 0.6934\n",
      "Epoch: 102, Train_loss: 0.8469, Train_acc: 0.7245, Test_loss: 0.9674, Test_acc: 0.6949\n",
      "Epoch: 103, Train_loss: 0.8476, Train_acc: 0.7244, Test_loss: 0.9669, Test_acc: 0.6955\n",
      "Epoch: 104, Train_loss: 0.8452, Train_acc: 0.7263, Test_loss: 0.9682, Test_acc: 0.6947\n",
      "Epoch: 105, Train_loss: 0.8457, Train_acc: 0.7238, Test_loss: 0.9657, Test_acc: 0.6957\n",
      "Epoch: 106, Train_loss: 0.8461, Train_acc: 0.7246, Test_loss: 0.9681, Test_acc: 0.6961\n",
      "Epoch: 107, Train_loss: 0.8419, Train_acc: 0.7266, Test_loss: 0.9669, Test_acc: 0.6969\n",
      "Epoch: 108, Train_loss: 0.8436, Train_acc: 0.7251, Test_loss: 0.9703, Test_acc: 0.6949\n",
      "Epoch: 109, Train_loss: 0.8415, Train_acc: 0.7270, Test_loss: 0.9679, Test_acc: 0.6951\n",
      "Epoch: 110, Train_loss: 0.8401, Train_acc: 0.7264, Test_loss: 0.9674, Test_acc: 0.6939\n",
      "Epoch: 111, Train_loss: 0.8401, Train_acc: 0.7273, Test_loss: 0.9694, Test_acc: 0.6947\n",
      "Epoch: 112, Train_loss: 0.8386, Train_acc: 0.7271, Test_loss: 0.9663, Test_acc: 0.6945\n",
      "Epoch: 113, Train_loss: 0.8426, Train_acc: 0.7250, Test_loss: 0.9721, Test_acc: 0.6933\n",
      "Epoch: 114, Train_loss: 0.8351, Train_acc: 0.7295, Test_loss: 0.9646, Test_acc: 0.6970\n",
      "Epoch: 115, Train_loss: 0.8375, Train_acc: 0.7269, Test_loss: 0.9655, Test_acc: 0.6961\n",
      "Epoch: 116, Train_loss: 0.8346, Train_acc: 0.7294, Test_loss: 0.9691, Test_acc: 0.6968\n",
      "Epoch: 117, Train_loss: 0.8354, Train_acc: 0.7279, Test_loss: 0.9652, Test_acc: 0.6964\n",
      "Epoch: 118, Train_loss: 0.8329, Train_acc: 0.7289, Test_loss: 0.9685, Test_acc: 0.6944\n",
      "Epoch: 119, Train_loss: 0.8352, Train_acc: 0.7279, Test_loss: 0.9672, Test_acc: 0.6965\n",
      "Epoch: 120, Train_loss: 0.8313, Train_acc: 0.7314, Test_loss: 0.9690, Test_acc: 0.6967\n",
      "Epoch: 121, Train_loss: 0.8315, Train_acc: 0.7310, Test_loss: 0.9666, Test_acc: 0.6955\n",
      "Epoch: 122, Train_loss: 0.8321, Train_acc: 0.7291, Test_loss: 0.9685, Test_acc: 0.6965\n",
      "Epoch: 123, Train_loss: 0.8328, Train_acc: 0.7284, Test_loss: 0.9701, Test_acc: 0.6985\n",
      "Epoch: 124, Train_loss: 0.8295, Train_acc: 0.7322, Test_loss: 0.9619, Test_acc: 0.6965\n",
      "Epoch: 125, Train_loss: 0.8270, Train_acc: 0.7316, Test_loss: 0.9678, Test_acc: 0.6971\n",
      "Epoch: 126, Train_loss: 0.8274, Train_acc: 0.7316, Test_loss: 0.9683, Test_acc: 0.6956\n",
      "Epoch: 127, Train_loss: 0.8265, Train_acc: 0.7313, Test_loss: 0.9671, Test_acc: 0.6959\n",
      "Epoch: 128, Train_loss: 0.8305, Train_acc: 0.7296, Test_loss: 0.9744, Test_acc: 0.6947\n",
      "Epoch: 129, Train_loss: 0.8249, Train_acc: 0.7323, Test_loss: 0.9678, Test_acc: 0.6963\n",
      "Epoch: 130, Train_loss: 0.8264, Train_acc: 0.7300, Test_loss: 0.9684, Test_acc: 0.6950\n",
      "Epoch: 131, Train_loss: 0.8234, Train_acc: 0.7324, Test_loss: 0.9699, Test_acc: 0.6956\n",
      "Epoch: 132, Train_loss: 0.8217, Train_acc: 0.7332, Test_loss: 0.9663, Test_acc: 0.6967\n",
      "Epoch: 133, Train_loss: 0.8239, Train_acc: 0.7318, Test_loss: 0.9698, Test_acc: 0.6971\n",
      "Epoch: 134, Train_loss: 0.8224, Train_acc: 0.7321, Test_loss: 0.9683, Test_acc: 0.6957\n",
      "Epoch: 135, Train_loss: 0.8247, Train_acc: 0.7299, Test_loss: 0.9696, Test_acc: 0.6949\n",
      "Epoch: 136, Train_loss: 0.8231, Train_acc: 0.7320, Test_loss: 0.9674, Test_acc: 0.6937\n",
      "Epoch: 137, Train_loss: 0.8226, Train_acc: 0.7329, Test_loss: 0.9698, Test_acc: 0.6943\n",
      "Epoch: 138, Train_loss: 0.8190, Train_acc: 0.7327, Test_loss: 0.9687, Test_acc: 0.6976\n",
      "Epoch: 139, Train_loss: 0.8185, Train_acc: 0.7346, Test_loss: 0.9683, Test_acc: 0.6961\n",
      "Epoch: 140, Train_loss: 0.8190, Train_acc: 0.7321, Test_loss: 0.9687, Test_acc: 0.6964\n",
      "Epoch: 141, Train_loss: 0.8155, Train_acc: 0.7348, Test_loss: 0.9672, Test_acc: 0.6973\n",
      "Epoch: 142, Train_loss: 0.8176, Train_acc: 0.7335, Test_loss: 0.9709, Test_acc: 0.6958\n",
      "Epoch: 143, Train_loss: 0.8146, Train_acc: 0.7362, Test_loss: 0.9672, Test_acc: 0.6985\n",
      "Epoch: 144, Train_loss: 0.8154, Train_acc: 0.7343, Test_loss: 0.9686, Test_acc: 0.6961\n",
      "Epoch: 145, Train_loss: 0.8155, Train_acc: 0.7349, Test_loss: 0.9672, Test_acc: 0.6976\n",
      "Epoch: 146, Train_loss: 0.8144, Train_acc: 0.7347, Test_loss: 0.9687, Test_acc: 0.6966\n",
      "Epoch: 147, Train_loss: 0.8139, Train_acc: 0.7359, Test_loss: 0.9728, Test_acc: 0.6960\n",
      "Epoch: 148, Train_loss: 0.8137, Train_acc: 0.7346, Test_loss: 0.9711, Test_acc: 0.6976\n",
      "Epoch: 149, Train_loss: 0.8118, Train_acc: 0.7358, Test_loss: 0.9697, Test_acc: 0.6981\n",
      "Epoch: 150, Train_loss: 0.8126, Train_acc: 0.7359, Test_loss: 0.9693, Test_acc: 0.6965\n",
      "Epoch: 151, Train_loss: 0.8112, Train_acc: 0.7359, Test_loss: 0.9700, Test_acc: 0.6957\n",
      "Epoch: 152, Train_loss: 0.8130, Train_acc: 0.7353, Test_loss: 0.9700, Test_acc: 0.6986\n",
      "Epoch: 153, Train_loss: 0.8145, Train_acc: 0.7340, Test_loss: 0.9714, Test_acc: 0.6965\n",
      "Epoch: 154, Train_loss: 0.8132, Train_acc: 0.7338, Test_loss: 0.9735, Test_acc: 0.6959\n",
      "Epoch: 155, Train_loss: 0.8123, Train_acc: 0.7351, Test_loss: 0.9710, Test_acc: 0.6983\n",
      "Epoch: 156, Train_loss: 0.8094, Train_acc: 0.7373, Test_loss: 0.9722, Test_acc: 0.6980\n",
      "Epoch: 157, Train_loss: 0.8116, Train_acc: 0.7352, Test_loss: 0.9721, Test_acc: 0.6951\n",
      "Epoch: 158, Train_loss: 0.8076, Train_acc: 0.7365, Test_loss: 0.9706, Test_acc: 0.6964\n",
      "Epoch: 159, Train_loss: 0.8104, Train_acc: 0.7361, Test_loss: 0.9756, Test_acc: 0.6939\n",
      "Epoch: 160, Train_loss: 0.8095, Train_acc: 0.7380, Test_loss: 0.9760, Test_acc: 0.6970\n",
      "Epoch: 161, Train_loss: 0.8068, Train_acc: 0.7370, Test_loss: 0.9741, Test_acc: 0.7003\n",
      "Epoch: 162, Train_loss: 0.8059, Train_acc: 0.7374, Test_loss: 0.9711, Test_acc: 0.6984\n",
      "Epoch: 163, Train_loss: 0.8055, Train_acc: 0.7373, Test_loss: 0.9722, Test_acc: 0.6969\n",
      "Epoch: 164, Train_loss: 0.8048, Train_acc: 0.7377, Test_loss: 0.9718, Test_acc: 0.6975\n",
      "Epoch: 165, Train_loss: 0.8071, Train_acc: 0.7365, Test_loss: 0.9761, Test_acc: 0.6990\n",
      "Epoch: 166, Train_loss: 0.8031, Train_acc: 0.7383, Test_loss: 0.9713, Test_acc: 0.6996\n",
      "Epoch: 167, Train_loss: 0.8044, Train_acc: 0.7386, Test_loss: 0.9753, Test_acc: 0.6960\n",
      "Epoch: 168, Train_loss: 0.8055, Train_acc: 0.7373, Test_loss: 0.9736, Test_acc: 0.6977\n",
      "Epoch: 169, Train_loss: 0.8029, Train_acc: 0.7379, Test_loss: 0.9715, Test_acc: 0.6989\n",
      "Epoch: 170, Train_loss: 0.8022, Train_acc: 0.7380, Test_loss: 0.9702, Test_acc: 0.6975\n",
      "Epoch: 171, Train_loss: 0.8042, Train_acc: 0.7361, Test_loss: 0.9748, Test_acc: 0.6970\n",
      "Epoch: 172, Train_loss: 0.8029, Train_acc: 0.7381, Test_loss: 0.9754, Test_acc: 0.6977\n",
      "Epoch: 173, Train_loss: 0.8045, Train_acc: 0.7367, Test_loss: 0.9760, Test_acc: 0.6975\n",
      "Epoch: 174, Train_loss: 0.8009, Train_acc: 0.7391, Test_loss: 0.9731, Test_acc: 0.6975\n",
      "Epoch: 175, Train_loss: 0.8014, Train_acc: 0.7385, Test_loss: 0.9754, Test_acc: 0.6974\n",
      "Epoch: 176, Train_loss: 0.8007, Train_acc: 0.7378, Test_loss: 0.9786, Test_acc: 0.6969\n",
      "Epoch: 177, Train_loss: 0.8005, Train_acc: 0.7395, Test_loss: 0.9752, Test_acc: 0.6986\n",
      "Epoch: 178, Train_loss: 0.7990, Train_acc: 0.7392, Test_loss: 0.9775, Test_acc: 0.6990\n",
      "Epoch: 179, Train_loss: 0.8026, Train_acc: 0.7369, Test_loss: 0.9800, Test_acc: 0.6983\n",
      "Epoch: 180, Train_loss: 0.8022, Train_acc: 0.7370, Test_loss: 0.9788, Test_acc: 0.6963\n",
      "Epoch: 181, Train_loss: 0.8004, Train_acc: 0.7381, Test_loss: 0.9763, Test_acc: 0.6996\n",
      "Epoch: 182, Train_loss: 0.7993, Train_acc: 0.7390, Test_loss: 0.9755, Test_acc: 0.6977\n",
      "Epoch: 183, Train_loss: 0.7987, Train_acc: 0.7385, Test_loss: 0.9788, Test_acc: 0.6969\n",
      "Epoch: 184, Train_loss: 0.7962, Train_acc: 0.7399, Test_loss: 0.9724, Test_acc: 0.6983\n",
      "Epoch: 185, Train_loss: 0.7980, Train_acc: 0.7395, Test_loss: 0.9771, Test_acc: 0.6990\n",
      "Epoch: 186, Train_loss: 0.7958, Train_acc: 0.7391, Test_loss: 0.9763, Test_acc: 0.6975\n",
      "Epoch: 187, Train_loss: 0.7987, Train_acc: 0.7381, Test_loss: 0.9760, Test_acc: 0.6976\n",
      "Epoch: 188, Train_loss: 0.7944, Train_acc: 0.7406, Test_loss: 0.9774, Test_acc: 0.6994\n",
      "Epoch: 189, Train_loss: 0.7938, Train_acc: 0.7413, Test_loss: 0.9748, Test_acc: 0.6998\n",
      "Epoch: 190, Train_loss: 0.7988, Train_acc: 0.7393, Test_loss: 0.9793, Test_acc: 0.6980\n",
      "Epoch: 191, Train_loss: 0.7946, Train_acc: 0.7401, Test_loss: 0.9771, Test_acc: 0.6989\n",
      "Epoch: 192, Train_loss: 0.7943, Train_acc: 0.7408, Test_loss: 0.9760, Test_acc: 0.6976\n",
      "Epoch: 193, Train_loss: 0.7969, Train_acc: 0.7387, Test_loss: 0.9769, Test_acc: 0.6981\n",
      "Epoch: 194, Train_loss: 0.7950, Train_acc: 0.7409, Test_loss: 0.9773, Test_acc: 0.6990\n",
      "Epoch: 195, Train_loss: 0.7937, Train_acc: 0.7404, Test_loss: 0.9745, Test_acc: 0.6978\n",
      "Epoch: 196, Train_loss: 0.7922, Train_acc: 0.7412, Test_loss: 0.9783, Test_acc: 0.6974\n",
      "Epoch: 197, Train_loss: 0.7907, Train_acc: 0.7420, Test_loss: 0.9761, Test_acc: 0.6970\n",
      "Epoch: 198, Train_loss: 0.7938, Train_acc: 0.7402, Test_loss: 0.9781, Test_acc: 0.6989\n",
      "Epoch: 199, Train_loss: 0.7911, Train_acc: 0.7430, Test_loss: 0.9766, Test_acc: 0.6969\n",
      "Epoch: 200, Train_loss: 0.7911, Train_acc: 0.7431, Test_loss: 0.9792, Test_acc: 0.7000\n",
      "Model saved at ./models/ft_checkpoints/model_random.pt.\n",
      ">>> Fine-tuning done!\n",
      ">>> Training model_target_only\n",
      "Epoch: 001, Train_loss: 0.5923, Train_acc: 0.8231, Test_loss: 0.5925, Test_acc: 0.8221\n",
      "Epoch: 002, Train_loss: 0.5094, Train_acc: 0.8433, Test_loss: 0.5217, Test_acc: 0.8425\n",
      "Epoch: 003, Train_loss: 0.4327, Train_acc: 0.8623, Test_loss: 0.4620, Test_acc: 0.8560\n",
      "Epoch: 004, Train_loss: 0.3900, Train_acc: 0.8741, Test_loss: 0.4405, Test_acc: 0.8558\n",
      "Epoch: 005, Train_loss: 0.3551, Train_acc: 0.8836, Test_loss: 0.4284, Test_acc: 0.8616\n",
      "Epoch: 006, Train_loss: 0.3253, Train_acc: 0.8949, Test_loss: 0.4083, Test_acc: 0.8679\n",
      "Epoch: 007, Train_loss: 0.3108, Train_acc: 0.8979, Test_loss: 0.4131, Test_acc: 0.8645\n",
      "Epoch: 008, Train_loss: 0.2911, Train_acc: 0.9050, Test_loss: 0.4048, Test_acc: 0.8695\n",
      "Epoch: 009, Train_loss: 0.2598, Train_acc: 0.9137, Test_loss: 0.4062, Test_acc: 0.8743\n",
      "Epoch: 010, Train_loss: 0.2423, Train_acc: 0.9208, Test_loss: 0.4025, Test_acc: 0.8747\n",
      "Epoch: 011, Train_loss: 0.2376, Train_acc: 0.9212, Test_loss: 0.4152, Test_acc: 0.8735\n",
      "Epoch: 012, Train_loss: 0.2314, Train_acc: 0.9230, Test_loss: 0.4363, Test_acc: 0.8689\n",
      "Epoch: 013, Train_loss: 0.2037, Train_acc: 0.9313, Test_loss: 0.4412, Test_acc: 0.8728\n",
      "Epoch: 014, Train_loss: 0.1941, Train_acc: 0.9355, Test_loss: 0.4380, Test_acc: 0.8718\n",
      "Epoch: 015, Train_loss: 0.1813, Train_acc: 0.9397, Test_loss: 0.4425, Test_acc: 0.8700\n",
      "Epoch: 016, Train_loss: 0.1635, Train_acc: 0.9461, Test_loss: 0.4691, Test_acc: 0.8751\n",
      "Epoch: 017, Train_loss: 0.1769, Train_acc: 0.9400, Test_loss: 0.4925, Test_acc: 0.8669\n",
      "Epoch: 018, Train_loss: 0.1527, Train_acc: 0.9489, Test_loss: 0.4952, Test_acc: 0.8687\n",
      "Epoch: 019, Train_loss: 0.1308, Train_acc: 0.9581, Test_loss: 0.4973, Test_acc: 0.8708\n",
      "Epoch: 020, Train_loss: 0.1327, Train_acc: 0.9555, Test_loss: 0.5378, Test_acc: 0.8686\n",
      "Epoch: 021, Train_loss: 0.1277, Train_acc: 0.9570, Test_loss: 0.5452, Test_acc: 0.8680\n",
      "Epoch: 022, Train_loss: 0.1110, Train_acc: 0.9634, Test_loss: 0.5654, Test_acc: 0.8708\n",
      "Epoch: 023, Train_loss: 0.1190, Train_acc: 0.9599, Test_loss: 0.5831, Test_acc: 0.8650\n",
      "Epoch: 024, Train_loss: 0.1046, Train_acc: 0.9649, Test_loss: 0.6069, Test_acc: 0.8701\n",
      "Epoch: 025, Train_loss: 0.1030, Train_acc: 0.9658, Test_loss: 0.6243, Test_acc: 0.8618\n",
      "Epoch: 026, Train_loss: 0.0875, Train_acc: 0.9709, Test_loss: 0.6687, Test_acc: 0.8672\n",
      "Epoch: 027, Train_loss: 0.0834, Train_acc: 0.9715, Test_loss: 0.7219, Test_acc: 0.8648\n",
      "Epoch: 028, Train_loss: 0.0802, Train_acc: 0.9733, Test_loss: 0.7279, Test_acc: 0.8650\n",
      "Epoch: 029, Train_loss: 0.0812, Train_acc: 0.9723, Test_loss: 0.7362, Test_acc: 0.8661\n",
      "Epoch: 030, Train_loss: 0.0887, Train_acc: 0.9689, Test_loss: 0.7517, Test_acc: 0.8650\n",
      "Epoch: 031, Train_loss: 0.0751, Train_acc: 0.9744, Test_loss: 0.8095, Test_acc: 0.8641\n",
      "Epoch: 032, Train_loss: 0.0998, Train_acc: 0.9656, Test_loss: 0.8877, Test_acc: 0.8647\n",
      "Epoch: 033, Train_loss: 0.0725, Train_acc: 0.9749, Test_loss: 0.8310, Test_acc: 0.8622\n",
      "Epoch: 034, Train_loss: 0.0662, Train_acc: 0.9781, Test_loss: 0.8533, Test_acc: 0.8605\n",
      "Epoch: 035, Train_loss: 0.0650, Train_acc: 0.9779, Test_loss: 0.8746, Test_acc: 0.8621\n",
      "Epoch: 036, Train_loss: 0.0585, Train_acc: 0.9809, Test_loss: 0.8906, Test_acc: 0.8615\n",
      "Epoch: 037, Train_loss: 0.0566, Train_acc: 0.9815, Test_loss: 0.9065, Test_acc: 0.8640\n",
      "Epoch: 038, Train_loss: 0.0577, Train_acc: 0.9806, Test_loss: 0.9799, Test_acc: 0.8576\n",
      "Epoch: 039, Train_loss: 0.0670, Train_acc: 0.9772, Test_loss: 0.9833, Test_acc: 0.8610\n",
      "Epoch: 040, Train_loss: 0.0495, Train_acc: 0.9838, Test_loss: 1.0246, Test_acc: 0.8630\n",
      "Epoch: 041, Train_loss: 0.0550, Train_acc: 0.9821, Test_loss: 1.0196, Test_acc: 0.8627\n",
      "Epoch: 042, Train_loss: 0.0465, Train_acc: 0.9838, Test_loss: 1.0432, Test_acc: 0.8625\n",
      "Epoch: 043, Train_loss: 0.0405, Train_acc: 0.9866, Test_loss: 1.0772, Test_acc: 0.8626\n",
      "Epoch: 044, Train_loss: 0.0528, Train_acc: 0.9817, Test_loss: 1.0601, Test_acc: 0.8589\n",
      "Epoch: 045, Train_loss: 0.0588, Train_acc: 0.9799, Test_loss: 1.0830, Test_acc: 0.8602\n",
      "Epoch: 046, Train_loss: 0.0544, Train_acc: 0.9816, Test_loss: 1.1148, Test_acc: 0.8617\n",
      "Epoch: 047, Train_loss: 0.0550, Train_acc: 0.9820, Test_loss: 1.1916, Test_acc: 0.8597\n",
      "Epoch: 048, Train_loss: 0.0642, Train_acc: 0.9785, Test_loss: 1.2063, Test_acc: 0.8607\n",
      "Epoch: 049, Train_loss: 0.0316, Train_acc: 0.9900, Test_loss: 1.1531, Test_acc: 0.8647\n",
      "Epoch: 050, Train_loss: 0.0336, Train_acc: 0.9897, Test_loss: 1.1508, Test_acc: 0.8603\n",
      "Epoch: 051, Train_loss: 0.0438, Train_acc: 0.9862, Test_loss: 1.1808, Test_acc: 0.8603\n",
      "Epoch: 052, Train_loss: 0.0572, Train_acc: 0.9804, Test_loss: 1.2691, Test_acc: 0.8588\n",
      "Epoch: 053, Train_loss: 0.0398, Train_acc: 0.9873, Test_loss: 1.2634, Test_acc: 0.8601\n",
      "Epoch: 054, Train_loss: 0.0695, Train_acc: 0.9772, Test_loss: 1.3676, Test_acc: 0.8571\n",
      "Epoch: 055, Train_loss: 0.0418, Train_acc: 0.9863, Test_loss: 1.2629, Test_acc: 0.8602\n",
      "Epoch: 056, Train_loss: 0.0311, Train_acc: 0.9901, Test_loss: 1.3097, Test_acc: 0.8590\n",
      "Epoch: 057, Train_loss: 0.0308, Train_acc: 0.9898, Test_loss: 1.3140, Test_acc: 0.8611\n",
      "Epoch: 058, Train_loss: 0.0425, Train_acc: 0.9856, Test_loss: 1.2487, Test_acc: 0.8618\n",
      "Epoch: 059, Train_loss: 0.0611, Train_acc: 0.9802, Test_loss: 1.3898, Test_acc: 0.8558\n",
      "Epoch: 060, Train_loss: 0.0421, Train_acc: 0.9863, Test_loss: 1.3979, Test_acc: 0.8606\n",
      "Epoch: 061, Train_loss: 0.0366, Train_acc: 0.9881, Test_loss: 1.3928, Test_acc: 0.8595\n",
      "Epoch: 062, Train_loss: 0.0418, Train_acc: 0.9867, Test_loss: 1.3332, Test_acc: 0.8602\n",
      "Epoch: 063, Train_loss: 0.0182, Train_acc: 0.9948, Test_loss: 1.3764, Test_acc: 0.8606\n",
      "Epoch: 064, Train_loss: 0.0362, Train_acc: 0.9878, Test_loss: 1.4743, Test_acc: 0.8601\n",
      "Epoch: 065, Train_loss: 0.0360, Train_acc: 0.9879, Test_loss: 1.4824, Test_acc: 0.8575\n",
      "Epoch: 066, Train_loss: 0.0302, Train_acc: 0.9900, Test_loss: 1.4913, Test_acc: 0.8623\n",
      "Epoch: 067, Train_loss: 0.0446, Train_acc: 0.9854, Test_loss: 1.4891, Test_acc: 0.8589\n",
      "Epoch: 068, Train_loss: 0.0288, Train_acc: 0.9907, Test_loss: 1.4903, Test_acc: 0.8612\n",
      "Epoch: 069, Train_loss: 0.0184, Train_acc: 0.9946, Test_loss: 1.4639, Test_acc: 0.8618\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-be77ed673917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Train model_target_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>> Training model_target_only'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_target_only_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_target_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model_target_only.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/shared/chao/transfer_fl/TranferableFL/src/trainers/finetune.py\u001b[0m in \u001b[0;36mft_train\u001b[0;34m(model, options, device, train_loader, test_loader, checkpoint_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Get train stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Get test stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/shared/chao/transfer_fl/TranferableFL/src/trainers/finetune.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, device, data_loader, criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = './models/lenet_mnist_20230914223349.pt'\n",
    "flat_model_params = torch.load(model_path)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "options['device'] = 'cuda:0'\n",
    "options['ft_epochs'] = 200\n",
    "options['ft_batch_size'] = 128\n",
    "options['ft_lr'] = 1e-3\n",
    "options['ft_wd'] = 0\n",
    "options['last_k'] = 1\n",
    "options['early_stopping'] = 10\n",
    "\n",
    "for _ in range(10):\n",
    "    # Initialize new models\n",
    "    model_source_only = choose_model(options)  # baseline: lower bound (f on source, g on source)\n",
    "    model_ft = choose_model(options)  # baseline: standard fine-tune (f on source, g on target)\n",
    "    model_target_only = choose_model(options)  # baseline: upper bound (f on target, g on target)\n",
    "    model_random = choose_model(options)  # baseline: lower bound (f random, g on target)\n",
    "\n",
    "    # Assign model params\n",
    "    set_flat_params_to(model_source_only, flat_model_params)\n",
    "    set_flat_params_to(model_ft, flat_model_params)\n",
    "\n",
    "    # Now model is set with flat_model_params\n",
    "    # Start fine-tuning below\n",
    "    # First, freeze all but last k fc layers\n",
    "    freeze(model_random, options['last_k'])\n",
    "    freeze(model_ft, options['last_k'])\n",
    "\n",
    "    # load the fine-tuning dataset\n",
    "    ft_train_loader, ft_test_loader = get_loader('./data/mnist_m', options['ft_dataset'], options['ft_batch_size'], num_workers=16)\n",
    "    checkpoint_prefix = f'./models/ft_checkpoints/'\n",
    "\n",
    "    # Train model_target_only\n",
    "    print('>>> Training model_target_only')\n",
    "    _, model_target_only_results = ft_train(model_target_only, options, options['device'], ft_train_loader, ft_test_loader, checkpoint_prefix + 'model_target_only.pt')\n",
    "\n",
    "    # fine-tuning\n",
    "    print('>>> Training model_ft')\n",
    "    _, model_ft_results = ft_train(model_ft, options, options['device'], ft_train_loader, ft_test_loader, checkpoint_prefix + 'model_ft.pt')\n",
    "\n",
    "    # fine-tuning random model\n",
    "    print('>>> Training model_random')\n",
    "    _, model_random_results = ft_train(model_random, options, options['device'], ft_train_loader, ft_test_loader, checkpoint_prefix + 'model_random.pt')\n",
    "\n",
    "    # evaluate model_source_only\n",
    "#     print('>>> Evaluating model_source_only')\n",
    "#     model_source_only = model_source_only.to(options['device'])\n",
    "#     model_source_only_results = [0., 0., 0., 0.]\n",
    "#     model_source_only_results[0], model_source_only_results[1] = eval(model_source_only, options['device'],\n",
    "#                                                                       ft_train_loader, criterion=criterion)\n",
    "#     model_source_only_results[2], model_source_only_results[3] = eval(model_source_only, options['device'],\n",
    "#                                                                       ft_test_loader, criterion=criterion)\n",
    "\n",
    "#     print(f'model_target_only: {model_target_only_results}')\n",
    "#     print(f'model_ft: {model_ft_results}')\n",
    "#     print(f'model_random: {model_random_results}')\n",
    "#     print(f'model_source_only: {model_source_only_results}')\n",
    "#     print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "465b3c36-3c42-40b7-999e-d5e451f1508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 20230912113952\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "uid = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "print(type(uid), uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ba6c08-8888-474e-ad34-275fd529bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59001, 28, 28])\n",
      "tensor(0.4549)\n",
      "tensor(0.2208)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "        transforms.Resize(28),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale()\n",
    "    ])\n",
    "\n",
    "train_list = './data/mnist_m/mnist_m_train_labels.txt'\n",
    "\n",
    "with open(train_list, 'r') as f:\n",
    "    data_list = f.readlines()\n",
    "\n",
    "img_paths = []\n",
    "for data in data_list:\n",
    "    img_paths.append(data[:-3])\n",
    "\n",
    "data = []\n",
    "\n",
    "for img_path in img_paths:\n",
    "    img = Image.open(f'./data/mnist_m/mnist_m_train/{img_path}').convert('RGB')\n",
    "    img = img_transform(img)\n",
    "    data.append(img)\n",
    "\n",
    "data = torch.cat(data, dim=0)\n",
    "print(data.shape)\n",
    "print(data.float().mean())\n",
    "print(data.float().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f52ec640-9893-418d-9653-5044a621f4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302461d-9d53-45b8-91e6-45f2873ca33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
