{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6e4e19-39e2-4593-85de-00e94803f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import warnings\n",
    "import importlib\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from data.finetune.ft_loader import get_loader\n",
    "from src.utils.worker_utils import read_data\n",
    "from src.models.model import choose_model\n",
    "from src.trainers.finetune import ft_train, eval\n",
    "from config import OPTIMIZERS, DATASETS, MODEL_PARAMS, TRAINERS\n",
    "from src.utils.torch_utils import get_flat_grad, get_state_dict, get_flat_params_from, set_flat_params_to\n",
    "from pyhessian import hessian\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c96043-3542-42fc-8901-c42463696794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hessian(model, data_loader, criterion, device, compute_alpha):\n",
    "    # only compute alpha (spectral norm of hessian) when the estimate is unknown\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_num = 0\n",
    "    alpha_list = []\n",
    "    for inputs, targets in data_loader:\n",
    "        # we use cuda to make the computation fast\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, targets)\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        total_num += targets.size(0)\n",
    "        \n",
    "        if compute_alpha:\n",
    "            # create the hessian computation module\n",
    "            hessian_comp = hessian(model, criterion, data=(inputs, targets), cuda=True)\n",
    "            top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues()\n",
    "            alpha_list.append(top_eigenvalues[-1])\n",
    "    \n",
    "    total_loss /= total_num\n",
    "    if len(alpha_list) == 0:\n",
    "        alpha = 0.0\n",
    "    else:\n",
    "        alpha = max(alpha_list)\n",
    "    return total_loss, alpha\n",
    "\n",
    "\n",
    "def freeze(model, k):\n",
    "    # only fine-tune the last k fc layers (if there are more than k fc layers)\n",
    "    num_layer = 0\n",
    "    for mod in model.children():\n",
    "        for params in mod.parameters():\n",
    "            params.requires_grad = False\n",
    "        num_layer += 1\n",
    "\n",
    "    for mod in model.children():\n",
    "        num_layer -= 1\n",
    "        if num_layer <= k and isinstance(mod, torch.nn.Linear):\n",
    "            for params in mod.parameters():\n",
    "                params.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087e3519-5c59-42a6-8822-827011ad50e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      ">>> Arguments:\n",
      "\t             algo : fedavgtl\n",
      "\t            alpha : 51871.06640625\n",
      "\t       batch_size : 64\n",
      "\tclients_per_round : 100\n",
      "\t          dataset : mnist_all_data_1_equal_niid\n",
      "\t           device : cuda:0\n",
      "\t              dis : \n",
      "\t       eval_every : 5\n",
      "\t    ft_batch_size : 128\n",
      "\t       ft_dataset : mnist-m\n",
      "\t        ft_epochs : 10\n",
      "\t            ft_lr : 0.01\n",
      "\t            ft_wd : 0\n",
      "\t              gpu : True\n",
      "\t      input_shape : (1, 28, 28)\n",
      "\t           last_k : 2\n",
      "\t               lr : 0.01\n",
      "\t            model : lenet\n",
      "\t           n_init : 10\n",
      "\t        noaverage : False\n",
      "\t          noprint : False\n",
      "\t        num_class : 10\n",
      "\t        num_epoch : 1\n",
      "\t        num_round : 100\n",
      "\t           opt_lr : False\n",
      "\t            reg_J : True\n",
      "\t       reg_J_coef : 0.001\n",
      "\t             seed : 0\n",
      "\t               wd : 0.0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--algo',\n",
    "                    help='name of trainer;',\n",
    "                    type=str,\n",
    "                    choices=OPTIMIZERS,\n",
    "                    default='fedavgtl')\n",
    "parser.add_argument('--dataset',\n",
    "                    help='name of dataset;',\n",
    "                    type=str,\n",
    "                    default='mnist_all_data_1_equal_niid')\n",
    "parser.add_argument('--model',\n",
    "                    help='name of model;',\n",
    "                    type=str,\n",
    "                    default='lenet')\n",
    "parser.add_argument('--wd',\n",
    "                    help='weight decay parameter;',\n",
    "                    type=float,\n",
    "                    default=0.0)\n",
    "parser.add_argument('--lr',\n",
    "                    help='learning rate for inner solver;',\n",
    "                    type=float,\n",
    "                    default=0.01)\n",
    "parser.add_argument('--gpu',\n",
    "                    action='store_true',\n",
    "                    default=True,\n",
    "                    help='use gpu (default: False)')\n",
    "parser.add_argument('--noprint',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='whether to print inner result (default: False)')\n",
    "parser.add_argument('--noaverage',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='whether to only average local solutions (default: False)')\n",
    "parser.add_argument('--device',\n",
    "                    help='selected CUDA device',\n",
    "                    default='0',\n",
    "                    type=str)\n",
    "parser.add_argument('--num_round',\n",
    "                    help='number of rounds to simulate;',\n",
    "                    type=int,\n",
    "                    default=100)\n",
    "parser.add_argument('--eval_every',\n",
    "                    help='evaluate every ____ rounds;',\n",
    "                    type=int,\n",
    "                    default=5)\n",
    "parser.add_argument('--clients_per_round',\n",
    "                    help='number of clients trained per round;',\n",
    "                    type=int,\n",
    "                    default=100)\n",
    "parser.add_argument('--batch_size',\n",
    "                    help='batch size when clients train on data;',\n",
    "                    type=int,\n",
    "                    default=64)\n",
    "parser.add_argument('--num_epoch',\n",
    "                    help='number of epochs when clients train on data;',\n",
    "                    type=int,\n",
    "                    default=1)\n",
    "parser.add_argument('--seed',\n",
    "                    help='seed for randomness;',\n",
    "                    type=int,\n",
    "                    default=0)\n",
    "parser.add_argument('--dis',\n",
    "                    help='add more information;',\n",
    "                    type=str,\n",
    "                    default='')\n",
    "parser.add_argument('--opt_lr',\n",
    "                    help='flag for optimizing local learning rate at each round (default: False);',\n",
    "                    action='store_true',\n",
    "                    default=False)\n",
    "parser.add_argument('--reg_J',\n",
    "                    help='flag for regularizing Jacobian (default: False);',\n",
    "                    action='store_true',\n",
    "                    default=True)\n",
    "parser.add_argument('--reg_J_coef',\n",
    "                    help='coefficient for regularization on Jacobian;',\n",
    "                    type=float,\n",
    "                    default=1e-3)\n",
    "parser.add_argument('--ft_dataset',\n",
    "                    help='dataset for fine-tuning;',\n",
    "                    type=str,\n",
    "                    default='mnist-m')\n",
    "parser.add_argument('--ft_epochs',\n",
    "                    help='epochs for fine-tuning;',\n",
    "                    type=int,\n",
    "                    default=10)\n",
    "parser.add_argument('--ft_batch_size',\n",
    "                    help='batch size for fine-tuning;',\n",
    "                    type=int,\n",
    "                    default=128)\n",
    "parser.add_argument('--ft_lr',\n",
    "                    help='learning rate for fine-tuning;',\n",
    "                    type=float,\n",
    "                    default=0.01)\n",
    "parser.add_argument('--ft_wd',\n",
    "                    help='weight decay for fine-tuning;',\n",
    "                    type=float,\n",
    "                    default=0)\n",
    "parser.add_argument('--n_init',\n",
    "                    help='number of initial models to consider;',\n",
    "                    type=int,\n",
    "                    default=10)\n",
    "parser.add_argument('--alpha',\n",
    "                    help='estimate of lipschitz continuous gradient constant (0 means no estimate yet);',\n",
    "                    type=float,\n",
    "                    default=51871.06640625)\n",
    "parser.add_argument('--last_k',\n",
    "                    help='number of fc layers to fine-tune;',\n",
    "                    type=int,\n",
    "                    default=2)\n",
    "\n",
    "parsed = parser.parse_args([])\n",
    "options = parsed.__dict__\n",
    "options['gpu'] = options['gpu'] and torch.cuda.is_available()\n",
    "\n",
    "if not options['gpu']:\n",
    "    options['device'] = 'cpu'\n",
    "else:\n",
    "    options['device'] = 'cuda:' + options['device']\n",
    "print('Using device: ' + options['device'])\n",
    "\n",
    "# read data\n",
    "idx = options['dataset'].find(\"_\")\n",
    "if idx != -1:\n",
    "    dataset_name, sub_data = options['dataset'][:idx], options['dataset'][idx+1:]\n",
    "else:\n",
    "    dataset_name, sub_data = options['dataset'], None\n",
    "assert dataset_name in DATASETS, \"{} not in dataset {}!\".format(dataset_name, DATASETS)\n",
    "\n",
    "# Add model arguments\n",
    "options.update(MODEL_PARAMS(dataset_name, options['model']))\n",
    "\n",
    "# Load selected trainer\n",
    "trainer_path = 'src.trainers.%s' % options['algo']\n",
    "mod = importlib.import_module(trainer_path)\n",
    "trainer_class = getattr(mod, TRAINERS[options['algo']])\n",
    "\n",
    "# Print arguments and return\n",
    "max_length = max([len(key) for key in options.keys()])\n",
    "fmt_string = '\\t%' + str(max_length) + 's : %s'\n",
    "print('>>> Arguments:')\n",
    "for keyPair in sorted(options.items()):\n",
    "    print(fmt_string % keyPair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a75c4d-2113-4a25-9f01-3844e450a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Read data from:\n",
      "     ./data/mnist/data/all_data_1.pkl\n",
      ">>> The estimate of constant alpha is 51.87106640625.\n"
     ]
    }
   ],
   "source": [
    "# first estimate the value for alpha\n",
    "data_all_path = os.path.join('./data', dataset_name, 'data')\n",
    "_, _, data_all, _ = read_data(train_data_dir=data_all_path, key='all_data_1.pkl')\n",
    "data_all = data_all[0]\n",
    "data_all_loader = DataLoader(data_all, batch_size=1024, shuffle=False)\n",
    "\n",
    "# initialize several models to compute the lowest initial loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "best_alpha = 0 if options['alpha'] == 0 else options['alpha']\n",
    "compute_alpha = True if options['alpha'] == 0 else False\n",
    "start = time.time()\n",
    "\n",
    "for i_init in range(options['n_init']):\n",
    "    # randomly initialize a new model\n",
    "    random_model = choose_model(options)\n",
    "    i_loss, i_alpha = eval_hessian(random_model, data_all_loader, criterion, options['device'], compute_alpha)\n",
    "    # save the best model so far\n",
    "    if i_loss < best_loss:\n",
    "        best_loss = i_loss\n",
    "        best_model = random_model\n",
    "\n",
    "    if compute_alpha:\n",
    "        best_alpha = max(best_alpha, i_alpha)\n",
    "\n",
    "best_model_init = get_flat_params_from(best_model).detach()\n",
    "options['alpha'] = best_alpha\n",
    "options['model_init'] = best_model_init\n",
    "print(f'>>> The estimate of constant alpha is {best_alpha}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42633aa8-2e52-4f89-8cb6-5a331323c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "np.random.seed(1 + options['seed'])\n",
    "torch.manual_seed(12 + options['seed'])\n",
    "if options['gpu']:\n",
    "    torch.cuda.manual_seed_all(123 + options['seed'])\n",
    "\n",
    "train_path = os.path.join('./data', dataset_name, 'data', 'train')\n",
    "test_path = os.path.join('./data', dataset_name, 'data', 'test')\n",
    "\n",
    "# `dataset` is a tuple like (cids, groups, train_data, test_data)\n",
    "all_data_info = read_data(train_path, test_path, sub_data)\n",
    "\n",
    "# Call appropriate trainer\n",
    "trainer = trainer_class(options, all_data_info)\n",
    "trainer.train()\n",
    "\n",
    "# FL training finish here, save the latest server model\n",
    "flat_model_params = trainer.latest_model\n",
    "model_path = f\"./models/{options['model']}_{dataset_name}_{options['algo']}.pt\"\n",
    "torch.save(flat_model_params, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35f4951-8598-466e-8cd7-573acd2810e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training model_target_only\n",
      "Epoch: 001, Train_loss: 1.2227, Train_acc: 0.6084, Test_loss: 1.2163, Test_acc: 0.6108\n",
      "Epoch: 002, Train_loss: 1.0485, Train_acc: 0.6744, Test_loss: 1.0463, Test_acc: 0.6768\n",
      "Epoch: 003, Train_loss: 0.9168, Train_acc: 0.7105, Test_loss: 0.9224, Test_acc: 0.7096\n",
      "Epoch: 004, Train_loss: 1.0087, Train_acc: 0.6950, Test_loss: 1.0185, Test_acc: 0.6921\n",
      "Epoch: 005, Train_loss: 0.9355, Train_acc: 0.7056, Test_loss: 0.9493, Test_acc: 0.7057\n",
      "Epoch: 006, Train_loss: 0.8694, Train_acc: 0.7296, Test_loss: 0.8747, Test_acc: 0.7228\n",
      "Epoch: 007, Train_loss: 0.8733, Train_acc: 0.7270, Test_loss: 0.8871, Test_acc: 0.7248\n",
      "Epoch: 008, Train_loss: 0.8907, Train_acc: 0.7196, Test_loss: 0.8908, Test_acc: 0.7213\n",
      "Epoch: 009, Train_loss: 0.8679, Train_acc: 0.7301, Test_loss: 0.8816, Test_acc: 0.7291\n",
      "Epoch: 010, Train_loss: 0.8503, Train_acc: 0.7410, Test_loss: 0.8832, Test_acc: 0.7343\n",
      ">>> Fine-tuning done!\n",
      ">>> Training model_ft\n",
      "Epoch: 001, Train_loss: 1.5132, Train_acc: 0.4746, Test_loss: 1.5383, Test_acc: 0.4697\n",
      "Epoch: 002, Train_loss: 1.4389, Train_acc: 0.4990, Test_loss: 1.4927, Test_acc: 0.4845\n",
      "Epoch: 003, Train_loss: 1.4122, Train_acc: 0.5084, Test_loss: 1.5039, Test_acc: 0.4873\n",
      "Epoch: 004, Train_loss: 1.4026, Train_acc: 0.5140, Test_loss: 1.5071, Test_acc: 0.4887\n",
      "Epoch: 005, Train_loss: 1.3782, Train_acc: 0.5122, Test_loss: 1.4992, Test_acc: 0.4774\n",
      "Epoch: 006, Train_loss: 1.3431, Train_acc: 0.5313, Test_loss: 1.4979, Test_acc: 0.4935\n",
      "Epoch: 007, Train_loss: 1.3532, Train_acc: 0.5299, Test_loss: 1.5148, Test_acc: 0.4965\n",
      "Epoch: 008, Train_loss: 1.3224, Train_acc: 0.5363, Test_loss: 1.5074, Test_acc: 0.4914\n",
      "Epoch: 009, Train_loss: 1.3171, Train_acc: 0.5402, Test_loss: 1.5300, Test_acc: 0.4931\n",
      "Epoch: 010, Train_loss: 1.3339, Train_acc: 0.5355, Test_loss: 1.5358, Test_acc: 0.4953\n",
      ">>> Fine-tuning done!\n",
      ">>> Training model_random\n",
      "Epoch: 001, Train_loss: 1.8656, Train_acc: 0.3066, Test_loss: 1.8585, Test_acc: 0.3135\n",
      "Epoch: 002, Train_loss: 1.7497, Train_acc: 0.3571, Test_loss: 1.7284, Test_acc: 0.3573\n",
      "Epoch: 003, Train_loss: 1.7537, Train_acc: 0.3700, Test_loss: 1.7366, Test_acc: 0.3754\n",
      "Epoch: 004, Train_loss: 1.6957, Train_acc: 0.3821, Test_loss: 1.6863, Test_acc: 0.3837\n",
      "Epoch: 005, Train_loss: 1.7571, Train_acc: 0.3498, Test_loss: 1.7479, Test_acc: 0.3567\n",
      "Epoch: 006, Train_loss: 1.7056, Train_acc: 0.3939, Test_loss: 1.6897, Test_acc: 0.3945\n",
      "Epoch: 007, Train_loss: 1.6931, Train_acc: 0.3950, Test_loss: 1.6825, Test_acc: 0.3985\n",
      "Epoch: 008, Train_loss: 1.7002, Train_acc: 0.3912, Test_loss: 1.6838, Test_acc: 0.3904\n",
      "Epoch: 009, Train_loss: 1.6409, Train_acc: 0.4152, Test_loss: 1.6294, Test_acc: 0.4190\n",
      "Epoch: 010, Train_loss: 1.6302, Train_acc: 0.4161, Test_loss: 1.6221, Test_acc: 0.4155\n",
      ">>> Fine-tuning done!\n",
      ">>> Evaluating model_source_only\n",
      "model_target_only: tensor([0.8503, 0.7410, 0.8832, 0.7343])\n",
      "model_ft: tensor([1.3339, 0.5355, 1.5358, 0.4953])\n",
      "model_random: tensor([1.6302, 0.4161, 1.6221, 0.4155])\n",
      "model_source_only: [2.2967622668251635, 0.12218436975644481, 2.2957382774289457, 0.12620819908899011]\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning part\n",
    "# load the previous model\n",
    "model_path = f\"./models/{options['model']}_{dataset_name}_{options['algo']}.pt\"\n",
    "flat_model_params = torch.load(model_path)\n",
    "\n",
    "# Initialize new models\n",
    "model_source_only = choose_model(options)  # baseline: lower bound (f on source, g on source)\n",
    "model_ft = choose_model(options)  # baseline: standard fine-tune (f on source, g on target)\n",
    "model_target_only = choose_model(options)  # baseline: upper bound (f on target, g on target)\n",
    "model_random = choose_model(options)  # baseline: lower bound (f random, g on target)\n",
    "\n",
    "# Assign model params\n",
    "set_flat_params_to(model_source_only, flat_model_params)\n",
    "set_flat_params_to(model_ft, flat_model_params)\n",
    "\n",
    "# Now model is set with flat_model_params\n",
    "# Start fine-tuning below\n",
    "# First, freeze all but last layer\n",
    "freeze(model_random, options['last_k'])\n",
    "freeze(model_ft, options['last_k'])\n",
    "\n",
    "# # load the fine-tuning dataset\n",
    "ft_train_loader, ft_test_loader = get_loader('./data/mnist_m', options['ft_dataset'], options['ft_batch_size'], num_workers=16)\n",
    "\n",
    "# Train model_target_only\n",
    "print('>>> Training model_target_only')\n",
    "model_target_only_results = ft_train(model_target_only, options, options['device'], ft_train_loader, ft_test_loader)\n",
    "\n",
    "# fine-tuning\n",
    "print('>>> Training model_ft')\n",
    "model_ft_results = ft_train(model_ft, options, options['device'], ft_train_loader, ft_test_loader)\n",
    "\n",
    "# fine-tuning random model\n",
    "print('>>> Training model_random')\n",
    "model_random_results = ft_train(model_random, options, options['device'], ft_train_loader, ft_test_loader)\n",
    "\n",
    "# evaluate model_source_only\n",
    "print('>>> Evaluating model_source_only')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model_source_only = model_source_only.to(options['device'])\n",
    "model_source_only_results = [0., 0., 0., 0.]\n",
    "model_source_only_results[0], model_source_only_results[1] = eval(model_source_only, options['device'],\n",
    "                                                                  ft_train_loader, criterion=criterion)\n",
    "model_source_only_results[2], model_source_only_results[3] = eval(model_source_only, options['device'],\n",
    "                                                                  ft_test_loader, criterion=criterion)\n",
    "\n",
    "print(f'model_target_only: {model_target_only_results[-1]}')\n",
    "print(f'model_ft: {model_ft_results[-1]}')\n",
    "print(f'model_random: {model_random_results[-1]}')\n",
    "print(f'model_source_only: {model_source_only_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b3c36-3c42-40b7-999e-d5e451f1508e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba6c08-8888-474e-ad34-275fd529bddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
