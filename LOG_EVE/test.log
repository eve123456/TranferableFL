nohup: ignoring input
(null): can't open file 'Transfer__MNIST_MNIST-M.py': [Errno 2] No such file or directory
nohup: ignoring input
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 1
	            ft_lr : 0.001
	            ft_wd : 0.0
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 10
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 2
	           opt_lr : False
	            reg_J : False
	       reg_J_coef : 0.0
	           repeat : 5
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230919150828
FL pretrained model will be saved at ./models/lenet_mnist_20230919150828.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.000% / Loss: 2.3051 / Grad Norm: 0.1347 / Grad Diff: 1.1953 / Time: 9.67s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3033 / Time: 0.78s
======================================================================================================

round 0 local learning rate = 0.01
round 1 local learning rate = 0.01
>>> Training model_target_only
Epoch: 001, Train_loss: 0.6159, Train_acc: 0.8170, Test_loss: 0.6183, Test_acc: 0.8129
Model saved at ./models/ft_checkpoints/20230919150828_model_target_only.pt.
>>> Fine-tuning done!
>>> Training model_ft
Epoch: 001, Train_loss: 1.8052, Train_acc: 0.4342, Test_loss: 1.8021, Test_acc: 0.4378
Model saved at ./models/ft_checkpoints/20230919150828_model_ft.pt.
>>> Fine-tuning done!
>>> Training model_random
Epoch: 001, Train_loss: 1.7718, Train_acc: 0.4268, Test_loss: 1.7654, Test_acc: 0.4300
Model saved at ./models/ft_checkpoints/20230919150828_model_random.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_target_only: [0.6158555316235264, 0.8169692039117981, 0.6183234228384308, 0.8129096767025886]
model_ft: [1.8052444564587193, 0.43419603057575296, 1.8021441060375498, 0.4378402399733363]
model_random: [1.7717570705605035, 0.4267893764512466, 1.765367373481431, 0.4299522275302744]
model_source_only: [2.3036467183741656, 0.11252351655056694, 2.3044216629929335, 0.1128763470725475]

************************************************************************************************************************

uid: 20230919151019
FL pretrained model will be saved at ./models/lenet_mnist_20230919151019.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.000% / Loss: 2.3051 / Grad Norm: 0.1347 / Grad Diff: 1.1953 / Time: 9.31s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3033 / Time: 0.80s
======================================================================================================

round 0 local learning rate = 0.01
round 1 local learning rate = 0.01
>>> Training model_target_only
Epoch: 001, Train_loss: 0.6541, Train_acc: 0.8042, Test_loss: 0.6528, Test_acc: 0.8025
Model saved at ./models/ft_checkpoints/20230919151019_model_target_only.pt.
>>> Fine-tuning done!
>>> Training model_ft
Epoch: 001, Train_loss: 1.7828, Train_acc: 0.4342, Test_loss: 1.7777, Test_acc: 0.4411
Model saved at ./models/ft_checkpoints/20230919151019_model_ft.pt.
>>> Fine-tuning done!
>>> Training model_random
Epoch: 001, Train_loss: 1.9042, Train_acc: 0.3800, Test_loss: 1.9061, Test_acc: 0.3797
Model saved at ./models/ft_checkpoints/20230919151019_model_random.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_target_only: [0.654092285237521, 0.8042236572261487, 0.652820482030602, 0.8024663926230419]
model_ft: [1.7827970810525366, 0.4341790817104795, 1.7776509186729115, 0.44106210421064324]
model_random: [1.90415685576618, 0.380027457161743, 1.9061058219069045, 0.3797355849350072]
model_source_only: [2.303643485412414, 0.11252351655056694, 2.304418807956275, 0.1128763470725475]

************************************************************************************************************************

uid: 20230919151210
FL pretrained model will be saved at ./models/lenet_mnist_20230919151210.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.000% / Loss: 2.3051 / Grad Norm: 0.1347 / Grad Diff: 1.1953 / Time: 9.25s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3033 / Time: 0.78s
======================================================================================================

round 0 local learning rate = 0.01
round 1 local learning rate = 0.01
>>> Training model_target_only
Epoch: 001, Train_loss: 0.6108, Train_acc: 0.8181, Test_loss: 0.6027, Test_acc: 0.8194
Model saved at ./models/ft_checkpoints/20230919151210_model_target_only.pt.
>>> Fine-tuning done!
>>> Training model_ft
Epoch: 001, Train_loss: 1.7828, Train_acc: 0.4443, Test_loss: 1.7792, Test_acc: 0.4461
Model saved at ./models/ft_checkpoints/20230919151210_model_ft.pt.
>>> Fine-tuning done!
>>> Training model_random
Epoch: 001, Train_loss: 1.8064, Train_acc: 0.4252, Test_loss: 1.7919, Test_acc: 0.4335
Model saved at ./models/ft_checkpoints/20230919151210_model_random.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_target_only: [0.6108109844460111, 0.8180539312893002, 0.6027283723580017, 0.8193534051772026]
model_ft: [1.7828456465606901, 0.444331452009288, 1.779237127598094, 0.44606154871680925]
model_random: [1.806386649757116, 0.4252131319808139, 1.7918747000767912, 0.43350738806799244]
model_source_only: [2.303650192106314, 0.11252351655056694, 2.30442372577321, 0.1128763470725475]

************************************************************************************************************************

uid: 20230919151409
FL pretrained model will be saved at ./models/lenet_mnist_20230919151409.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.000% / Loss: 2.3051 / Grad Norm: 0.1347 / Grad Diff: 1.1953 / Time: 9.98s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3033 / Time: 0.81s
======================================================================================================

round 0 local learning rate = 0.01
round 1 local learning rate = 0.01
>>> Training model_target_only
Epoch: 001, Train_loss: 0.5827, Train_acc: 0.8310, Test_loss: 0.5861, Test_acc: 0.8341
Model saved at ./models/ft_checkpoints/20230919151409_model_target_only.pt.
>>> Fine-tuning done!
>>> Training model_ft
Epoch: 001, Train_loss: 1.7884, Train_acc: 0.4401, Test_loss: 1.7854, Test_acc: 0.4415
Model saved at ./models/ft_checkpoints/20230919151409_model_ft.pt.
>>> Fine-tuning done!
>>> Training model_random
Epoch: 001, Train_loss: 1.7301, Train_acc: 0.4615, Test_loss: 1.7185, Test_acc: 0.4657
Model saved at ./models/ft_checkpoints/20230919151409_model_random.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_target_only: [0.5826618077148795, 0.8310367620887782, 0.5861365775733031, 0.8341295411620931]
model_ft: [1.7884087577997367, 0.440077286825647, 1.7853672654744506, 0.441506499277858]
model_random: [1.7301265898180163, 0.4614837036660396, 1.7185460819613734, 0.4657260304410621]
model_source_only: [2.303649690166934, 0.11252351655056694, 2.3044221024819977, 0.1128763470725475]

************************************************************************************************************************

uid: 20230919151604
FL pretrained model will be saved at ./models/lenet_mnist_20230919151604.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.000% / Loss: 2.3051 / Grad Norm: 0.1347 / Grad Diff: 1.1953 / Time: 9.54s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3033 / Time: 0.80s
======================================================================================================

round 0 local learning rate = 0.01
round 1 local learning rate = 0.01
>>> Training model_target_only
Epoch: 001, Train_loss: 0.5907, Train_acc: 0.8219, Test_loss: 0.5884, Test_acc: 0.8226
Model saved at ./models/ft_checkpoints/20230919151604_model_target_only.pt.
>>> Fine-tuning done!
>>> Training model_ft
Epoch: 001, Train_loss: 1.7966, Train_acc: 0.4248, Test_loss: 1.7918, Test_acc: 0.4264
Model saved at ./models/ft_checkpoints/20230919151604_model_ft.pt.
>>> Fine-tuning done!
>>> Training model_random
Epoch: 001, Train_loss: 1.8035, Train_acc: 0.4227, Test_loss: 1.7931, Test_acc: 0.4256
Model saved at ./models/ft_checkpoints/20230919151604_model_random.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_target_only: [0.5906952492839326, 0.8218504771105575, 0.5883531794589356, 0.8225752694145095]
model_ft: [1.7966442188750533, 0.4248063592142506, 1.7917908994294103, 0.42639706699255636]
model_random: [1.8034605116535531, 0.42268775105506684, 1.7930718590426267, 0.4256193756249306]
model_source_only: [2.303647130904053, 0.11252351655056694, 2.304423195218367, 0.1128763470725475]
fl_test_acc_mean 0.1135
model_source_only_test_acc_mean 0.1128763470725475
model_ft_test_acc_mean 0.43857349183424066
model_target_only_test_acc_mean 0.8182868570158872
model_random_test_acc_mean 0.42690812131985334
