nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928191558
FL pretrained model will be saved at ./models/lenet_mnist_20230928191558.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 12.825% / Loss: 2.3061 /Time: 4.09s
======================================================================================================

= Test = round: 0 / acc: 13.430% / loss: 2.3056 / Time: 0.78s
======================================================================================================

round 0: local lr = 0.01
Traceback (most recent call last):
  File "main_mnist_mnist_m.py", line 401, in <module>
    main()
  File "main_mnist_mnist_m.py", line 331, in main
    fl_test_acc[repeat_i] = trainer.train()
  File "/data/shared/eve/TranferableFL/src/trainers/fedavgtl.py", line 82, in train
    solns, stats = self.local_train(round_i, selected_clients)
  File "/data/shared/eve/TranferableFL/src/trainers/base.py", line 165, in local_train
    soln, stat = c.local_train(last_round_avg_local_grad_norm = torch.sqrt(max_grad_norm_sq_at_global_weight_last_round) , 
UnboundLocalError: local variable 'max_grad_norm_sq_at_global_weight_last_round' referenced before assignment
nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928191726
FL pretrained model will be saved at ./models/lenet_mnist_20230928191726.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.000% / Loss: 2.3028 /Time: 4.03s
======================================================================================================

= Test = round: 0 / acc: 9.820% / loss: 2.3034 / Time: 0.77s
======================================================================================================

round 0: local lr = 0.01
Traceback (most recent call last):
  File "main_mnist_mnist_m.py", line 401, in <module>
    main()
  File "main_mnist_mnist_m.py", line 331, in main
    fl_test_acc[repeat_i] = trainer.train()
  File "/data/shared/eve/TranferableFL/src/trainers/fedavgtl.py", line 82, in train
    solns, stats = self.local_train(round_i, selected_clients)
  File "/data/shared/eve/TranferableFL/src/trainers/base.py", line 165, in local_train
    soln, stat = c.local_train(last_round_avg_local_grad_norm = torch.sqrt(max_grad_norm_sq_at_global_weight_last_round) , 
TypeError: sqrt(): argument 'input' (position 1) must be Tensor, not NoneType
nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928192031
FL pretrained model will be saved at ./models/lenet_mnist_20230928192031.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 9.982% / Loss: 2.3020 /Time: 4.00s
======================================================================================================

= Test = round: 0 / acc: 8.890% / loss: 2.3034 / Time: 0.77s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.0015021921135485172, sq_norm_avg_grad = 0.042929526418447495, avg_sq_norm_grad = 2.159816265106201,                  max_norm_grad = 1.677667498588562, var_grad = 2.116886854171753
round 2: local lr = 0.001498067518696189, sq_norm_avg_grad = 0.04245397076010704, avg_sq_norm_grad = 2.1417715549468994,                  max_norm_grad = 1.6692399978637695, var_grad = 2.0993175506591797
round 3: local lr = 0.0014947161544114351, sq_norm_avg_grad = 0.04200897738337517, avg_sq_norm_grad = 2.1240737438201904,                  max_norm_grad = 1.6590180397033691, var_grad = 2.0820648670196533
round 4: local lr = 0.0014928133459761739, sq_norm_avg_grad = 0.041625700891017914, avg_sq_norm_grad = 2.107377052307129,                  max_norm_grad = 1.6503574848175049, var_grad = 2.065751314163208

>>> Round:    5 / Acc: 10.004% / Loss: 2.2963 /Time: 3.96s
======================================================================================================

= Test = round: 5 / acc: 8.910% / loss: 2.2974 / Time: 0.76s
======================================================================================================

round 5: local lr = 0.001490227528847754, sq_norm_avg_grad = 0.0412428043782711, avg_sq_norm_grad = 2.0916154384613037,                  max_norm_grad = 1.642730712890625, var_grad = 2.05037260055542
round 6: local lr = 0.0014910269528627396, sq_norm_avg_grad = 0.04096577689051628, avg_sq_norm_grad = 2.0764520168304443,                  max_norm_grad = 1.6344375610351562, var_grad = 2.0354862213134766
round 7: local lr = 0.0014908445300534368, sq_norm_avg_grad = 0.04069475084543228, avg_sq_norm_grad = 2.062966823577881,                  max_norm_grad = 1.628387451171875, var_grad = 2.0222721099853516
round 8: local lr = 0.001492431852966547, sq_norm_avg_grad = 0.040478408336639404, avg_sq_norm_grad = 2.0498170852661133,                  max_norm_grad = 1.6210204362869263, var_grad = 2.009338617324829
round 9: local lr = 0.0014921483816578984, sq_norm_avg_grad = 0.040226370096206665, avg_sq_norm_grad = 2.0374410152435303,                  max_norm_grad = 1.6153966188430786, var_grad = 1.997214674949646

>>> Round:   10 / Acc: 10.127% / Loss: 2.2933 /Time: 3.90s
======================================================================================================

= Test = round: 10 / acc: 9.060% / loss: 2.2943 / Time: 0.76s
======================================================================================================

round 10: local lr = 0.0014950698241591454, sq_norm_avg_grad = 0.040070582181215286, avg_sq_norm_grad = 2.0255846977233887,                  max_norm_grad = 1.6089152097702026, var_grad = 1.9855141639709473
round 11: local lr = 0.0014966280432417989, sq_norm_avg_grad = 0.03989974781870842, avg_sq_norm_grad = 2.0148489475250244,                  max_norm_grad = 1.6032108068466187, var_grad = 1.9749492406845093
round 12: local lr = 0.0014977984828874469, sq_norm_avg_grad = 0.03972376510500908, avg_sq_norm_grad = 2.00439453125,                  max_norm_grad = 1.5978678464889526, var_grad = 1.9646707773208618
round 13: local lr = 0.0014991448260843754, sq_norm_avg_grad = 0.03956808149814606, avg_sq_norm_grad = 1.9947460889816284,                  max_norm_grad = 1.592919111251831, var_grad = 1.9551780223846436
round 14: local lr = 0.0015001376159489155, sq_norm_avg_grad = 0.03941335901618004, avg_sq_norm_grad = 1.9856311082839966,                  max_norm_grad = 1.5871809720993042, var_grad = 1.9462177753448486

>>> Round:   15 / Acc: 10.520% / Loss: 2.2904 /Time: 4.02s
======================================================================================================

= Test = round: 15 / acc: 9.490% / loss: 2.2912 / Time: 0.76s
======================================================================================================

round 15: local lr = 0.0014998612459748983, sq_norm_avg_grad = 0.039227381348609924, avg_sq_norm_grad = 1.976625680923462,                  max_norm_grad = 1.5835185050964355, var_grad = 1.9373983144760132
round 16: local lr = 0.0014994670636951923, sq_norm_avg_grad = 0.03906776010990143, avg_sq_norm_grad = 1.969099998474121,                  max_norm_grad = 1.5790269374847412, var_grad = 1.9300322532653809
round 17: local lr = 0.001498483121395111, sq_norm_avg_grad = 0.03889719024300575, avg_sq_norm_grad = 1.9617904424667358,                  max_norm_grad = 1.5758790969848633, var_grad = 1.9228932857513428
round 18: local lr = 0.001497653778642416, sq_norm_avg_grad = 0.03874525800347328, avg_sq_norm_grad = 1.9552096128463745,                  max_norm_grad = 1.5716650485992432, var_grad = 1.9164643287658691
round 19: local lr = 0.0014974471414461732, sq_norm_avg_grad = 0.038635462522506714, avg_sq_norm_grad = 1.9499380588531494,                  max_norm_grad = 1.5683577060699463, var_grad = 1.9113025665283203

>>> Round:   20 / Acc: 11.196% / Loss: 2.2876 /Time: 3.90s
======================================================================================================

= Test = round: 20 / acc: 10.160% / loss: 2.2882 / Time: 0.75s
======================================================================================================

round 20: local lr = 0.0014980742707848549, sq_norm_avg_grad = 0.03854288160800934, avg_sq_norm_grad = 1.9444512128829956,                  max_norm_grad = 1.5640733242034912, var_grad = 1.9059083461761475
round 21: local lr = 0.001498576020821929, sq_norm_avg_grad = 0.038467612117528915, avg_sq_norm_grad = 1.9400042295455933,                  max_norm_grad = 1.5614063739776611, var_grad = 1.9015365839004517
round 22: local lr = 0.0014993527438491583, sq_norm_avg_grad = 0.03840640187263489, avg_sq_norm_grad = 1.9359136819839478,                  max_norm_grad = 1.5592347383499146, var_grad = 1.8975073099136353
round 23: local lr = 0.0014996620593592525, sq_norm_avg_grad = 0.03834349662065506, avg_sq_norm_grad = 1.9323443174362183,                  max_norm_grad = 1.557114601135254, var_grad = 1.894000768661499
round 24: local lr = 0.0015028048073872924, sq_norm_avg_grad = 0.03836404159665108, avg_sq_norm_grad = 1.9293365478515625,                  max_norm_grad = 1.554819941520691, var_grad = 1.8909724950790405

>>> Round:   25 / Acc: 11.982% / Loss: 2.2848 /Time: 3.93s
======================================================================================================

= Test = round: 25 / acc: 11.090% / loss: 2.2854 / Time: 0.78s
======================================================================================================

round 25: local lr = 0.0015077824937179685, sq_norm_avg_grad = 0.038437746465206146, avg_sq_norm_grad = 1.9266616106033325,                  max_norm_grad = 1.5539840459823608, var_grad = 1.8882238864898682
round 26: local lr = 0.0015109899686649442, sq_norm_avg_grad = 0.03848955035209656, avg_sq_norm_grad = 1.9251629114151,                  max_norm_grad = 1.5521231889724731, var_grad = 1.8866733312606812
round 27: local lr = 0.0015144750941544771, sq_norm_avg_grad = 0.03856333717703819, avg_sq_norm_grad = 1.9244147539138794,                  max_norm_grad = 1.5507073402404785, var_grad = 1.8858513832092285
round 28: local lr = 0.0015188218094408512, sq_norm_avg_grad = 0.038662154227495193, avg_sq_norm_grad = 1.923824429512024,                  max_norm_grad = 1.549203872680664, var_grad = 1.8851622343063354
round 29: local lr = 0.0015246424591168761, sq_norm_avg_grad = 0.038807760924100876, avg_sq_norm_grad = 1.9236974716186523,                  max_norm_grad = 1.5485265254974365, var_grad = 1.8848897218704224

>>> Round:   30 / Acc: 12.703% / Loss: 2.2820 /Time: 4.09s
======================================================================================================

= Test = round: 30 / acc: 11.760% / loss: 2.2825 / Time: 0.79s
======================================================================================================

round 30: local lr = 0.0015292796306312084, sq_norm_avg_grad = 0.03893531858921051, avg_sq_norm_grad = 1.9241682291030884,                  max_norm_grad = 1.5485079288482666, var_grad = 1.885232925415039
round 31: local lr = 0.001535279443487525, sq_norm_avg_grad = 0.03911356255412102, avg_sq_norm_grad = 1.9254229068756104,                  max_norm_grad = 1.5509788990020752, var_grad = 1.8863093852996826
round 32: local lr = 0.0015420971903949976, sq_norm_avg_grad = 0.03931815177202225, avg_sq_norm_grad = 1.926937222480774,                  max_norm_grad = 1.552752137184143, var_grad = 1.8876190185546875
round 33: local lr = 0.0015495236730203032, sq_norm_avg_grad = 0.039552319794893265, avg_sq_norm_grad = 1.9291231632232666,                  max_norm_grad = 1.556140422821045, var_grad = 1.8895708322525024
