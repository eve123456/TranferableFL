nohup: ignoring input
working!
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.01
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 10
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 50
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.001
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 5
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20231001225008
FL pretrained model will be saved at ./models/lenet_mnist_20231001225008.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.343% / Loss: 2.3012 /Time: 5.29s
======================================================================================================

= Test = round: 0 / acc: 10.690% / loss: 2.2988 / Time: 1.01s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.012684745714068413, avg_sq_norm_grad = 0.9449564814567566,                  max_norm_grad = 1.055822491645813, var_grad = 0.9322717189788818
round 2: local lr = 0.01, sq_norm_avg_grad = 0.013460473157465458, avg_sq_norm_grad = 1.0061191320419312,                  max_norm_grad = 1.0878756046295166, var_grad = 0.9926586747169495
round 3: local lr = 0.01, sq_norm_avg_grad = 0.014394703321158886, avg_sq_norm_grad = 1.084870457649231,                  max_norm_grad = 1.1340136528015137, var_grad = 1.070475697517395
round 4: local lr = 0.01, sq_norm_avg_grad = 0.01574769802391529, avg_sq_norm_grad = 1.1873328685760498,                  max_norm_grad = 1.1929343938827515, var_grad = 1.171585202217102

>>> Round:    5 / Acc: 16.421% / Loss: 2.2923 /Time: 5.46s
======================================================================================================

= Test = round: 5 / acc: 16.680% / loss: 2.2900 / Time: 0.97s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.017643574625253677, avg_sq_norm_grad = 1.3234935998916626,                  max_norm_grad = 1.2660895586013794, var_grad = 1.3058500289916992
round 6: local lr = 0.01, sq_norm_avg_grad = 0.02021676115691662, avg_sq_norm_grad = 1.508739948272705,                  max_norm_grad = 1.359022855758667, var_grad = 1.488523244857788
round 7: local lr = 0.01, sq_norm_avg_grad = 0.024034179747104645, avg_sq_norm_grad = 1.7732452154159546,                  max_norm_grad = 1.4816457033157349, var_grad = 1.749211072921753
round 8: local lr = 0.01, sq_norm_avg_grad = 0.02988855354487896, avg_sq_norm_grad = 2.1784582138061523,                  max_norm_grad = 1.6474056243896484, var_grad = 2.1485695838928223
round 9: local lr = 0.01, sq_norm_avg_grad = 0.03914100304245949, avg_sq_norm_grad = 2.859987735748291,                  max_norm_grad = 1.8912240266799927, var_grad = 2.8208467960357666

>>> Round:   10 / Acc: 18.002% / Loss: 2.2703 /Time: 5.30s
======================================================================================================

= Test = round: 10 / acc: 18.590% / loss: 2.2686 / Time: 1.09s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.05286979675292969, avg_sq_norm_grad = 4.135890007019043,                  max_norm_grad = 2.288072109222412, var_grad = 4.083020210266113
round 11: local lr = 0.01, sq_norm_avg_grad = 0.08167069405317307, avg_sq_norm_grad = 6.933786392211914,                  max_norm_grad = 2.9900853633880615, var_grad = 6.852115631103516
round 12: local lr = 0.01, sq_norm_avg_grad = 0.16358348727226257, avg_sq_norm_grad = 14.822538375854492,                  max_norm_grad = 4.422229766845703, var_grad = 14.658954620361328
round 13: local lr = 0.01, sq_norm_avg_grad = 0.3985498249530792, avg_sq_norm_grad = 39.68730163574219,                  max_norm_grad = 7.324798107147217, var_grad = 39.288753509521484
round 14: local lr = 0.01, sq_norm_avg_grad = 0.8388680815696716, avg_sq_norm_grad = 91.08567810058594,                  max_norm_grad = 11.12337589263916, var_grad = 90.24681091308594

>>> Round:   15 / Acc: 21.129% / Loss: 2.1456 /Time: 5.94s
======================================================================================================

= Test = round: 15 / acc: 22.530% / loss: 2.1419 / Time: 0.99s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.5376561880111694, avg_sq_norm_grad = 132.80081176757812,                  max_norm_grad = 13.57615852355957, var_grad = 131.26315307617188
round 16: local lr = 0.01, sq_norm_avg_grad = 2.208311080932617, avg_sq_norm_grad = 142.90921020507812,                  max_norm_grad = 14.22297191619873, var_grad = 140.70089721679688
round 17: local lr = 0.01, sq_norm_avg_grad = 2.8858447074890137, avg_sq_norm_grad = 142.32557678222656,                  max_norm_grad = 14.350250244140625, var_grad = 139.43972778320312
round 18: local lr = 0.01, sq_norm_avg_grad = 2.831550359725952, avg_sq_norm_grad = 140.50038146972656,                  max_norm_grad = 14.307812690734863, var_grad = 137.66883850097656
round 19: local lr = 0.01, sq_norm_avg_grad = 3.0774803161621094, avg_sq_norm_grad = 142.84548950195312,                  max_norm_grad = 14.45770263671875, var_grad = 139.76800537109375

>>> Round:   20 / Acc: 47.391% / Loss: 1.9767 /Time: 5.15s
======================================================================================================

= Test = round: 20 / acc: 48.880% / loss: 1.9642 / Time: 1.01s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 3.584610939025879, avg_sq_norm_grad = 142.75180053710938,                  max_norm_grad = 14.492897033691406, var_grad = 139.1671905517578
round 21: local lr = 0.01, sq_norm_avg_grad = 3.9416818618774414, avg_sq_norm_grad = 144.43031311035156,                  max_norm_grad = 14.57702922821045, var_grad = 140.48863220214844
round 22: local lr = 0.01, sq_norm_avg_grad = 4.617306232452393, avg_sq_norm_grad = 148.75657653808594,                  max_norm_grad = 14.781024932861328, var_grad = 144.13926696777344
round 23: local lr = 0.01, sq_norm_avg_grad = 3.8017489910125732, avg_sq_norm_grad = 146.6292266845703,                  max_norm_grad = 14.484880447387695, var_grad = 142.82748413085938
round 24: local lr = 0.01, sq_norm_avg_grad = 3.8869857788085938, avg_sq_norm_grad = 141.89222717285156,                  max_norm_grad = 14.19357681274414, var_grad = 138.0052490234375

>>> Round:   25 / Acc: 65.954% / Loss: 1.6926 /Time: 5.16s
======================================================================================================

= Test = round: 25 / acc: 67.660% / loss: 1.6695 / Time: 0.99s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 4.088409900665283, avg_sq_norm_grad = 145.63253784179688,                  max_norm_grad = 14.279928207397461, var_grad = 141.54412841796875
round 26: local lr = 0.01, sq_norm_avg_grad = 4.361790657043457, avg_sq_norm_grad = 147.0924530029297,                  max_norm_grad = 14.228022575378418, var_grad = 142.7306671142578
round 27: local lr = 0.01, sq_norm_avg_grad = 4.7547760009765625, avg_sq_norm_grad = 148.43751525878906,                  max_norm_grad = 14.327489852905273, var_grad = 143.6827392578125
round 28: local lr = 0.01, sq_norm_avg_grad = 3.9896273612976074, avg_sq_norm_grad = 145.22740173339844,                  max_norm_grad = 14.063824653625488, var_grad = 141.23777770996094
round 29: local lr = 0.01, sq_norm_avg_grad = 4.151351451873779, avg_sq_norm_grad = 143.897705078125,                  max_norm_grad = 14.117166519165039, var_grad = 139.74635314941406

>>> Round:   30 / Acc: 70.312% / Loss: 1.3858 /Time: 5.30s
======================================================================================================

= Test = round: 30 / acc: 71.810% / loss: 1.3531 / Time: 1.05s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 3.818713665008545, avg_sq_norm_grad = 139.90628051757812,                  max_norm_grad = 14.121086120605469, var_grad = 136.0875701904297
round 31: local lr = 0.01, sq_norm_avg_grad = 3.974252700805664, avg_sq_norm_grad = 138.15560913085938,                  max_norm_grad = 14.214056968688965, var_grad = 134.1813507080078
round 32: local lr = 0.01, sq_norm_avg_grad = 3.3512771129608154, avg_sq_norm_grad = 132.49197387695312,                  max_norm_grad = 14.013251304626465, var_grad = 129.1407012939453
round 33: local lr = 0.01, sq_norm_avg_grad = 3.560600996017456, avg_sq_norm_grad = 130.80921936035156,                  max_norm_grad = 14.229231834411621, var_grad = 127.24861907958984
round 34: local lr = 0.01, sq_norm_avg_grad = 3.679108142852783, avg_sq_norm_grad = 129.82373046875,                  max_norm_grad = 14.369355201721191, var_grad = 126.14462280273438

>>> Round:   35 / Acc: 72.880% / Loss: 1.1507 /Time: 5.73s
======================================================================================================

= Test = round: 35 / acc: 74.230% / loss: 1.1133 / Time: 1.19s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 4.479883670806885, avg_sq_norm_grad = 130.17591857910156,                  max_norm_grad = 14.725175857543945, var_grad = 125.69603729248047
round 36: local lr = 0.01, sq_norm_avg_grad = 4.456264019012451, avg_sq_norm_grad = 126.97283935546875,                  max_norm_grad = 14.603761672973633, var_grad = 122.5165786743164
round 37: local lr = 0.01, sq_norm_avg_grad = 4.4900407791137695, avg_sq_norm_grad = 124.37681579589844,                  max_norm_grad = 14.546807289123535, var_grad = 119.88677215576172
round 38: local lr = 0.01, sq_norm_avg_grad = 5.060964584350586, avg_sq_norm_grad = 124.75497436523438,                  max_norm_grad = 14.799765586853027, var_grad = 119.69400787353516
round 39: local lr = 0.01, sq_norm_avg_grad = 6.182633876800537, avg_sq_norm_grad = 123.16748809814453,                  max_norm_grad = 14.958784103393555, var_grad = 116.98485565185547

>>> Round:   40 / Acc: 73.103% / Loss: 0.9981 /Time: 5.38s
======================================================================================================

= Test = round: 40 / acc: 74.970% / loss: 0.9592 / Time: 1.14s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 6.82365608215332, avg_sq_norm_grad = 123.22972106933594,                  max_norm_grad = 15.107744216918945, var_grad = 116.40606689453125
round 41: local lr = 0.01, sq_norm_avg_grad = 5.898226737976074, avg_sq_norm_grad = 119.16385650634766,                  max_norm_grad = 14.760726928710938, var_grad = 113.26563262939453
round 42: local lr = 0.01, sq_norm_avg_grad = 5.992616176605225, avg_sq_norm_grad = 117.94564819335938,                  max_norm_grad = 14.713286399841309, var_grad = 111.95303344726562
round 43: local lr = 0.01, sq_norm_avg_grad = 6.428893566131592, avg_sq_norm_grad = 114.05790710449219,                  max_norm_grad = 14.671101570129395, var_grad = 107.62901306152344
round 44: local lr = 0.01, sq_norm_avg_grad = 5.7963480949401855, avg_sq_norm_grad = 112.626953125,                  max_norm_grad = 14.406631469726562, var_grad = 106.83060455322266

>>> Round:   45 / Acc: 74.589% / Loss: 0.8843 /Time: 5.29s
======================================================================================================

= Test = round: 45 / acc: 76.300% / loss: 0.8460 / Time: 1.10s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 7.063557147979736, avg_sq_norm_grad = 114.36251831054688,                  max_norm_grad = 15.095060348510742, var_grad = 107.29895782470703
round 46: local lr = 0.01, sq_norm_avg_grad = 6.525705814361572, avg_sq_norm_grad = 110.05064392089844,                  max_norm_grad = 14.489015579223633, var_grad = 103.52494049072266
round 47: local lr = 0.01, sq_norm_avg_grad = 6.771719455718994, avg_sq_norm_grad = 109.4448013305664,                  max_norm_grad = 14.529036521911621, var_grad = 102.67308044433594
round 48: local lr = 0.01, sq_norm_avg_grad = 6.9643874168396, avg_sq_norm_grad = 107.63833618164062,                  max_norm_grad = 14.34115219116211, var_grad = 100.6739501953125
round 49: local lr = 0.01, sq_norm_avg_grad = 6.953006267547607, avg_sq_norm_grad = 107.21932220458984,                  max_norm_grad = 14.272117614746094, var_grad = 100.26631927490234

>>> Round:   50 / Acc: 76.026% / Loss: 0.8018 /Time: 5.07s
======================================================================================================

= Test = round: 50 / acc: 77.880% / loss: 0.7649 / Time: 0.89s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4525, Train_acc: 0.5587, Test_loss: 1.4334, Test_acc: 0.5748
Epoch: 006, Train_loss: 1.2195, Train_acc: 0.6355, Test_loss: 1.2018, Test_acc: 0.6447
Epoch: 011, Train_loss: 1.1896, Train_acc: 0.6516, Test_loss: 1.1738, Test_acc: 0.6554
Epoch: 016, Train_loss: 1.1857, Train_acc: 0.6434, Test_loss: 1.1701, Test_acc: 0.6496
Epoch: 021, Train_loss: 1.1798, Train_acc: 0.6489, Test_loss: 1.1659, Test_acc: 0.6535
Epoch: 026, Train_loss: 1.1732, Train_acc: 0.6566, Test_loss: 1.1569, Test_acc: 0.6633
Epoch: 031, Train_loss: 1.1819, Train_acc: 0.6475, Test_loss: 1.1683, Test_acc: 0.6513
Epoch: 036, Train_loss: 1.1781, Train_acc: 0.6531, Test_loss: 1.1659, Test_acc: 0.6585
Epoch: 041, Train_loss: 1.1900, Train_acc: 0.6436, Test_loss: 1.1715, Test_acc: 0.6524
Epoch: 046, Train_loss: 1.1722, Train_acc: 0.6560, Test_loss: 1.1594, Test_acc: 0.6625
Epoch: 051, Train_loss: 1.1720, Train_acc: 0.6534, Test_loss: 1.1571, Test_acc: 0.6586
Epoch: 056, Train_loss: 1.1671, Train_acc: 0.6569, Test_loss: 1.1535, Test_acc: 0.6615
Epoch: 061, Train_loss: 1.1656, Train_acc: 0.6594, Test_loss: 1.1523, Test_acc: 0.6628
Epoch: 066, Train_loss: 1.1685, Train_acc: 0.6548, Test_loss: 1.1545, Test_acc: 0.6631
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001225008_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001225008_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.1655966841191172, 0.6593956034643481, 1.1523356496618928, 0.6628152427508055]
model_source_only: [2.414247735468275, 0.3624514838731547, 2.4295910069338813, 0.36484835018331296]

************************************************************************************************************************

uid: 20231001233157
FL pretrained model will be saved at ./models/lenet_mnist_20231001233157.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.343% / Loss: 2.3012 /Time: 5.63s
======================================================================================================

= Test = round: 0 / acc: 10.690% / loss: 2.2988 / Time: 1.11s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.012686576694250107, avg_sq_norm_grad = 0.9451811909675598,                  max_norm_grad = 1.0561059713363647, var_grad = 0.9324946403503418
round 2: local lr = 0.01, sq_norm_avg_grad = 0.013447373174130917, avg_sq_norm_grad = 1.0058621168136597,                  max_norm_grad = 1.088078498840332, var_grad = 0.9924147725105286
round 3: local lr = 0.01, sq_norm_avg_grad = 0.01437762938439846, avg_sq_norm_grad = 1.0845186710357666,                  max_norm_grad = 1.1336874961853027, var_grad = 1.070141077041626
round 4: local lr = 0.01, sq_norm_avg_grad = 0.015728536993265152, avg_sq_norm_grad = 1.1870486736297607,                  max_norm_grad = 1.1927800178527832, var_grad = 1.1713200807571411

>>> Round:    5 / Acc: 16.755% / Loss: 2.2923 /Time: 5.44s
======================================================================================================

= Test = round: 5 / acc: 17.030% / loss: 2.2899 / Time: 0.89s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.017623567953705788, avg_sq_norm_grad = 1.3227883577346802,                  max_norm_grad = 1.2656974792480469, var_grad = 1.3051648139953613
round 6: local lr = 0.01, sq_norm_avg_grad = 0.020210150629281998, avg_sq_norm_grad = 1.5078965425491333,                  max_norm_grad = 1.3585492372512817, var_grad = 1.4876863956451416
round 7: local lr = 0.01, sq_norm_avg_grad = 0.024027356877923012, avg_sq_norm_grad = 1.7715115547180176,                  max_norm_grad = 1.4809702634811401, var_grad = 1.7474842071533203
round 8: local lr = 0.01, sq_norm_avg_grad = 0.02988494746387005, avg_sq_norm_grad = 2.177624225616455,                  max_norm_grad = 1.647470474243164, var_grad = 2.1477391719818115
round 9: local lr = 0.01, sq_norm_avg_grad = 0.0391007661819458, avg_sq_norm_grad = 2.855975389480591,                  max_norm_grad = 1.8892168998718262, var_grad = 2.8168745040893555

>>> Round:   10 / Acc: 18.081% / Loss: 2.2704 /Time: 5.35s
======================================================================================================

= Test = round: 10 / acc: 18.560% / loss: 2.2687 / Time: 0.98s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.05290048196911812, avg_sq_norm_grad = 4.125576496124268,                  max_norm_grad = 2.2856290340423584, var_grad = 4.072676181793213
round 11: local lr = 0.01, sq_norm_avg_grad = 0.08150999248027802, avg_sq_norm_grad = 6.892435073852539,                  max_norm_grad = 2.981605291366577, var_grad = 6.810925006866455
round 12: local lr = 0.01, sq_norm_avg_grad = 0.1625894457101822, avg_sq_norm_grad = 14.709015846252441,                  max_norm_grad = 4.398800373077393, var_grad = 14.546426773071289
round 13: local lr = 0.01, sq_norm_avg_grad = 0.3924104869365692, avg_sq_norm_grad = 39.32732391357422,                  max_norm_grad = 7.276379585266113, var_grad = 38.934913635253906
round 14: local lr = 0.01, sq_norm_avg_grad = 0.9041215181350708, avg_sq_norm_grad = 90.7981948852539,                  max_norm_grad = 11.105234146118164, var_grad = 89.89407348632812

>>> Round:   15 / Acc: 19.688% / Loss: 2.1467 /Time: 5.75s
======================================================================================================

= Test = round: 15 / acc: 20.850% / loss: 2.1432 / Time: 1.14s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.612529993057251, avg_sq_norm_grad = 132.412109375,                  max_norm_grad = 13.584786415100098, var_grad = 130.79957580566406
round 16: local lr = 0.01, sq_norm_avg_grad = 2.1233224868774414, avg_sq_norm_grad = 138.75929260253906,                  max_norm_grad = 14.061807632446289, var_grad = 136.63597106933594
round 17: local lr = 0.01, sq_norm_avg_grad = 2.705063819885254, avg_sq_norm_grad = 141.24868774414062,                  max_norm_grad = 14.297932624816895, var_grad = 138.5436248779297
round 18: local lr = 0.01, sq_norm_avg_grad = 2.906658887863159, avg_sq_norm_grad = 142.32586669921875,                  max_norm_grad = 14.334606170654297, var_grad = 139.41920471191406
round 19: local lr = 0.01, sq_norm_avg_grad = 3.3925845623016357, avg_sq_norm_grad = 144.34478759765625,                  max_norm_grad = 14.427077293395996, var_grad = 140.95220947265625

>>> Round:   20 / Acc: 48.443% / Loss: 1.9750 /Time: 5.65s
======================================================================================================

= Test = round: 20 / acc: 50.230% / loss: 1.9620 / Time: 0.94s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 3.6436572074890137, avg_sq_norm_grad = 145.17539978027344,                  max_norm_grad = 14.481077194213867, var_grad = 141.53173828125
round 21: local lr = 0.01, sq_norm_avg_grad = 4.383335590362549, avg_sq_norm_grad = 146.3333282470703,                  max_norm_grad = 14.547821998596191, var_grad = 141.9499969482422
round 22: local lr = 0.01, sq_norm_avg_grad = 4.307771682739258, avg_sq_norm_grad = 147.0185089111328,                  max_norm_grad = 14.660494804382324, var_grad = 142.7107391357422
round 23: local lr = 0.01, sq_norm_avg_grad = 4.40175724029541, avg_sq_norm_grad = 148.51773071289062,                  max_norm_grad = 14.587403297424316, var_grad = 144.115966796875
round 24: local lr = 0.01, sq_norm_avg_grad = 4.469493389129639, avg_sq_norm_grad = 145.90074157714844,                  max_norm_grad = 14.39768123626709, var_grad = 141.43124389648438

>>> Round:   25 / Acc: 63.581% / Loss: 1.6932 /Time: 5.46s
======================================================================================================

= Test = round: 25 / acc: 65.390% / loss: 1.6708 / Time: 1.20s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 4.326391696929932, avg_sq_norm_grad = 145.3697052001953,                  max_norm_grad = 14.34002685546875, var_grad = 141.04331970214844
round 26: local lr = 0.01, sq_norm_avg_grad = 4.238974571228027, avg_sq_norm_grad = 147.40289306640625,                  max_norm_grad = 14.374119758605957, var_grad = 143.16392517089844
round 27: local lr = 0.01, sq_norm_avg_grad = 4.406641483306885, avg_sq_norm_grad = 144.44125366210938,                  max_norm_grad = 14.14515495300293, var_grad = 140.03460693359375
round 28: local lr = 0.01, sq_norm_avg_grad = 4.029669284820557, avg_sq_norm_grad = 143.0370330810547,                  max_norm_grad = 14.040003776550293, var_grad = 139.0073699951172
round 29: local lr = 0.01, sq_norm_avg_grad = 4.17581033706665, avg_sq_norm_grad = 144.1309356689453,                  max_norm_grad = 14.384819984436035, var_grad = 139.9551239013672

>>> Round:   30 / Acc: 70.231% / Loss: 1.3854 /Time: 5.43s
======================================================================================================

= Test = round: 30 / acc: 72.020% / loss: 1.3517 / Time: 0.96s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 3.895092487335205, avg_sq_norm_grad = 140.08213806152344,                  max_norm_grad = 14.166009902954102, var_grad = 136.18704223632812
round 31: local lr = 0.01, sq_norm_avg_grad = 3.7597835063934326, avg_sq_norm_grad = 136.3105010986328,                  max_norm_grad = 14.137238502502441, var_grad = 132.55072021484375
round 32: local lr = 0.01, sq_norm_avg_grad = 3.3642210960388184, avg_sq_norm_grad = 131.51885986328125,                  max_norm_grad = 13.940727233886719, var_grad = 128.15463256835938
round 33: local lr = 0.01, sq_norm_avg_grad = 3.8101086616516113, avg_sq_norm_grad = 131.2645721435547,                  max_norm_grad = 14.021739959716797, var_grad = 127.45446014404297
round 34: local lr = 0.01, sq_norm_avg_grad = 3.4960477352142334, avg_sq_norm_grad = 128.62913513183594,                  max_norm_grad = 14.025823593139648, var_grad = 125.13308715820312

>>> Round:   35 / Acc: 73.356% / Loss: 1.1485 /Time: 5.73s
======================================================================================================

= Test = round: 35 / acc: 74.810% / loss: 1.1106 / Time: 1.09s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 3.5276412963867188, avg_sq_norm_grad = 126.35045623779297,                  max_norm_grad = 14.124471664428711, var_grad = 122.82281494140625
round 36: local lr = 0.01, sq_norm_avg_grad = 4.335603713989258, avg_sq_norm_grad = 127.04796600341797,                  max_norm_grad = 14.518364906311035, var_grad = 122.71236419677734
round 37: local lr = 0.01, sq_norm_avg_grad = 4.2395339012146, avg_sq_norm_grad = 125.02100372314453,                  max_norm_grad = 14.515514373779297, var_grad = 120.7814712524414
round 38: local lr = 0.01, sq_norm_avg_grad = 4.223803520202637, avg_sq_norm_grad = 122.82746124267578,                  max_norm_grad = 14.572256088256836, var_grad = 118.6036605834961
round 39: local lr = 0.01, sq_norm_avg_grad = 4.9141716957092285, avg_sq_norm_grad = 119.07345581054688,                  max_norm_grad = 14.684853553771973, var_grad = 114.15928649902344

>>> Round:   40 / Acc: 74.085% / Loss: 0.9925 /Time: 6.21s
======================================================================================================

= Test = round: 40 / acc: 75.540% / loss: 0.9532 / Time: 0.94s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 4.890865802764893, avg_sq_norm_grad = 116.65727996826172,                  max_norm_grad = 14.614518165588379, var_grad = 111.76641082763672
round 41: local lr = 0.01, sq_norm_avg_grad = 5.802989959716797, avg_sq_norm_grad = 118.19458770751953,                  max_norm_grad = 14.939888954162598, var_grad = 112.3916015625
round 42: local lr = 0.01, sq_norm_avg_grad = 5.890450954437256, avg_sq_norm_grad = 117.0782699584961,                  max_norm_grad = 14.813247680664062, var_grad = 111.18782043457031
round 43: local lr = 0.01, sq_norm_avg_grad = 6.29571533203125, avg_sq_norm_grad = 113.52842712402344,                  max_norm_grad = 14.662165641784668, var_grad = 107.23271179199219
round 44: local lr = 0.01, sq_norm_avg_grad = 6.240963459014893, avg_sq_norm_grad = 111.02078247070312,                  max_norm_grad = 14.523675918579102, var_grad = 104.77981567382812

>>> Round:   45 / Acc: 74.766% / Loss: 0.8812 /Time: 5.25s
======================================================================================================

= Test = round: 45 / acc: 76.530% / loss: 0.8433 / Time: 0.98s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 6.357299327850342, avg_sq_norm_grad = 110.75873565673828,                  max_norm_grad = 14.479965209960938, var_grad = 104.40143585205078
round 46: local lr = 0.01, sq_norm_avg_grad = 7.764021396636963, avg_sq_norm_grad = 111.83866119384766,                  max_norm_grad = 14.693953514099121, var_grad = 104.07463836669922
round 47: local lr = 0.01, sq_norm_avg_grad = 6.244504451751709, avg_sq_norm_grad = 106.96823120117188,                  max_norm_grad = 14.167159080505371, var_grad = 100.72372436523438
round 48: local lr = 0.01, sq_norm_avg_grad = 6.296910762786865, avg_sq_norm_grad = 106.36134338378906,                  max_norm_grad = 14.062907218933105, var_grad = 100.0644302368164
round 49: local lr = 0.01, sq_norm_avg_grad = 6.880531311035156, avg_sq_norm_grad = 106.67061614990234,                  max_norm_grad = 14.122543334960938, var_grad = 99.79008483886719

>>> Round:   50 / Acc: 76.264% / Loss: 0.7968 /Time: 4.85s
======================================================================================================

= Test = round: 50 / acc: 78.050% / loss: 0.7610 / Time: 1.04s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4525, Train_acc: 0.5570, Test_loss: 1.4354, Test_acc: 0.5644
Epoch: 006, Train_loss: 1.2182, Train_acc: 0.6341, Test_loss: 1.2019, Test_acc: 0.6405
Epoch: 011, Train_loss: 1.1911, Train_acc: 0.6423, Test_loss: 1.1755, Test_acc: 0.6480
Epoch: 016, Train_loss: 1.1882, Train_acc: 0.6451, Test_loss: 1.1720, Test_acc: 0.6533
Epoch: 021, Train_loss: 1.1765, Train_acc: 0.6541, Test_loss: 1.1615, Test_acc: 0.6608
Epoch: 026, Train_loss: 1.1892, Train_acc: 0.6466, Test_loss: 1.1750, Test_acc: 0.6517
Epoch: 031, Train_loss: 1.1767, Train_acc: 0.6583, Test_loss: 1.1610, Test_acc: 0.6620
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001233157_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001233157_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.1748855440792914, 0.6544465348044949, 1.1605324746436403, 0.660482168647928]
model_source_only: [2.4124134014898253, 0.36592600125421604, 2.4283018624195853, 0.3691812020886568]

************************************************************************************************************************

uid: 20231002000052
FL pretrained model will be saved at ./models/lenet_mnist_20231002000052.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.343% / Loss: 2.3012 /Time: 4.84s
======================================================================================================

= Test = round: 0 / acc: 10.690% / loss: 2.2988 / Time: 1.26s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.012701433151960373, avg_sq_norm_grad = 0.944974958896637,                  max_norm_grad = 1.0558173656463623, var_grad = 0.9322735071182251
round 2: local lr = 0.01, sq_norm_avg_grad = 0.013467102311551571, avg_sq_norm_grad = 1.0059038400650024,                  max_norm_grad = 1.087937593460083, var_grad = 0.9924367666244507
round 3: local lr = 0.01, sq_norm_avg_grad = 0.0144027229398489, avg_sq_norm_grad = 1.0842293500900269,                  max_norm_grad = 1.1336973905563354, var_grad = 1.069826602935791
round 4: local lr = 0.01, sq_norm_avg_grad = 0.015767693519592285, avg_sq_norm_grad = 1.186574101448059,                  max_norm_grad = 1.1921466588974, var_grad = 1.1708064079284668

>>> Round:    5 / Acc: 16.493% / Loss: 2.2923 /Time: 4.47s
======================================================================================================

= Test = round: 5 / acc: 16.730% / loss: 2.2900 / Time: 0.94s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.017642496153712273, avg_sq_norm_grad = 1.3223917484283447,                  max_norm_grad = 1.2657442092895508, var_grad = 1.3047492504119873
round 6: local lr = 0.01, sq_norm_avg_grad = 0.020218389108777046, avg_sq_norm_grad = 1.5078469514846802,                  max_norm_grad = 1.3583860397338867, var_grad = 1.4876285791397095
round 7: local lr = 0.01, sq_norm_avg_grad = 0.024036826565861702, avg_sq_norm_grad = 1.7726194858551025,                  max_norm_grad = 1.4812840223312378, var_grad = 1.7485826015472412
round 8: local lr = 0.01, sq_norm_avg_grad = 0.02988988347351551, avg_sq_norm_grad = 2.176494836807251,                  max_norm_grad = 1.646657943725586, var_grad = 2.1466050148010254
round 9: local lr = 0.01, sq_norm_avg_grad = 0.03912416473031044, avg_sq_norm_grad = 2.8572278022766113,                  max_norm_grad = 1.8900178670883179, var_grad = 2.818103551864624

>>> Round:   10 / Acc: 17.762% / Loss: 2.2704 /Time: 4.64s
======================================================================================================

= Test = round: 10 / acc: 18.210% / loss: 2.2687 / Time: 0.94s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.05285030975937843, avg_sq_norm_grad = 4.128818988800049,                  max_norm_grad = 2.285315990447998, var_grad = 4.0759687423706055
round 11: local lr = 0.01, sq_norm_avg_grad = 0.08145318180322647, avg_sq_norm_grad = 6.901040077209473,                  max_norm_grad = 2.981813907623291, var_grad = 6.819586753845215
round 12: local lr = 0.01, sq_norm_avg_grad = 0.16174794733524323, avg_sq_norm_grad = 14.762453079223633,                  max_norm_grad = 4.414835453033447, var_grad = 14.60070514678955
round 13: local lr = 0.01, sq_norm_avg_grad = 0.40338706970214844, avg_sq_norm_grad = 39.34638214111328,                  max_norm_grad = 7.298205375671387, var_grad = 38.9429931640625
round 14: local lr = 0.01, sq_norm_avg_grad = 0.9161560535430908, avg_sq_norm_grad = 90.5846939086914,                  max_norm_grad = 11.117981910705566, var_grad = 89.66854095458984

>>> Round:   15 / Acc: 19.280% / Loss: 2.1448 /Time: 4.73s
======================================================================================================

= Test = round: 15 / acc: 20.510% / loss: 2.1414 / Time: 0.85s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.552514672279358, avg_sq_norm_grad = 135.128173828125,                  max_norm_grad = 13.640713691711426, var_grad = 133.57565307617188
round 16: local lr = 0.01, sq_norm_avg_grad = 2.041978120803833, avg_sq_norm_grad = 139.22923278808594,                  max_norm_grad = 13.953583717346191, var_grad = 137.187255859375
round 17: local lr = 0.01, sq_norm_avg_grad = 2.5896475315093994, avg_sq_norm_grad = 141.75889587402344,                  max_norm_grad = 14.185969352722168, var_grad = 139.16925048828125
round 18: local lr = 0.01, sq_norm_avg_grad = 2.990109920501709, avg_sq_norm_grad = 145.8187713623047,                  max_norm_grad = 14.444501876831055, var_grad = 142.8286590576172
round 19: local lr = 0.01, sq_norm_avg_grad = 3.266451358795166, avg_sq_norm_grad = 145.55540466308594,                  max_norm_grad = 14.42878532409668, var_grad = 142.28895568847656

>>> Round:   20 / Acc: 50.517% / Loss: 1.9759 /Time: 5.55s
======================================================================================================

= Test = round: 20 / acc: 52.010% / loss: 1.9627 / Time: 1.17s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 3.6129672527313232, avg_sq_norm_grad = 144.94728088378906,                  max_norm_grad = 14.426654815673828, var_grad = 141.33432006835938
round 21: local lr = 0.01, sq_norm_avg_grad = 3.4463846683502197, avg_sq_norm_grad = 142.8361358642578,                  max_norm_grad = 14.339638710021973, var_grad = 139.38975524902344
round 22: local lr = 0.01, sq_norm_avg_grad = 3.876591444015503, avg_sq_norm_grad = 143.24510192871094,                  max_norm_grad = 14.265653610229492, var_grad = 139.36851501464844
round 23: local lr = 0.01, sq_norm_avg_grad = 3.765442132949829, avg_sq_norm_grad = 145.30349731445312,                  max_norm_grad = 14.325268745422363, var_grad = 141.53805541992188
round 24: local lr = 0.01, sq_norm_avg_grad = 4.259543418884277, avg_sq_norm_grad = 147.01031494140625,                  max_norm_grad = 14.303617477416992, var_grad = 142.7507781982422

>>> Round:   25 / Acc: 65.002% / Loss: 1.6936 /Time: 5.68s
======================================================================================================

= Test = round: 25 / acc: 66.560% / loss: 1.6693 / Time: 0.99s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 4.5945048332214355, avg_sq_norm_grad = 148.72122192382812,                  max_norm_grad = 14.532769203186035, var_grad = 144.12672424316406
round 26: local lr = 0.01, sq_norm_avg_grad = 3.732440948486328, avg_sq_norm_grad = 143.568115234375,                  max_norm_grad = 14.204105377197266, var_grad = 139.83567810058594
round 27: local lr = 0.01, sq_norm_avg_grad = 3.646899700164795, avg_sq_norm_grad = 143.8656463623047,                  max_norm_grad = 14.018423080444336, var_grad = 140.21875
round 28: local lr = 0.01, sq_norm_avg_grad = 3.885854959487915, avg_sq_norm_grad = 144.00228881835938,                  max_norm_grad = 14.219971656799316, var_grad = 140.11643981933594
round 29: local lr = 0.01, sq_norm_avg_grad = 3.8088741302490234, avg_sq_norm_grad = 139.9899444580078,                  max_norm_grad = 14.01927375793457, var_grad = 136.1810760498047

>>> Round:   30 / Acc: 70.790% / Loss: 1.3876 /Time: 5.17s
======================================================================================================

= Test = round: 30 / acc: 72.560% / loss: 1.3539 / Time: 1.29s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 3.6061532497406006, avg_sq_norm_grad = 137.06256103515625,                  max_norm_grad = 13.942159652709961, var_grad = 133.45640563964844
round 31: local lr = 0.01, sq_norm_avg_grad = 3.8818821907043457, avg_sq_norm_grad = 134.6832275390625,                  max_norm_grad = 14.093680381774902, var_grad = 130.8013458251953
round 32: local lr = 0.01, sq_norm_avg_grad = 4.259585857391357, avg_sq_norm_grad = 132.96473693847656,                  max_norm_grad = 14.379859924316406, var_grad = 128.7051544189453
round 33: local lr = 0.01, sq_norm_avg_grad = 4.376213550567627, avg_sq_norm_grad = 133.07916259765625,                  max_norm_grad = 14.620500564575195, var_grad = 128.70294189453125
round 34: local lr = 0.01, sq_norm_avg_grad = 4.397216320037842, avg_sq_norm_grad = 131.51759338378906,                  max_norm_grad = 14.643912315368652, var_grad = 127.12037658691406

>>> Round:   35 / Acc: 72.312% / Loss: 1.1543 /Time: 5.21s
======================================================================================================

= Test = round: 35 / acc: 73.590% / loss: 1.1163 / Time: 0.99s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 4.763427734375, avg_sq_norm_grad = 129.10409545898438,                  max_norm_grad = 14.76637077331543, var_grad = 124.34066772460938
round 36: local lr = 0.01, sq_norm_avg_grad = 4.965211868286133, avg_sq_norm_grad = 130.73463439941406,                  max_norm_grad = 15.01306438446045, var_grad = 125.76942443847656
round 37: local lr = 0.01, sq_norm_avg_grad = 4.712452411651611, avg_sq_norm_grad = 125.42143249511719,                  max_norm_grad = 14.607304573059082, var_grad = 120.70897674560547
round 38: local lr = 0.01, sq_norm_avg_grad = 4.996367931365967, avg_sq_norm_grad = 122.32906341552734,                  max_norm_grad = 14.603537559509277, var_grad = 117.33269500732422
round 39: local lr = 0.01, sq_norm_avg_grad = 5.130627155303955, avg_sq_norm_grad = 120.11691284179688,                  max_norm_grad = 14.553921699523926, var_grad = 114.98628234863281

>>> Round:   40 / Acc: 73.347% / Loss: 0.9960 /Time: 5.20s
======================================================================================================

= Test = round: 40 / acc: 75.070% / loss: 0.9565 / Time: 1.02s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 5.9173583984375, avg_sq_norm_grad = 120.33318328857422,                  max_norm_grad = 14.893386840820312, var_grad = 114.41582489013672
round 41: local lr = 0.01, sq_norm_avg_grad = 5.379827976226807, avg_sq_norm_grad = 115.15042877197266,                  max_norm_grad = 14.434468269348145, var_grad = 109.77059936523438
round 42: local lr = 0.01, sq_norm_avg_grad = 5.546811103820801, avg_sq_norm_grad = 113.54151916503906,                  max_norm_grad = 14.40893840789795, var_grad = 107.99470520019531
round 43: local lr = 0.01, sq_norm_avg_grad = 6.783872604370117, avg_sq_norm_grad = 114.18424224853516,                  max_norm_grad = 14.690166473388672, var_grad = 107.4003677368164
round 44: local lr = 0.01, sq_norm_avg_grad = 6.06664514541626, avg_sq_norm_grad = 113.1745376586914,                  max_norm_grad = 14.626699447631836, var_grad = 107.10789489746094

>>> Round:   45 / Acc: 74.779% / Loss: 0.8848 /Time: 5.28s
======================================================================================================

= Test = round: 45 / acc: 76.590% / loss: 0.8463 / Time: 0.99s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 6.5905046463012695, avg_sq_norm_grad = 111.01388549804688,                  max_norm_grad = 14.610404014587402, var_grad = 104.42337799072266
round 46: local lr = 0.01, sq_norm_avg_grad = 5.9879536628723145, avg_sq_norm_grad = 111.08956909179688,                  max_norm_grad = 14.505023956298828, var_grad = 105.10161590576172
round 47: local lr = 0.01, sq_norm_avg_grad = 6.2196574211120605, avg_sq_norm_grad = 110.09674835205078,                  max_norm_grad = 14.516879081726074, var_grad = 103.87709045410156
round 48: local lr = 0.01, sq_norm_avg_grad = 6.567359447479248, avg_sq_norm_grad = 110.22333526611328,                  max_norm_grad = 14.780823707580566, var_grad = 103.65597534179688
round 49: local lr = 0.01, sq_norm_avg_grad = 6.9355034828186035, avg_sq_norm_grad = 107.04288482666016,                  max_norm_grad = 14.504645347595215, var_grad = 100.10738372802734

>>> Round:   50 / Acc: 75.993% / Loss: 0.8061 /Time: 5.37s
======================================================================================================

= Test = round: 50 / acc: 77.640% / loss: 0.7703 / Time: 1.07s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4523, Train_acc: 0.5528, Test_loss: 1.4370, Test_acc: 0.5609
Epoch: 006, Train_loss: 1.2271, Train_acc: 0.6241, Test_loss: 1.2113, Test_acc: 0.6305
Epoch: 011, Train_loss: 1.1919, Train_acc: 0.6508, Test_loss: 1.1750, Test_acc: 0.6583
Epoch: 016, Train_loss: 1.1920, Train_acc: 0.6496, Test_loss: 1.1763, Test_acc: 0.6545
Epoch: 021, Train_loss: 1.1887, Train_acc: 0.6493, Test_loss: 1.1727, Test_acc: 0.6548
Epoch: 026, Train_loss: 1.1784, Train_acc: 0.6502, Test_loss: 1.1640, Test_acc: 0.6578
Epoch: 031, Train_loss: 1.1874, Train_acc: 0.6475, Test_loss: 1.1753, Test_acc: 0.6474
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002000052_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002000052_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.1740846145322967, 0.654090608633752, 1.1591975738466904, 0.6603710698811244]
model_source_only: [2.4105373974143522, 0.36338367146319556, 2.4261920323386192, 0.36484835018331296]

************************************************************************************************************************

uid: 20231002003151
FL pretrained model will be saved at ./models/lenet_mnist_20231002003151.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.343% / Loss: 2.3012 /Time: 4.49s
======================================================================================================

= Test = round: 0 / acc: 10.690% / loss: 2.2988 / Time: 0.88s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.012686679139733315, avg_sq_norm_grad = 0.9452508091926575,                  max_norm_grad = 1.0559587478637695, var_grad = 0.9325641393661499
round 2: local lr = 0.01, sq_norm_avg_grad = 0.013437624089419842, avg_sq_norm_grad = 1.0063406229019165,                  max_norm_grad = 1.0882259607315063, var_grad = 0.9929029941558838
round 3: local lr = 0.01, sq_norm_avg_grad = 0.014396832324564457, avg_sq_norm_grad = 1.0847878456115723,                  max_norm_grad = 1.1340141296386719, var_grad = 1.070391058921814
round 4: local lr = 0.01, sq_norm_avg_grad = 0.015753446146845818, avg_sq_norm_grad = 1.187523603439331,                  max_norm_grad = 1.1928998231887817, var_grad = 1.1717702150344849

>>> Round:    5 / Acc: 16.555% / Loss: 2.2923 /Time: 4.58s
======================================================================================================

= Test = round: 5 / acc: 16.820% / loss: 2.2900 / Time: 0.86s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.01764007844030857, avg_sq_norm_grad = 1.323525071144104,                  max_norm_grad = 1.2660794258117676, var_grad = 1.3058849573135376
round 6: local lr = 0.01, sq_norm_avg_grad = 0.02022521197795868, avg_sq_norm_grad = 1.5091168880462646,                  max_norm_grad = 1.3586935997009277, var_grad = 1.4888917207717896
round 7: local lr = 0.01, sq_norm_avg_grad = 0.024017097428441048, avg_sq_norm_grad = 1.7723922729492188,                  max_norm_grad = 1.4809880256652832, var_grad = 1.7483751773834229
round 8: local lr = 0.01, sq_norm_avg_grad = 0.02987794019281864, avg_sq_norm_grad = 2.1751580238342285,                  max_norm_grad = 1.6452966928482056, var_grad = 2.145280122756958
round 9: local lr = 0.01, sq_norm_avg_grad = 0.039115529507398605, avg_sq_norm_grad = 2.851914405822754,                  max_norm_grad = 1.8870110511779785, var_grad = 2.8127989768981934

>>> Round:   10 / Acc: 17.792% / Loss: 2.2704 /Time: 4.59s
======================================================================================================

= Test = round: 10 / acc: 18.170% / loss: 2.2687 / Time: 0.87s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.052815232425928116, avg_sq_norm_grad = 4.112096309661865,                  max_norm_grad = 2.2797582149505615, var_grad = 4.059280872344971
round 11: local lr = 0.01, sq_norm_avg_grad = 0.08128818869590759, avg_sq_norm_grad = 6.881907939910889,                  max_norm_grad = 2.9794323444366455, var_grad = 6.800619602203369
round 12: local lr = 0.01, sq_norm_avg_grad = 0.15963904559612274, avg_sq_norm_grad = 14.629749298095703,                  max_norm_grad = 4.382110595703125, var_grad = 14.470109939575195
round 13: local lr = 0.01, sq_norm_avg_grad = 0.39049574732780457, avg_sq_norm_grad = 39.17772674560547,                  max_norm_grad = 7.271586894989014, var_grad = 38.7872314453125
round 14: local lr = 0.01, sq_norm_avg_grad = 0.9125548601150513, avg_sq_norm_grad = 90.53058624267578,                  max_norm_grad = 11.103187561035156, var_grad = 89.61803436279297

>>> Round:   15 / Acc: 21.638% / Loss: 2.1457 /Time: 4.62s
======================================================================================================

= Test = round: 15 / acc: 22.730% / loss: 2.1425 / Time: 0.86s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.6635509729385376, avg_sq_norm_grad = 136.2428741455078,                  max_norm_grad = 13.718128204345703, var_grad = 134.57933044433594
round 16: local lr = 0.01, sq_norm_avg_grad = 2.035050868988037, avg_sq_norm_grad = 140.11036682128906,                  max_norm_grad = 13.992091178894043, var_grad = 138.0753173828125
round 17: local lr = 0.01, sq_norm_avg_grad = 2.6088755130767822, avg_sq_norm_grad = 143.5413055419922,                  max_norm_grad = 14.308538436889648, var_grad = 140.93243408203125
round 18: local lr = 0.01, sq_norm_avg_grad = 2.9340004920959473, avg_sq_norm_grad = 142.26194763183594,                  max_norm_grad = 14.3351469039917, var_grad = 139.32794189453125
round 19: local lr = 0.01, sq_norm_avg_grad = 3.276676893234253, avg_sq_norm_grad = 140.4130859375,                  max_norm_grad = 14.282368659973145, var_grad = 137.13641357421875

>>> Round:   20 / Acc: 50.513% / Loss: 1.9774 /Time: 4.53s
======================================================================================================

= Test = round: 20 / acc: 52.140% / loss: 1.9640 / Time: 0.89s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 3.616267442703247, avg_sq_norm_grad = 142.8612518310547,                  max_norm_grad = 14.392134666442871, var_grad = 139.24497985839844
round 21: local lr = 0.01, sq_norm_avg_grad = 3.4685685634613037, avg_sq_norm_grad = 144.73548889160156,                  max_norm_grad = 14.42984390258789, var_grad = 141.2669219970703
round 22: local lr = 0.01, sq_norm_avg_grad = 3.585247755050659, avg_sq_norm_grad = 142.06488037109375,                  max_norm_grad = 14.26232624053955, var_grad = 138.47962951660156
round 23: local lr = 0.01, sq_norm_avg_grad = 3.911943197250366, avg_sq_norm_grad = 142.65904235839844,                  max_norm_grad = 14.31466007232666, var_grad = 138.74710083007812
round 24: local lr = 0.01, sq_norm_avg_grad = 4.031426906585693, avg_sq_norm_grad = 152.3056640625,                  max_norm_grad = 14.695308685302734, var_grad = 148.27423095703125

>>> Round:   25 / Acc: 65.201% / Loss: 1.6929 /Time: 4.70s
======================================================================================================

= Test = round: 25 / acc: 66.830% / loss: 1.6701 / Time: 0.95s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 4.1231689453125, avg_sq_norm_grad = 144.4201202392578,                  max_norm_grad = 14.253161430358887, var_grad = 140.2969512939453
round 26: local lr = 0.01, sq_norm_avg_grad = 4.405772686004639, avg_sq_norm_grad = 147.15716552734375,                  max_norm_grad = 14.225166320800781, var_grad = 142.7513885498047
round 27: local lr = 0.01, sq_norm_avg_grad = 4.215447425842285, avg_sq_norm_grad = 142.44224548339844,                  max_norm_grad = 14.105358123779297, var_grad = 138.22679138183594
round 28: local lr = 0.01, sq_norm_avg_grad = 3.718053102493286, avg_sq_norm_grad = 144.6481170654297,                  max_norm_grad = 13.9465970993042, var_grad = 140.93006896972656
round 29: local lr = 0.01, sq_norm_avg_grad = 3.854660987854004, avg_sq_norm_grad = 144.22166442871094,                  max_norm_grad = 14.06042766571045, var_grad = 140.36700439453125

>>> Round:   30 / Acc: 71.308% / Loss: 1.3810 /Time: 4.65s
======================================================================================================

= Test = round: 30 / acc: 72.880% / loss: 1.3475 / Time: 0.88s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 3.41742205619812, avg_sq_norm_grad = 140.01658630371094,                  max_norm_grad = 14.082610130310059, var_grad = 136.5991668701172
round 31: local lr = 0.01, sq_norm_avg_grad = 3.891799211502075, avg_sq_norm_grad = 139.63555908203125,                  max_norm_grad = 14.358280181884766, var_grad = 135.74375915527344
round 32: local lr = 0.01, sq_norm_avg_grad = 3.9571735858917236, avg_sq_norm_grad = 135.46376037597656,                  max_norm_grad = 14.437045097351074, var_grad = 131.506591796875
round 33: local lr = 0.01, sq_norm_avg_grad = 3.714787483215332, avg_sq_norm_grad = 132.5728759765625,                  max_norm_grad = 14.358922958374023, var_grad = 128.85809326171875
round 34: local lr = 0.01, sq_norm_avg_grad = 3.932455062866211, avg_sq_norm_grad = 133.52206420898438,                  max_norm_grad = 14.613656997680664, var_grad = 129.58961486816406

>>> Round:   35 / Acc: 72.771% / Loss: 1.1507 /Time: 4.68s
======================================================================================================

= Test = round: 35 / acc: 74.210% / loss: 1.1119 / Time: 0.85s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 4.021844863891602, avg_sq_norm_grad = 128.93466186523438,                  max_norm_grad = 14.509202003479004, var_grad = 124.9128189086914
round 36: local lr = 0.01, sq_norm_avg_grad = 4.406311511993408, avg_sq_norm_grad = 128.47657775878906,                  max_norm_grad = 14.790191650390625, var_grad = 124.07026672363281
round 37: local lr = 0.01, sq_norm_avg_grad = 4.8889479637146, avg_sq_norm_grad = 124.78334045410156,                  max_norm_grad = 14.707685470581055, var_grad = 119.89439392089844
round 38: local lr = 0.01, sq_norm_avg_grad = 5.57548189163208, avg_sq_norm_grad = 124.18712615966797,                  max_norm_grad = 14.863088607788086, var_grad = 118.61164093017578
round 39: local lr = 0.01, sq_norm_avg_grad = 4.7374162673950195, avg_sq_norm_grad = 120.0946273803711,                  max_norm_grad = 14.47605037689209, var_grad = 115.35720825195312

>>> Round:   40 / Acc: 73.921% / Loss: 0.9954 /Time: 4.50s
======================================================================================================

= Test = round: 40 / acc: 75.490% / loss: 0.9560 / Time: 0.83s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 5.303121566772461, avg_sq_norm_grad = 118.38877868652344,                  max_norm_grad = 14.668534278869629, var_grad = 113.08565521240234
round 41: local lr = 0.01, sq_norm_avg_grad = 5.756375312805176, avg_sq_norm_grad = 116.5193099975586,                  max_norm_grad = 14.716002464294434, var_grad = 110.76293182373047
round 42: local lr = 0.01, sq_norm_avg_grad = 5.561226844787598, avg_sq_norm_grad = 115.52863311767578,                  max_norm_grad = 14.53410530090332, var_grad = 109.9674072265625
round 43: local lr = 0.01, sq_norm_avg_grad = 6.149665832519531, avg_sq_norm_grad = 113.37853240966797,                  max_norm_grad = 14.52676010131836, var_grad = 107.22886657714844
round 44: local lr = 0.01, sq_norm_avg_grad = 6.529361724853516, avg_sq_norm_grad = 112.07870483398438,                  max_norm_grad = 14.470256805419922, var_grad = 105.54934692382812

>>> Round:   45 / Acc: 74.742% / Loss: 0.8820 /Time: 4.69s
======================================================================================================

= Test = round: 45 / acc: 76.650% / loss: 0.8427 / Time: 0.85s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 6.53041934967041, avg_sq_norm_grad = 111.83638763427734,                  max_norm_grad = 14.445669174194336, var_grad = 105.30596923828125
round 46: local lr = 0.01, sq_norm_avg_grad = 6.014486789703369, avg_sq_norm_grad = 109.4819107055664,                  max_norm_grad = 14.271117210388184, var_grad = 103.46742248535156
round 47: local lr = 0.01, sq_norm_avg_grad = 6.259145736694336, avg_sq_norm_grad = 107.55824279785156,                  max_norm_grad = 14.131390571594238, var_grad = 101.2990951538086
round 48: local lr = 0.01, sq_norm_avg_grad = 7.224504470825195, avg_sq_norm_grad = 107.7487564086914,                  max_norm_grad = 14.39384937286377, var_grad = 100.52425384521484
round 49: local lr = 0.01, sq_norm_avg_grad = 8.267624855041504, avg_sq_norm_grad = 108.33497619628906,                  max_norm_grad = 14.506600379943848, var_grad = 100.06735229492188

>>> Round:   50 / Acc: 76.129% / Loss: 0.8036 /Time: 4.28s
======================================================================================================

= Test = round: 50 / acc: 77.880% / loss: 0.7664 / Time: 0.82s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4437, Train_acc: 0.5491, Test_loss: 1.4277, Test_acc: 0.5589
Epoch: 006, Train_loss: 1.2124, Train_acc: 0.6423, Test_loss: 1.1980, Test_acc: 0.6484
Epoch: 011, Train_loss: 1.1871, Train_acc: 0.6507, Test_loss: 1.1693, Test_acc: 0.6543
Epoch: 016, Train_loss: 1.1825, Train_acc: 0.6517, Test_loss: 1.1685, Test_acc: 0.6565
Epoch: 021, Train_loss: 1.1750, Train_acc: 0.6562, Test_loss: 1.1594, Test_acc: 0.6619
Epoch: 026, Train_loss: 1.1782, Train_acc: 0.6521, Test_loss: 1.1621, Test_acc: 0.6596
Epoch: 031, Train_loss: 1.1799, Train_acc: 0.6501, Test_loss: 1.1662, Test_acc: 0.6599
Epoch: 036, Train_loss: 1.1828, Train_acc: 0.6443, Test_loss: 1.1696, Test_acc: 0.6549
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002003151_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002003151_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.1712295879653118, 0.6570397111913358, 1.1555861756147616, 0.6657038106877013]
model_source_only: [2.4010496852870746, 0.3631972339451874, 2.416087580871455, 0.36584823908454617]

************************************************************************************************************************

uid: 20231002005818
FL pretrained model will be saved at ./models/lenet_mnist_20231002005818.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.343% / Loss: 2.3012 /Time: 4.54s
======================================================================================================

= Test = round: 0 / acc: 10.690% / loss: 2.2988 / Time: 0.89s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.01269085519015789, avg_sq_norm_grad = 0.9455258250236511,                  max_norm_grad = 1.0563324689865112, var_grad = 0.9328349828720093
round 2: local lr = 0.01, sq_norm_avg_grad = 0.013449784368276596, avg_sq_norm_grad = 1.006692886352539,                  max_norm_grad = 1.0884143114089966, var_grad = 0.9932430982589722
round 3: local lr = 0.01, sq_norm_avg_grad = 0.0143930334597826, avg_sq_norm_grad = 1.085427165031433,                  max_norm_grad = 1.1342016458511353, var_grad = 1.0710340738296509
round 4: local lr = 0.01, sq_norm_avg_grad = 0.015753256157040596, avg_sq_norm_grad = 1.1889065504074097,                  max_norm_grad = 1.1935629844665527, var_grad = 1.173153281211853

>>> Round:    5 / Acc: 16.677% / Loss: 2.2923 /Time: 4.39s
======================================================================================================

= Test = round: 5 / acc: 16.920% / loss: 2.2900 / Time: 0.82s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.017650699242949486, avg_sq_norm_grad = 1.3258590698242188,                  max_norm_grad = 1.2670328617095947, var_grad = 1.3082083463668823
round 6: local lr = 0.01, sq_norm_avg_grad = 0.020235788077116013, avg_sq_norm_grad = 1.5111923217773438,                  max_norm_grad = 1.3598600625991821, var_grad = 1.4909565448760986
round 7: local lr = 0.01, sq_norm_avg_grad = 0.024061119183897972, avg_sq_norm_grad = 1.777201771736145,                  max_norm_grad = 1.482935905456543, var_grad = 1.7531406879425049
round 8: local lr = 0.01, sq_norm_avg_grad = 0.029967188835144043, avg_sq_norm_grad = 2.1854865550994873,                  max_norm_grad = 1.6498734951019287, var_grad = 2.155519485473633
round 9: local lr = 0.01, sq_norm_avg_grad = 0.03928568214178085, avg_sq_norm_grad = 2.8702385425567627,                  max_norm_grad = 1.8939052820205688, var_grad = 2.8309528827667236

>>> Round:   10 / Acc: 17.651% / Loss: 2.2703 /Time: 4.46s
======================================================================================================

= Test = round: 10 / acc: 18.000% / loss: 2.2686 / Time: 0.86s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.053092990070581436, avg_sq_norm_grad = 4.154860019683838,                  max_norm_grad = 2.294179677963257, var_grad = 4.101767063140869
round 11: local lr = 0.01, sq_norm_avg_grad = 0.08208426833152771, avg_sq_norm_grad = 6.971948623657227,                  max_norm_grad = 2.99809193611145, var_grad = 6.889864444732666
round 12: local lr = 0.01, sq_norm_avg_grad = 0.16366104781627655, avg_sq_norm_grad = 14.929335594177246,                  max_norm_grad = 4.442540645599365, var_grad = 14.765674591064453
round 13: local lr = 0.01, sq_norm_avg_grad = 0.3951447010040283, avg_sq_norm_grad = 39.892295837402344,                  max_norm_grad = 7.339017868041992, var_grad = 39.49715042114258
round 14: local lr = 0.01, sq_norm_avg_grad = 0.8356372117996216, avg_sq_norm_grad = 91.18695068359375,                  max_norm_grad = 11.119210243225098, var_grad = 90.35131072998047

>>> Round:   15 / Acc: 19.399% / Loss: 2.1457 /Time: 4.64s
======================================================================================================

= Test = round: 15 / acc: 20.560% / loss: 2.1426 / Time: 0.85s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.6583651304244995, avg_sq_norm_grad = 135.71542358398438,                  max_norm_grad = 13.76887035369873, var_grad = 134.0570526123047
round 16: local lr = 0.01, sq_norm_avg_grad = 2.346099376678467, avg_sq_norm_grad = 141.5980987548828,                  max_norm_grad = 14.207318305969238, var_grad = 139.2519989013672
round 17: local lr = 0.01, sq_norm_avg_grad = 2.853358030319214, avg_sq_norm_grad = 142.85452270507812,                  max_norm_grad = 14.384199142456055, var_grad = 140.00115966796875
round 18: local lr = 0.01, sq_norm_avg_grad = 2.8906619548797607, avg_sq_norm_grad = 142.4405059814453,                  max_norm_grad = 14.369372367858887, var_grad = 139.5498504638672
round 19: local lr = 0.01, sq_norm_avg_grad = 2.993989944458008, avg_sq_norm_grad = 142.60513305664062,                  max_norm_grad = 14.34992790222168, var_grad = 139.61114501953125

>>> Round:   20 / Acc: 50.910% / Loss: 1.9744 /Time: 4.49s
======================================================================================================

= Test = round: 20 / acc: 52.320% / loss: 1.9614 / Time: 0.90s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 3.236137628555298, avg_sq_norm_grad = 140.455810546875,                  max_norm_grad = 14.200389862060547, var_grad = 137.21966552734375
round 21: local lr = 0.01, sq_norm_avg_grad = 3.662595272064209, avg_sq_norm_grad = 142.4823760986328,                  max_norm_grad = 14.283620834350586, var_grad = 138.8197784423828
round 22: local lr = 0.01, sq_norm_avg_grad = 3.798299551010132, avg_sq_norm_grad = 146.1185302734375,                  max_norm_grad = 14.499309539794922, var_grad = 142.3202362060547
round 23: local lr = 0.01, sq_norm_avg_grad = 4.12148380279541, avg_sq_norm_grad = 145.6815643310547,                  max_norm_grad = 14.398796081542969, var_grad = 141.56007385253906
round 24: local lr = 0.01, sq_norm_avg_grad = 4.197958469390869, avg_sq_norm_grad = 146.75247192382812,                  max_norm_grad = 14.30749225616455, var_grad = 142.5545196533203

>>> Round:   25 / Acc: 65.100% / Loss: 1.6946 /Time: 4.56s
======================================================================================================

= Test = round: 25 / acc: 66.840% / loss: 1.6711 / Time: 0.92s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 4.227605819702148, avg_sq_norm_grad = 143.69766235351562,                  max_norm_grad = 14.112277030944824, var_grad = 139.47006225585938
round 26: local lr = 0.01, sq_norm_avg_grad = 4.351603984832764, avg_sq_norm_grad = 147.538330078125,                  max_norm_grad = 14.124302864074707, var_grad = 143.1867218017578
round 27: local lr = 0.01, sq_norm_avg_grad = 4.233877658843994, avg_sq_norm_grad = 144.7535858154297,                  max_norm_grad = 14.083657264709473, var_grad = 140.51971435546875
round 28: local lr = 0.01, sq_norm_avg_grad = 4.338207244873047, avg_sq_norm_grad = 142.47528076171875,                  max_norm_grad = 14.13912582397461, var_grad = 138.13706970214844
round 29: local lr = 0.01, sq_norm_avg_grad = 4.220505237579346, avg_sq_norm_grad = 143.60354614257812,                  max_norm_grad = 14.240939140319824, var_grad = 139.38304138183594

>>> Round:   30 / Acc: 70.256% / Loss: 1.3881 /Time: 4.77s
======================================================================================================

= Test = round: 30 / acc: 71.830% / loss: 1.3555 / Time: 0.88s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 4.162892818450928, avg_sq_norm_grad = 139.2912139892578,                  max_norm_grad = 14.264884948730469, var_grad = 135.12832641601562
round 31: local lr = 0.01, sq_norm_avg_grad = 4.56261682510376, avg_sq_norm_grad = 138.38916015625,                  max_norm_grad = 14.517041206359863, var_grad = 133.8265380859375
round 32: local lr = 0.01, sq_norm_avg_grad = 3.853598117828369, avg_sq_norm_grad = 135.54685974121094,                  max_norm_grad = 14.295201301574707, var_grad = 131.69326782226562
round 33: local lr = 0.01, sq_norm_avg_grad = 4.041648864746094, avg_sq_norm_grad = 134.27450561523438,                  max_norm_grad = 14.415708541870117, var_grad = 130.23284912109375
round 34: local lr = 0.01, sq_norm_avg_grad = 4.172379970550537, avg_sq_norm_grad = 132.4769287109375,                  max_norm_grad = 14.514741897583008, var_grad = 128.30455017089844

>>> Round:   35 / Acc: 72.638% / Loss: 1.1494 /Time: 4.73s
======================================================================================================

= Test = round: 35 / acc: 74.190% / loss: 1.1119 / Time: 0.92s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 4.346908092498779, avg_sq_norm_grad = 130.94175720214844,                  max_norm_grad = 14.607019424438477, var_grad = 126.5948486328125
round 36: local lr = 0.01, sq_norm_avg_grad = 3.6453065872192383, avg_sq_norm_grad = 126.34757232666016,                  max_norm_grad = 14.23790168762207, var_grad = 122.70226287841797
round 37: local lr = 0.01, sq_norm_avg_grad = 4.485997676849365, avg_sq_norm_grad = 126.10185241699219,                  max_norm_grad = 14.710652351379395, var_grad = 121.61585235595703
round 38: local lr = 0.01, sq_norm_avg_grad = 4.553567409515381, avg_sq_norm_grad = 123.98251342773438,                  max_norm_grad = 14.697399139404297, var_grad = 119.42894744873047
round 39: local lr = 0.01, sq_norm_avg_grad = 5.032454490661621, avg_sq_norm_grad = 120.61190795898438,                  max_norm_grad = 14.744050025939941, var_grad = 115.57945251464844

>>> Round:   40 / Acc: 73.784% / Loss: 0.9954 /Time: 4.85s
======================================================================================================

= Test = round: 40 / acc: 75.200% / loss: 0.9568 / Time: 0.89s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 5.614772796630859, avg_sq_norm_grad = 119.94093322753906,                  max_norm_grad = 14.953181266784668, var_grad = 114.32615661621094
round 41: local lr = 0.01, sq_norm_avg_grad = 5.630006313323975, avg_sq_norm_grad = 117.03547668457031,                  max_norm_grad = 14.798212051391602, var_grad = 111.40547180175781
round 42: local lr = 0.01, sq_norm_avg_grad = 6.074207782745361, avg_sq_norm_grad = 116.15593719482422,                  max_norm_grad = 14.87741470336914, var_grad = 110.08172607421875
round 43: local lr = 0.01, sq_norm_avg_grad = 6.460079669952393, avg_sq_norm_grad = 116.77989959716797,                  max_norm_grad = 15.013520240783691, var_grad = 110.31981658935547
round 44: local lr = 0.01, sq_norm_avg_grad = 6.348474979400635, avg_sq_norm_grad = 114.66495513916016,                  max_norm_grad = 14.913618087768555, var_grad = 108.31648254394531

>>> Round:   45 / Acc: 74.847% / Loss: 0.8847 /Time: 4.44s
======================================================================================================

= Test = round: 45 / acc: 76.500% / loss: 0.8469 / Time: 0.86s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 6.747722625732422, avg_sq_norm_grad = 112.20825958251953,                  max_norm_grad = 14.844383239746094, var_grad = 105.46054077148438
round 46: local lr = 0.01, sq_norm_avg_grad = 6.839524269104004, avg_sq_norm_grad = 109.32974243164062,                  max_norm_grad = 14.676950454711914, var_grad = 102.49021911621094
round 47: local lr = 0.01, sq_norm_avg_grad = 6.989799499511719, avg_sq_norm_grad = 109.25304412841797,                  max_norm_grad = 14.738288879394531, var_grad = 102.26324462890625
round 48: local lr = 0.01, sq_norm_avg_grad = 7.477553844451904, avg_sq_norm_grad = 107.3624496459961,                  max_norm_grad = 14.495147705078125, var_grad = 99.88489532470703
round 49: local lr = 0.01, sq_norm_avg_grad = 7.085971832275391, avg_sq_norm_grad = 105.5570068359375,                  max_norm_grad = 14.406116485595703, var_grad = 98.47103881835938

>>> Round:   50 / Acc: 76.387% / Loss: 0.7997 /Time: 4.56s
======================================================================================================

= Test = round: 50 / acc: 77.790% / loss: 0.7643 / Time: 0.92s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4480, Train_acc: 0.5554, Test_loss: 1.4297, Test_acc: 0.5636
Epoch: 006, Train_loss: 1.2186, Train_acc: 0.6349, Test_loss: 1.2005, Test_acc: 0.6442
Epoch: 011, Train_loss: 1.1929, Train_acc: 0.6439, Test_loss: 1.1800, Test_acc: 0.6511
Epoch: 016, Train_loss: 1.1817, Train_acc: 0.6554, Test_loss: 1.1668, Test_acc: 0.6593
Epoch: 021, Train_loss: 1.1791, Train_acc: 0.6473, Test_loss: 1.1636, Test_acc: 0.6555
Epoch: 026, Train_loss: 1.1808, Train_acc: 0.6550, Test_loss: 1.1654, Test_acc: 0.6598
Epoch: 031, Train_loss: 1.1838, Train_acc: 0.6470, Test_loss: 1.1695, Test_acc: 0.6549
Epoch: 036, Train_loss: 1.1812, Train_acc: 0.6475, Test_loss: 1.1673, Test_acc: 0.6524
Epoch: 041, Train_loss: 1.1820, Train_acc: 0.6435, Test_loss: 1.1702, Test_acc: 0.6519
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002005818_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002005818_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.1699001642811555, 0.6606159217640379, 1.1552892195861904, 0.6671480946561493]
model_source_only: [2.4137892335033526, 0.36760393891628956, 2.429651949699952, 0.36907010332185314]
fl_test_acc_mean 0.7761000000000001
model_source_only_test_acc_mean 0.3667592489723364
model_ft_test_acc_mean 0.6633040773247416
