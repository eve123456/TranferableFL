nohup: ignoring input
Using device: cuda:0
>>> Arguments:
	               algo : fedavgtl
	              alpha : 13.2316427
	         batch_size : 64
	  clients_per_round : 100
	            dataset : mnist_all_data_1_equal_niid
	             device : cuda:0
	                dis : 
	     early_stopping : 10
	         eval_every : 5
	      ft_batch_size : 128
	         ft_dataset : mnist-m
	          ft_epochs : 200
	              ft_lr : 0.001
	              ft_wd : 0.0
	                gpu : True
	        input_shape : (1, 28, 28)
	             last_k : 1
	                 lr : 0.01
	              model : lenet
	             n_init : 10
	          noaverage : False
	               noft : False
	            noprint : False
	          num_class : 10
	          num_epoch : 1
	          num_round : 200
	             opt_lr : False
	pretrain_model_path : ./models/lenet_mnist_20230928202628.pt
	              reg_J : True
	         reg_J_coef : 0.01
	             repeat : 1
	               seed : 0
	                 wd : 0.0
load pretrained model from ./models/lenet_mnist_20230928202628.pt

************************************************************************************************************************

uid: 20231001164600
>>> Training model_ft
>>> Evaluating model_source_only
model_source_only: [2.275725519084738, 0.44360265080252875, 2.309170228721645, 0.44039551160982116]
model_source_only_test_acc_mean 0.44039551160982116
>>> Training model_ft
Epoch: 001, Train_loss: 1.1635, Train_acc: 0.6430, Test_loss: 1.1486, Test_acc: 0.6456
Epoch: 006, Train_loss: 0.9014, Train_acc: 0.7229, Test_loss: 0.9157, Test_acc: 0.7190
Epoch: 011, Train_loss: 0.8493, Train_acc: 0.7338, Test_loss: 0.8747, Test_acc: 0.7261
Epoch: 016, Train_loss: 0.8025, Train_acc: 0.7468, Test_loss: 0.8442, Test_acc: 0.7397
Epoch: 021, Train_loss: 0.7615, Train_acc: 0.7582, Test_loss: 0.8126, Test_acc: 0.7487
Epoch: 026, Train_loss: 0.7407, Train_acc: 0.7648, Test_loss: 0.7977, Test_acc: 0.7511
Epoch: 031, Train_loss: 0.7237, Train_acc: 0.7686, Test_loss: 0.7980, Test_acc: 0.7526
Epoch: 036, Train_loss: 0.7185, Train_acc: 0.7691, Test_loss: 0.7972, Test_acc: 0.7495
Epoch: 041, Train_loss: 0.6955, Train_acc: 0.7757, Test_loss: 0.7801, Test_acc: 0.7548
Epoch: 046, Train_loss: 0.6891, Train_acc: 0.7780, Test_loss: 0.7840, Test_acc: 0.7555
Epoch: 051, Train_loss: 0.6742, Train_acc: 0.7830, Test_loss: 0.7806, Test_acc: 0.7576
Epoch: 056, Train_loss: 0.6666, Train_acc: 0.7854, Test_loss: 0.7792, Test_acc: 0.7565
Epoch: 061, Train_loss: 0.6709, Train_acc: 0.7816, Test_loss: 0.7898, Test_acc: 0.7527
Epoch: 066, Train_loss: 0.6667, Train_acc: 0.7828, Test_loss: 0.7892, Test_acc: 0.7561
Epoch: 071, Train_loss: 0.6566, Train_acc: 0.7864, Test_loss: 0.7916, Test_acc: 0.7558
Epoch: 076, Train_loss: 0.6658, Train_acc: 0.7834, Test_loss: 0.8048, Test_acc: 0.7500
Epoch: 081, Train_loss: 0.6441, Train_acc: 0.7916, Test_loss: 0.7931, Test_acc: 0.7551
Epoch: 086, Train_loss: 0.6381, Train_acc: 0.7926, Test_loss: 0.7892, Test_acc: 0.7600
Epoch: 091, Train_loss: 0.6353, Train_acc: 0.7924, Test_loss: 0.7870, Test_acc: 0.7576
Epoch: 096, Train_loss: 0.6292, Train_acc: 0.7942, Test_loss: 0.7929, Test_acc: 0.7587
Epoch: 101, Train_loss: 0.6257, Train_acc: 0.7959, Test_loss: 0.7899, Test_acc: 0.7592
Epoch: 106, Train_loss: 0.6276, Train_acc: 0.7936, Test_loss: 0.7948, Test_acc: 0.7559
Epoch: 111, Train_loss: 0.6179, Train_acc: 0.7988, Test_loss: 0.8008, Test_acc: 0.7576
Epoch: 116, Train_loss: 0.6253, Train_acc: 0.7954, Test_loss: 0.8052, Test_acc: 0.7566
Epoch: 121, Train_loss: 0.6183, Train_acc: 0.7982, Test_loss: 0.8093, Test_acc: 0.7597
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001164600_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001164600_model_ft.pt.
>>> Fine-tuning done!
model_ft: [0.6137696021944888, 0.7993254351621158, 0.7946717830356419, 0.7594711698700144]
model_ft_test_acc_mean 0.7594711698700144
