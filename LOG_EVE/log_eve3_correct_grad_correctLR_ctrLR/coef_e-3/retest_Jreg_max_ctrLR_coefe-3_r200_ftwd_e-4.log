nohup: ignoring input
uid: 20231003175522
Using device: cuda:0
>>> Arguments:
	               algo : fedavgtl
	              alpha : 13.2316427
	         batch_size : 64
	  clients_per_round : 100
	            dataset : mnist_all_data_1_equal_niid
	             device : cuda:0
	                dis : 
	     early_stopping : 10
	         eval_every : 5
	      ft_batch_size : 128
	         ft_dataset : mnist-m
	          ft_epochs : 200
	              ft_lr : 0.001
	              ft_wd : 0.0001
	                gpu : True
	        input_shape : (1, 28, 28)
	             last_k : 1
	                 lr : 0.01
	              model : lenet
	             n_init : 10
	          noaverage : False
	               noft : False
	            noprint : False
	          num_class : 10
	          num_epoch : 1
	          num_round : 200
	             opt_lr : False
	pretrain_model_path : ./models/lenet_mnist_20230928202628.pt
	              reg_J : True
	         reg_J_coef : 0.01
	             repeat : 5
	               seed : 0
	                 wd : 0.0
load pretrained model from ./models/lenet_mnist_20230928202628.pt

************************************************************************************************************************

>>> Training model_ft
>>> Evaluating model_source_only
model_source_only: [2.275725519084738, 0.44360265080252875, 2.309170228721645, 0.44039551160982116]
model_source_only_test_acc_mean 0.08807910232196423
>>> Training model_ft
Epoch: 001, Train_loss: 1.1598, Train_acc: 0.6430, Test_loss: 1.1445, Test_acc: 0.6467
Epoch: 006, Train_loss: 0.9034, Train_acc: 0.7221, Test_loss: 0.9177, Test_acc: 0.7205
Epoch: 011, Train_loss: 0.8526, Train_acc: 0.7338, Test_loss: 0.8773, Test_acc: 0.7278
Epoch: 016, Train_loss: 0.8067, Train_acc: 0.7469, Test_loss: 0.8464, Test_acc: 0.7364
Epoch: 021, Train_loss: 0.7663, Train_acc: 0.7577, Test_loss: 0.8166, Test_acc: 0.7454
Epoch: 026, Train_loss: 0.7466, Train_acc: 0.7633, Test_loss: 0.8054, Test_acc: 0.7500
Epoch: 031, Train_loss: 0.7289, Train_acc: 0.7683, Test_loss: 0.7994, Test_acc: 0.7532
Epoch: 036, Train_loss: 0.7286, Train_acc: 0.7641, Test_loss: 0.8038, Test_acc: 0.7466
Epoch: 041, Train_loss: 0.6997, Train_acc: 0.7762, Test_loss: 0.7808, Test_acc: 0.7561
Epoch: 046, Train_loss: 0.6920, Train_acc: 0.7773, Test_loss: 0.7826, Test_acc: 0.7572
Epoch: 051, Train_loss: 0.6826, Train_acc: 0.7808, Test_loss: 0.7821, Test_acc: 0.7584
Epoch: 056, Train_loss: 0.6803, Train_acc: 0.7803, Test_loss: 0.7829, Test_acc: 0.7555
Epoch: 061, Train_loss: 0.6829, Train_acc: 0.7781, Test_loss: 0.7901, Test_acc: 0.7527
Epoch: 066, Train_loss: 0.6757, Train_acc: 0.7811, Test_loss: 0.7853, Test_acc: 0.7569
Epoch: 071, Train_loss: 0.6681, Train_acc: 0.7839, Test_loss: 0.7891, Test_acc: 0.7537
Epoch: 076, Train_loss: 0.6683, Train_acc: 0.7826, Test_loss: 0.7925, Test_acc: 0.7561
Epoch: 081, Train_loss: 0.6509, Train_acc: 0.7882, Test_loss: 0.7802, Test_acc: 0.7592
Epoch: 086, Train_loss: 0.6540, Train_acc: 0.7871, Test_loss: 0.7850, Test_acc: 0.7597
Epoch: 091, Train_loss: 0.6502, Train_acc: 0.7893, Test_loss: 0.7827, Test_acc: 0.7601
Epoch: 096, Train_loss: 0.6406, Train_acc: 0.7911, Test_loss: 0.7790, Test_acc: 0.7568
Epoch: 101, Train_loss: 0.6440, Train_acc: 0.7902, Test_loss: 0.7791, Test_acc: 0.7594
Epoch: 106, Train_loss: 0.6360, Train_acc: 0.7927, Test_loss: 0.7775, Test_acc: 0.7584
Epoch: 111, Train_loss: 0.6376, Train_acc: 0.7917, Test_loss: 0.7862, Test_acc: 0.7581
Epoch: 116, Train_loss: 0.6338, Train_acc: 0.7952, Test_loss: 0.7832, Test_acc: 0.7598
Epoch: 121, Train_loss: 0.6271, Train_acc: 0.7972, Test_loss: 0.7829, Test_acc: 0.7606
Epoch: 126, Train_loss: 0.6342, Train_acc: 0.7929, Test_loss: 0.7847, Test_acc: 0.7552
Epoch: 131, Train_loss: 0.6316, Train_acc: 0.7935, Test_loss: 0.7824, Test_acc: 0.7605
Epoch: 136, Train_loss: 0.6208, Train_acc: 0.7983, Test_loss: 0.7747, Test_acc: 0.7609
Epoch: 141, Train_loss: 0.6327, Train_acc: 0.7939, Test_loss: 0.7947, Test_acc: 0.7578
Epoch: 146, Train_loss: 0.6245, Train_acc: 0.7962, Test_loss: 0.7882, Test_acc: 0.7581
Epoch: 151, Train_loss: 0.6155, Train_acc: 0.8003, Test_loss: 0.7800, Test_acc: 0.7611
Epoch: 156, Train_loss: 0.6190, Train_acc: 0.7969, Test_loss: 0.7862, Test_acc: 0.7572
Epoch: 161, Train_loss: 0.6227, Train_acc: 0.7959, Test_loss: 0.7887, Test_acc: 0.7532
Epoch: 166, Train_loss: 0.6113, Train_acc: 0.8002, Test_loss: 0.7802, Test_acc: 0.7618
Epoch: 171, Train_loss: 0.6164, Train_acc: 0.7974, Test_loss: 0.7840, Test_acc: 0.7578
Epoch: 176, Train_loss: 0.6154, Train_acc: 0.7993, Test_loss: 0.7894, Test_acc: 0.7604
Epoch: 181, Train_loss: 0.6089, Train_acc: 0.8019, Test_loss: 0.7840, Test_acc: 0.7592
Epoch: 186, Train_loss: 0.6152, Train_acc: 0.7981, Test_loss: 0.7910, Test_acc: 0.7575
Epoch: 191, Train_loss: 0.6058, Train_acc: 0.8006, Test_loss: 0.7865, Test_acc: 0.7591
Epoch: 196, Train_loss: 0.6043, Train_acc: 0.8019, Test_loss: 0.7839, Test_acc: 0.7586
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231003175522_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231003175522_model_ft.pt.
>>> Fine-tuning done!
model_ft: [0.6039919645890824, 0.8025457195640752, 0.7847731490424442, 0.7613598489056771]

************************************************************************************************************************

>>> Training model_ft
>>> Evaluating model_source_only
model_source_only: [2.2757255249723647, 0.44360265080252875, 2.309170228721645, 0.44039551160982116]
model_source_only_test_acc_mean 0.17615820464392845
>>> Training model_ft
Epoch: 001, Train_loss: 1.1573, Train_acc: 0.6426, Test_loss: 1.1456, Test_acc: 0.6443
Epoch: 006, Train_loss: 0.8962, Train_acc: 0.7241, Test_loss: 0.9071, Test_acc: 0.7223
Epoch: 011, Train_loss: 0.8327, Train_acc: 0.7378, Test_loss: 0.8614, Test_acc: 0.7305
Epoch: 016, Train_loss: 0.7945, Train_acc: 0.7494, Test_loss: 0.8347, Test_acc: 0.7431
Epoch: 021, Train_loss: 0.7671, Train_acc: 0.7585, Test_loss: 0.8195, Test_acc: 0.7480
Epoch: 026, Train_loss: 0.7448, Train_acc: 0.7643, Test_loss: 0.8041, Test_acc: 0.7481
Epoch: 031, Train_loss: 0.7228, Train_acc: 0.7699, Test_loss: 0.7911, Test_acc: 0.7527
Epoch: 036, Train_loss: 0.7213, Train_acc: 0.7684, Test_loss: 0.7959, Test_acc: 0.7524
Epoch: 041, Train_loss: 0.6956, Train_acc: 0.7777, Test_loss: 0.7760, Test_acc: 0.7566
Epoch: 046, Train_loss: 0.6893, Train_acc: 0.7785, Test_loss: 0.7742, Test_acc: 0.7592
Epoch: 051, Train_loss: 0.6773, Train_acc: 0.7810, Test_loss: 0.7760, Test_acc: 0.7569
Epoch: 056, Train_loss: 0.6966, Train_acc: 0.7732, Test_loss: 0.7977, Test_acc: 0.7478
Epoch: 061, Train_loss: 0.6639, Train_acc: 0.7859, Test_loss: 0.7741, Test_acc: 0.7592
Epoch: 066, Train_loss: 0.6666, Train_acc: 0.7850, Test_loss: 0.7794, Test_acc: 0.7546
Epoch: 071, Train_loss: 0.6507, Train_acc: 0.7897, Test_loss: 0.7653, Test_acc: 0.7596
Epoch: 076, Train_loss: 0.6483, Train_acc: 0.7913, Test_loss: 0.7664, Test_acc: 0.7625
Epoch: 081, Train_loss: 0.6477, Train_acc: 0.7907, Test_loss: 0.7684, Test_acc: 0.7556
Epoch: 086, Train_loss: 0.6548, Train_acc: 0.7856, Test_loss: 0.7800, Test_acc: 0.7565
Epoch: 091, Train_loss: 0.6433, Train_acc: 0.7904, Test_loss: 0.7733, Test_acc: 0.7581
Epoch: 096, Train_loss: 0.6427, Train_acc: 0.7911, Test_loss: 0.7735, Test_acc: 0.7562
Epoch: 101, Train_loss: 0.6334, Train_acc: 0.7932, Test_loss: 0.7760, Test_acc: 0.7565
Epoch: 106, Train_loss: 0.6380, Train_acc: 0.7929, Test_loss: 0.7770, Test_acc: 0.7584
Epoch: 111, Train_loss: 0.6331, Train_acc: 0.7943, Test_loss: 0.7790, Test_acc: 0.7578
Epoch: 116, Train_loss: 0.6304, Train_acc: 0.7947, Test_loss: 0.7735, Test_acc: 0.7620
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231003175522_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231003175522_model_ft.pt.
>>> Fine-tuning done!
model_ft: [0.6259752153012734, 0.7965627701225403, 0.7672890031513884, 0.7632485279413399]

************************************************************************************************************************

>>> Training model_ft
>>> Evaluating model_source_only
model_source_only: [2.275725517205708, 0.44360265080252875, 2.309170228721645, 0.44039551160982116]
model_source_only_test_acc_mean 0.26423730696589265
>>> Training model_ft
Epoch: 001, Train_loss: 1.1434, Train_acc: 0.6473, Test_loss: 1.1333, Test_acc: 0.6494
Epoch: 006, Train_loss: 0.9156, Train_acc: 0.7130, Test_loss: 0.9251, Test_acc: 0.7105
Epoch: 011, Train_loss: 0.8289, Train_acc: 0.7398, Test_loss: 0.8588, Test_acc: 0.7353
Epoch: 016, Train_loss: 0.7888, Train_acc: 0.7519, Test_loss: 0.8294, Test_acc: 0.7433
Epoch: 021, Train_loss: 0.7570, Train_acc: 0.7603, Test_loss: 0.8095, Test_acc: 0.7497
Epoch: 026, Train_loss: 0.7419, Train_acc: 0.7659, Test_loss: 0.7983, Test_acc: 0.7519
Epoch: 031, Train_loss: 0.7286, Train_acc: 0.7666, Test_loss: 0.7991, Test_acc: 0.7488
Epoch: 036, Train_loss: 0.7148, Train_acc: 0.7697, Test_loss: 0.7954, Test_acc: 0.7539
Epoch: 041, Train_loss: 0.6960, Train_acc: 0.7750, Test_loss: 0.7804, Test_acc: 0.7541
Epoch: 046, Train_loss: 0.6957, Train_acc: 0.7777, Test_loss: 0.7858, Test_acc: 0.7547
Epoch: 051, Train_loss: 0.6812, Train_acc: 0.7812, Test_loss: 0.7824, Test_acc: 0.7564
Epoch: 056, Train_loss: 0.6791, Train_acc: 0.7797, Test_loss: 0.7792, Test_acc: 0.7591
Epoch: 061, Train_loss: 0.6647, Train_acc: 0.7854, Test_loss: 0.7725, Test_acc: 0.7559
Epoch: 066, Train_loss: 0.6572, Train_acc: 0.7859, Test_loss: 0.7676, Test_acc: 0.7630
