nohup: ignoring input
working!
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.01
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 10
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 40
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.001
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 5
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

repeat:1/5
using torch seed 10
uid: 20231003230015
FL pretrained model will be saved at ./models/lenet_mnist_20231003230015.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.186% / Loss: 2.3040 /Time: 4.42s
======================================================================================================

= Test = round: 0 / acc: 11.400% / loss: 2.3016 / Time: 0.85s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.05529479682445526, avg_sq_norm_grad = 1.7111611366271973,                  max_norm_grad = 1.4722062349319458, var_grad = 1.6558663845062256
round 2: local lr = 0.01, sq_norm_avg_grad = 0.06856617331504822, avg_sq_norm_grad = 2.3469460010528564,                  max_norm_grad = 1.7770131826400757, var_grad = 2.2783799171447754
round 3: local lr = 0.01, sq_norm_avg_grad = 0.09568662941455841, avg_sq_norm_grad = 3.6393890380859375,                  max_norm_grad = 2.230839729309082, var_grad = 3.5437023639678955
round 4: local lr = 0.01, sq_norm_avg_grad = 0.1572040617465973, avg_sq_norm_grad = 6.788222312927246,                  max_norm_grad = 3.0734989643096924, var_grad = 6.631018161773682

>>> Round:    5 / Acc: 19.946% / Loss: 2.2499 /Time: 4.39s
======================================================================================================

= Test = round: 5 / acc: 21.610% / loss: 2.2427 / Time: 0.88s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.3131919205188751, avg_sq_norm_grad = 16.479829788208008,                  max_norm_grad = 4.822052001953125, var_grad = 16.166637420654297
round 6: local lr = 0.01, sq_norm_avg_grad = 0.7513536214828491, avg_sq_norm_grad = 47.566131591796875,                  max_norm_grad = 8.114325523376465, var_grad = 46.81477737426758
round 7: local lr = 0.01, sq_norm_avg_grad = 1.3704228401184082, avg_sq_norm_grad = 108.48210906982422,                  max_norm_grad = 12.247425079345703, var_grad = 107.11168670654297
round 8: local lr = 0.01, sq_norm_avg_grad = 1.615106225013733, avg_sq_norm_grad = 142.20889282226562,                  max_norm_grad = 14.121421813964844, var_grad = 140.59378051757812
round 9: local lr = 0.01, sq_norm_avg_grad = 2.2002480030059814, avg_sq_norm_grad = 144.28567504882812,                  max_norm_grad = 14.54314136505127, var_grad = 142.08543395996094

>>> Round:   10 / Acc: 45.768% / Loss: 2.0178 /Time: 4.11s
======================================================================================================

= Test = round: 10 / acc: 48.010% / loss: 2.0030 / Time: 0.80s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 2.7916646003723145, avg_sq_norm_grad = 146.3672637939453,                  max_norm_grad = 14.810043334960938, var_grad = 143.57559204101562
round 11: local lr = 0.01, sq_norm_avg_grad = 3.222207546234131, avg_sq_norm_grad = 148.7634735107422,                  max_norm_grad = 14.88910961151123, var_grad = 145.541259765625
round 12: local lr = 0.01, sq_norm_avg_grad = 4.288365840911865, avg_sq_norm_grad = 153.15492248535156,                  max_norm_grad = 15.300914764404297, var_grad = 148.86656188964844
round 13: local lr = 0.01, sq_norm_avg_grad = 4.81947660446167, avg_sq_norm_grad = 154.47982788085938,                  max_norm_grad = 15.458048820495605, var_grad = 149.6603546142578
round 14: local lr = 0.01, sq_norm_avg_grad = 4.829843521118164, avg_sq_norm_grad = 154.45166015625,                  max_norm_grad = 15.485017776489258, var_grad = 149.62181091308594

>>> Round:   15 / Acc: 57.415% / Loss: 1.7128 /Time: 4.37s
======================================================================================================

= Test = round: 15 / acc: 59.440% / loss: 1.6850 / Time: 0.86s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 4.803691864013672, avg_sq_norm_grad = 152.65089416503906,                  max_norm_grad = 15.417387008666992, var_grad = 147.84719848632812
round 16: local lr = 0.01, sq_norm_avg_grad = 5.236623764038086, avg_sq_norm_grad = 157.67906188964844,                  max_norm_grad = 15.656588554382324, var_grad = 152.44244384765625
round 17: local lr = 0.01, sq_norm_avg_grad = 5.2685956954956055, avg_sq_norm_grad = 153.70494079589844,                  max_norm_grad = 15.421258926391602, var_grad = 148.43634033203125
round 18: local lr = 0.01, sq_norm_avg_grad = 5.514389514923096, avg_sq_norm_grad = 151.3140411376953,                  max_norm_grad = 15.510637283325195, var_grad = 145.79965209960938
round 19: local lr = 0.01, sq_norm_avg_grad = 5.465738296508789, avg_sq_norm_grad = 151.3246612548828,                  max_norm_grad = 15.737512588500977, var_grad = 145.85891723632812

>>> Round:   20 / Acc: 64.692% / Loss: 1.3872 /Time: 4.27s
======================================================================================================

= Test = round: 20 / acc: 66.770% / loss: 1.3474 / Time: 0.82s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 5.239931583404541, avg_sq_norm_grad = 152.63832092285156,                  max_norm_grad = 15.744309425354004, var_grad = 147.3983917236328
round 21: local lr = 0.01, sq_norm_avg_grad = 5.5829877853393555, avg_sq_norm_grad = 150.05433654785156,                  max_norm_grad = 15.798417091369629, var_grad = 144.47134399414062
round 22: local lr = 0.01, sq_norm_avg_grad = 5.543671607971191, avg_sq_norm_grad = 145.65335083007812,                  max_norm_grad = 15.695536613464355, var_grad = 140.10968017578125
round 23: local lr = 0.01, sq_norm_avg_grad = 5.324534893035889, avg_sq_norm_grad = 145.0939178466797,                  max_norm_grad = 15.655506134033203, var_grad = 139.76937866210938
round 24: local lr = 0.01, sq_norm_avg_grad = 5.099308490753174, avg_sq_norm_grad = 137.36627197265625,                  max_norm_grad = 15.414366722106934, var_grad = 132.2669677734375

>>> Round:   25 / Acc: 68.819% / Loss: 1.1543 /Time: 4.30s
======================================================================================================

= Test = round: 25 / acc: 70.590% / loss: 1.1088 / Time: 0.84s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 5.123715877532959, avg_sq_norm_grad = 133.89523315429688,                  max_norm_grad = 15.247658729553223, var_grad = 128.77151489257812
round 26: local lr = 0.01, sq_norm_avg_grad = 4.986846923828125, avg_sq_norm_grad = 134.77842712402344,                  max_norm_grad = 15.239812850952148, var_grad = 129.7915802001953
round 27: local lr = 0.01, sq_norm_avg_grad = 4.714470863342285, avg_sq_norm_grad = 129.6641387939453,                  max_norm_grad = 15.132356643676758, var_grad = 124.94966888427734
round 28: local lr = 0.01, sq_norm_avg_grad = 4.595261096954346, avg_sq_norm_grad = 125.232421875,                  max_norm_grad = 14.849739074707031, var_grad = 120.63716125488281
round 29: local lr = 0.01, sq_norm_avg_grad = 4.284287929534912, avg_sq_norm_grad = 120.17816162109375,                  max_norm_grad = 14.584299087524414, var_grad = 115.89387512207031

>>> Round:   30 / Acc: 72.738% / Loss: 0.9822 /Time: 4.31s
======================================================================================================

= Test = round: 30 / acc: 74.360% / loss: 0.9356 / Time: 0.84s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 4.159021377563477, avg_sq_norm_grad = 118.13739776611328,                  max_norm_grad = 14.546424865722656, var_grad = 113.97837829589844
round 31: local lr = 0.01, sq_norm_avg_grad = 4.222633361816406, avg_sq_norm_grad = 114.3407974243164,                  max_norm_grad = 14.353656768798828, var_grad = 110.1181640625
round 32: local lr = 0.01, sq_norm_avg_grad = 4.615445613861084, avg_sq_norm_grad = 113.71900177001953,                  max_norm_grad = 14.50391960144043, var_grad = 109.10355377197266
round 33: local lr = 0.01, sq_norm_avg_grad = 5.511415958404541, avg_sq_norm_grad = 113.630859375,                  max_norm_grad = 14.622567176818848, var_grad = 108.11944580078125
round 34: local lr = 0.01, sq_norm_avg_grad = 5.715987205505371, avg_sq_norm_grad = 111.80860900878906,                  max_norm_grad = 14.617228507995605, var_grad = 106.09262084960938

>>> Round:   35 / Acc: 74.456% / Loss: 0.8754 /Time: 4.95s
======================================================================================================

= Test = round: 35 / acc: 76.180% / loss: 0.8320 / Time: 0.94s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 6.224170207977295, avg_sq_norm_grad = 111.14022064208984,                  max_norm_grad = 14.702396392822266, var_grad = 104.91605377197266
round 36: local lr = 0.01, sq_norm_avg_grad = 5.6710076332092285, avg_sq_norm_grad = 107.00692749023438,                  max_norm_grad = 14.54776668548584, var_grad = 101.33592224121094
round 37: local lr = 0.01, sq_norm_avg_grad = 5.658280849456787, avg_sq_norm_grad = 104.97317504882812,                  max_norm_grad = 14.442387580871582, var_grad = 99.31489562988281
round 38: local lr = 0.01, sq_norm_avg_grad = 5.792587757110596, avg_sq_norm_grad = 104.39591217041016,                  max_norm_grad = 14.35250186920166, var_grad = 98.60332489013672
round 39: local lr = 0.01, sq_norm_avg_grad = 6.554356575012207, avg_sq_norm_grad = 103.24467468261719,                  max_norm_grad = 14.47485637664795, var_grad = 96.69031524658203

>>> Round:   40 / Acc: 76.118% / Loss: 0.7982 /Time: 4.34s
======================================================================================================

= Test = round: 40 / acc: 77.770% / loss: 0.7576 / Time: 0.80s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4736, Train_acc: 0.5338, Test_loss: 1.4612, Test_acc: 0.5353
Epoch: 006, Train_loss: 1.2329, Train_acc: 0.6275, Test_loss: 1.2202, Test_acc: 0.6306
Epoch: 011, Train_loss: 1.2149, Train_acc: 0.6347, Test_loss: 1.2031, Test_acc: 0.6379
Epoch: 016, Train_loss: 1.1956, Train_acc: 0.6430, Test_loss: 1.1831, Test_acc: 0.6464
Epoch: 021, Train_loss: 1.1962, Train_acc: 0.6440, Test_loss: 1.1837, Test_acc: 0.6467
Epoch: 026, Train_loss: 1.1832, Train_acc: 0.6500, Test_loss: 1.1719, Test_acc: 0.6523
Epoch: 031, Train_loss: 1.1863, Train_acc: 0.6451, Test_loss: 1.1797, Test_acc: 0.6488
Epoch: 036, Train_loss: 1.1828, Train_acc: 0.6488, Test_loss: 1.1707, Test_acc: 0.6474
Epoch: 041, Train_loss: 1.1895, Train_acc: 0.6456, Test_loss: 1.1814, Test_acc: 0.6463
Epoch: 046, Train_loss: 1.1758, Train_acc: 0.6526, Test_loss: 1.1665, Test_acc: 0.6566
Epoch: 051, Train_loss: 1.1758, Train_acc: 0.6491, Test_loss: 1.1663, Test_acc: 0.6523
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231003230015_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231003230015_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.1751183276358212, 0.6485652785546008, 1.167776492413859, 0.6508165759360071]
model_source_only: [2.40673105976658, 0.3585532448602566, 2.4190116528073573, 0.35629374513942896]

************************************************************************************************************************

repeat:2/5
using torch seed 11
uid: 20231003232805
FL pretrained model will be saved at ./models/lenet_mnist_20231003232805.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.186% / Loss: 2.3040 /Time: 4.60s
======================================================================================================

= Test = round: 0 / acc: 11.400% / loss: 2.3016 / Time: 0.85s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.055296026170253754, avg_sq_norm_grad = 1.7109163999557495,                  max_norm_grad = 1.4718400239944458, var_grad = 1.6556203365325928
round 2: local lr = 0.01, sq_norm_avg_grad = 0.0683789774775505, avg_sq_norm_grad = 2.341675281524658,                  max_norm_grad = 1.7762267589569092, var_grad = 2.273296356201172
round 3: local lr = 0.01, sq_norm_avg_grad = 0.09543091803789139, avg_sq_norm_grad = 3.6319172382354736,                  max_norm_grad = 2.229402542114258, var_grad = 3.5364863872528076
round 4: local lr = 0.01, sq_norm_avg_grad = 0.15747714042663574, avg_sq_norm_grad = 6.772560119628906,                  max_norm_grad = 3.0694568157196045, var_grad = 6.615082740783691

>>> Round:    5 / Acc: 19.790% / Loss: 2.2500 /Time: 4.20s
======================================================================================================

= Test = round: 5 / acc: 21.390% / loss: 2.2428 / Time: 0.86s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.31194016337394714, avg_sq_norm_grad = 16.381969451904297,                  max_norm_grad = 4.803291320800781, var_grad = 16.070030212402344
round 6: local lr = 0.01, sq_norm_avg_grad = 0.7617349624633789, avg_sq_norm_grad = 47.46021270751953,                  max_norm_grad = 8.1065034866333, var_grad = 46.69847869873047
round 7: local lr = 0.01, sq_norm_avg_grad = 1.3224000930786133, avg_sq_norm_grad = 107.38863372802734,                  max_norm_grad = 12.183252334594727, var_grad = 106.06623077392578
round 8: local lr = 0.01, sq_norm_avg_grad = 1.6024192571640015, avg_sq_norm_grad = 140.28404235839844,                  max_norm_grad = 14.035565376281738, var_grad = 138.68162536621094
round 9: local lr = 0.01, sq_norm_avg_grad = 1.8718373775482178, avg_sq_norm_grad = 145.7055206298828,                  max_norm_grad = 14.55219841003418, var_grad = 143.83367919921875

>>> Round:   10 / Acc: 47.413% / Loss: 2.0169 /Time: 4.15s
======================================================================================================

= Test = round: 10 / acc: 50.070% / loss: 2.0006 / Time: 0.79s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 2.6726129055023193, avg_sq_norm_grad = 146.93585205078125,                  max_norm_grad = 14.796257972717285, var_grad = 144.26324462890625
round 11: local lr = 0.01, sq_norm_avg_grad = 3.191793918609619, avg_sq_norm_grad = 149.6578826904297,                  max_norm_grad = 14.959137916564941, var_grad = 146.46609497070312
round 12: local lr = 0.01, sq_norm_avg_grad = 3.7462785243988037, avg_sq_norm_grad = 148.81930541992188,                  max_norm_grad = 15.080488204956055, var_grad = 145.07302856445312
round 13: local lr = 0.01, sq_norm_avg_grad = 4.183958053588867, avg_sq_norm_grad = 152.34579467773438,                  max_norm_grad = 15.313068389892578, var_grad = 148.16183471679688
round 14: local lr = 0.01, sq_norm_avg_grad = 4.565221786499023, avg_sq_norm_grad = 150.33106994628906,                  max_norm_grad = 15.237561225891113, var_grad = 145.76585388183594

>>> Round:   15 / Acc: 57.424% / Loss: 1.7120 /Time: 4.12s
======================================================================================================

= Test = round: 15 / acc: 60.070% / loss: 1.6826 / Time: 0.78s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 4.730137348175049, avg_sq_norm_grad = 151.62728881835938,                  max_norm_grad = 15.413156509399414, var_grad = 146.89715576171875
round 16: local lr = 0.01, sq_norm_avg_grad = 4.615585803985596, avg_sq_norm_grad = 152.24826049804688,                  max_norm_grad = 15.344640731811523, var_grad = 147.63267517089844
round 17: local lr = 0.01, sq_norm_avg_grad = 5.228964328765869, avg_sq_norm_grad = 153.9777374267578,                  max_norm_grad = 15.606127738952637, var_grad = 148.748779296875
round 18: local lr = 0.01, sq_norm_avg_grad = 5.408222198486328, avg_sq_norm_grad = 153.21737670898438,                  max_norm_grad = 15.689957618713379, var_grad = 147.8091583251953
round 19: local lr = 0.01, sq_norm_avg_grad = 5.222752094268799, avg_sq_norm_grad = 151.99473571777344,                  max_norm_grad = 15.675396919250488, var_grad = 146.77198791503906

>>> Round:   20 / Acc: 64.127% / Loss: 1.3916 /Time: 4.24s
======================================================================================================

= Test = round: 20 / acc: 66.240% / loss: 1.3509 / Time: 0.81s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 5.306067943572998, avg_sq_norm_grad = 148.46734619140625,                  max_norm_grad = 15.461551666259766, var_grad = 143.16128540039062
round 21: local lr = 0.01, sq_norm_avg_grad = 5.783448219299316, avg_sq_norm_grad = 152.68394470214844,                  max_norm_grad = 15.853691101074219, var_grad = 146.90049743652344
round 22: local lr = 0.01, sq_norm_avg_grad = 5.305751800537109, avg_sq_norm_grad = 148.9594268798828,                  max_norm_grad = 15.790417671203613, var_grad = 143.65367126464844
round 23: local lr = 0.01, sq_norm_avg_grad = 5.143757343292236, avg_sq_norm_grad = 145.38156127929688,                  max_norm_grad = 15.558194160461426, var_grad = 140.23780822753906
round 24: local lr = 0.01, sq_norm_avg_grad = 5.241697788238525, avg_sq_norm_grad = 140.77748107910156,                  max_norm_grad = 15.61812686920166, var_grad = 135.53578186035156

>>> Round:   25 / Acc: 68.768% / Loss: 1.1531 /Time: 4.31s
======================================================================================================

= Test = round: 25 / acc: 70.400% / loss: 1.1089 / Time: 0.80s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 5.36167049407959, avg_sq_norm_grad = 136.2584686279297,                  max_norm_grad = 15.547415733337402, var_grad = 130.8968048095703
round 26: local lr = 0.01, sq_norm_avg_grad = 4.8399553298950195, avg_sq_norm_grad = 132.6835174560547,                  max_norm_grad = 15.497971534729004, var_grad = 127.84355926513672
round 27: local lr = 0.01, sq_norm_avg_grad = 4.905880451202393, avg_sq_norm_grad = 127.92261505126953,                  max_norm_grad = 15.291253089904785, var_grad = 123.01673126220703
round 28: local lr = 0.01, sq_norm_avg_grad = 5.156802654266357, avg_sq_norm_grad = 123.13328552246094,                  max_norm_grad = 15.072330474853516, var_grad = 117.97648620605469
round 29: local lr = 0.01, sq_norm_avg_grad = 5.248673915863037, avg_sq_norm_grad = 123.98979949951172,                  max_norm_grad = 15.167327880859375, var_grad = 118.74112701416016

>>> Round:   30 / Acc: 71.744% / Loss: 0.9924 /Time: 4.30s
======================================================================================================

= Test = round: 30 / acc: 73.420% / loss: 0.9465 / Time: 0.89s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 6.043157577514648, avg_sq_norm_grad = 121.58128356933594,                  max_norm_grad = 15.112144470214844, var_grad = 115.53812408447266
round 31: local lr = 0.01, sq_norm_avg_grad = 5.444298267364502, avg_sq_norm_grad = 118.59188842773438,                  max_norm_grad = 14.801857948303223, var_grad = 113.14759063720703
round 32: local lr = 0.01, sq_norm_avg_grad = 5.772086143493652, avg_sq_norm_grad = 116.28970336914062,                  max_norm_grad = 14.813260078430176, var_grad = 110.51761627197266
round 33: local lr = 0.01, sq_norm_avg_grad = 5.613614559173584, avg_sq_norm_grad = 113.37950897216797,                  max_norm_grad = 14.729462623596191, var_grad = 107.7658920288086
round 34: local lr = 0.01, sq_norm_avg_grad = 5.564792156219482, avg_sq_norm_grad = 111.24232482910156,                  max_norm_grad = 14.573692321777344, var_grad = 105.67753601074219

>>> Round:   35 / Acc: 74.928% / Loss: 0.8691 /Time: 4.28s
======================================================================================================

= Test = round: 35 / acc: 76.530% / loss: 0.8262 / Time: 0.83s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 5.596340179443359, avg_sq_norm_grad = 110.57402038574219,                  max_norm_grad = 14.488058090209961, var_grad = 104.97767639160156
round 36: local lr = 0.01, sq_norm_avg_grad = 5.905182838439941, avg_sq_norm_grad = 109.39395904541016,                  max_norm_grad = 14.490961074829102, var_grad = 103.48877716064453
round 37: local lr = 0.01, sq_norm_avg_grad = 6.898122310638428, avg_sq_norm_grad = 107.80734252929688,                  max_norm_grad = 14.471638679504395, var_grad = 100.90921783447266
round 38: local lr = 0.01, sq_norm_avg_grad = 6.3624725341796875, avg_sq_norm_grad = 107.07759857177734,                  max_norm_grad = 14.36607837677002, var_grad = 100.71512603759766
round 39: local lr = 0.01, sq_norm_avg_grad = 6.512848854064941, avg_sq_norm_grad = 105.35104370117188,                  max_norm_grad = 14.384023666381836, var_grad = 98.83819580078125

>>> Round:   40 / Acc: 76.729% / Loss: 0.7879 /Time: 4.11s
======================================================================================================

= Test = round: 40 / acc: 78.250% / loss: 0.7466 / Time: 0.78s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4711, Train_acc: 0.5375, Test_loss: 1.4555, Test_acc: 0.5376
Epoch: 006, Train_loss: 1.2381, Train_acc: 0.6228, Test_loss: 1.2216, Test_acc: 0.6297
Epoch: 011, Train_loss: 1.2054, Train_acc: 0.6349, Test_loss: 1.1959, Test_acc: 0.6355
Epoch: 016, Train_loss: 1.1980, Train_acc: 0.6392, Test_loss: 1.1882, Test_acc: 0.6404
Epoch: 021, Train_loss: 1.1978, Train_acc: 0.6353, Test_loss: 1.1896, Test_acc: 0.6386
Epoch: 026, Train_loss: 1.1872, Train_acc: 0.6477, Test_loss: 1.1762, Test_acc: 0.6508
Epoch: 031, Train_loss: 1.1981, Train_acc: 0.6429, Test_loss: 1.1879, Test_acc: 0.6462
Epoch: 036, Train_loss: 1.1893, Train_acc: 0.6450, Test_loss: 1.1788, Test_acc: 0.6478
Epoch: 041, Train_loss: 1.1782, Train_acc: 0.6494, Test_loss: 1.1699, Test_acc: 0.6502
Epoch: 046, Train_loss: 1.1929, Train_acc: 0.6404, Test_loss: 1.1823, Test_acc: 0.6444
Epoch: 051, Train_loss: 1.1808, Train_acc: 0.6466, Test_loss: 1.1738, Test_acc: 0.6442
Epoch: 056, Train_loss: 1.1790, Train_acc: 0.6482, Test_loss: 1.1693, Test_acc: 0.6494
Epoch: 061, Train_loss: 1.1804, Train_acc: 0.6464, Test_loss: 1.1695, Test_acc: 0.6488
Epoch: 066, Train_loss: 1.1877, Train_acc: 0.6481, Test_loss: 1.1799, Test_acc: 0.6482
Epoch: 071, Train_loss: 1.1707, Train_acc: 0.6504, Test_loss: 1.1623, Test_acc: 0.6551
Epoch: 076, Train_loss: 1.1717, Train_acc: 0.6505, Test_loss: 1.1653, Test_acc: 0.6544
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231003232805_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231003232805_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.170675912183579, 0.6504466025999559, 1.1623089119151306, 0.655149427841351]
model_source_only: [2.4006578016660165, 0.35902781308791376, 2.4124538965482683, 0.3565159426730363]

************************************************************************************************************************

repeat:3/5
using torch seed 12
uid: 20231003235917
FL pretrained model will be saved at ./models/lenet_mnist_20231003235917.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.186% / Loss: 2.3040 /Time: 4.52s
======================================================================================================

= Test = round: 0 / acc: 11.400% / loss: 2.3016 / Time: 1.13s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.055371034890413284, avg_sq_norm_grad = 1.7127376794815063,                  max_norm_grad = 1.4722280502319336, var_grad = 1.6573666334152222
round 2: local lr = 0.01, sq_norm_avg_grad = 0.06865806877613068, avg_sq_norm_grad = 2.3516483306884766,                  max_norm_grad = 1.7793411016464233, var_grad = 2.2829902172088623
round 3: local lr = 0.01, sq_norm_avg_grad = 0.09640247374773026, avg_sq_norm_grad = 3.663557767868042,                  max_norm_grad = 2.2389376163482666, var_grad = 3.567155361175537
round 4: local lr = 0.01, sq_norm_avg_grad = 0.15945923328399658, avg_sq_norm_grad = 6.856619834899902,                  max_norm_grad = 3.0868852138519287, var_grad = 6.697160720825195

>>> Round:    5 / Acc: 19.913% / Loss: 2.2495 /Time: 4.28s
======================================================================================================

= Test = round: 5 / acc: 21.580% / loss: 2.2423 / Time: 0.82s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.31564584374427795, avg_sq_norm_grad = 16.695749282836914,                  max_norm_grad = 4.8478102684021, var_grad = 16.380104064941406
round 6: local lr = 0.01, sq_norm_avg_grad = 0.7678781747817993, avg_sq_norm_grad = 48.28618240356445,                  max_norm_grad = 8.166014671325684, var_grad = 47.51830291748047
round 7: local lr = 0.01, sq_norm_avg_grad = 1.2883830070495605, avg_sq_norm_grad = 108.09107971191406,                  max_norm_grad = 12.261008262634277, var_grad = 106.80269622802734
round 8: local lr = 0.01, sq_norm_avg_grad = 1.7691864967346191, avg_sq_norm_grad = 139.627685546875,                  max_norm_grad = 14.118816375732422, var_grad = 137.85850524902344
round 9: local lr = 0.01, sq_norm_avg_grad = 2.4353017807006836, avg_sq_norm_grad = 145.17530822753906,                  max_norm_grad = 14.684408187866211, var_grad = 142.74000549316406

>>> Round:   10 / Acc: 41.708% / Loss: 2.0209 /Time: 4.30s
======================================================================================================

= Test = round: 10 / acc: 43.810% / loss: 2.0054 / Time: 0.82s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 3.2033069133758545, avg_sq_norm_grad = 147.95272827148438,                  max_norm_grad = 14.959309577941895, var_grad = 144.74942016601562
round 11: local lr = 0.01, sq_norm_avg_grad = 4.296288013458252, avg_sq_norm_grad = 150.56948852539062,                  max_norm_grad = 15.243167877197266, var_grad = 146.273193359375
round 12: local lr = 0.01, sq_norm_avg_grad = 4.7638678550720215, avg_sq_norm_grad = 154.26182556152344,                  max_norm_grad = 15.494305610656738, var_grad = 149.49795532226562
round 13: local lr = 0.01, sq_norm_avg_grad = 4.6854071617126465, avg_sq_norm_grad = 150.19384765625,                  max_norm_grad = 15.26482105255127, var_grad = 145.50843811035156
round 14: local lr = 0.01, sq_norm_avg_grad = 4.620498180389404, avg_sq_norm_grad = 154.54898071289062,                  max_norm_grad = 15.380425453186035, var_grad = 149.92848205566406

>>> Round:   15 / Acc: 55.795% / Loss: 1.7152 /Time: 4.11s
======================================================================================================

= Test = round: 15 / acc: 58.030% / loss: 1.6862 / Time: 0.82s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 5.404386043548584, avg_sq_norm_grad = 155.83958435058594,                  max_norm_grad = 15.645055770874023, var_grad = 150.43519592285156
round 16: local lr = 0.01, sq_norm_avg_grad = 5.863550186157227, avg_sq_norm_grad = 154.9425811767578,                  max_norm_grad = 15.768464088439941, var_grad = 149.0790252685547
round 17: local lr = 0.01, sq_norm_avg_grad = 5.939601421356201, avg_sq_norm_grad = 156.27647399902344,                  max_norm_grad = 15.741052627563477, var_grad = 150.3368682861328
round 18: local lr = 0.01, sq_norm_avg_grad = 5.544641494750977, avg_sq_norm_grad = 152.79940795898438,                  max_norm_grad = 15.637816429138184, var_grad = 147.2547607421875
round 19: local lr = 0.01, sq_norm_avg_grad = 5.402726650238037, avg_sq_norm_grad = 151.26930236816406,                  max_norm_grad = 15.666348457336426, var_grad = 145.8665771484375

>>> Round:   20 / Acc: 63.734% / Loss: 1.3947 /Time: 4.28s
======================================================================================================

= Test = round: 20 / acc: 65.630% / loss: 1.3541 / Time: 0.85s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 5.843277931213379, avg_sq_norm_grad = 149.62501525878906,                  max_norm_grad = 15.686774253845215, var_grad = 143.78173828125
round 21: local lr = 0.01, sq_norm_avg_grad = 5.814452648162842, avg_sq_norm_grad = 148.8561553955078,                  max_norm_grad = 15.78184986114502, var_grad = 143.0417022705078
round 22: local lr = 0.01, sq_norm_avg_grad = 5.918103218078613, avg_sq_norm_grad = 147.1589813232422,                  max_norm_grad = 15.657631874084473, var_grad = 141.24087524414062
round 23: local lr = 0.01, sq_norm_avg_grad = 6.068267345428467, avg_sq_norm_grad = 145.40838623046875,                  max_norm_grad = 15.621828079223633, var_grad = 139.34011840820312
round 24: local lr = 0.01, sq_norm_avg_grad = 5.702533721923828, avg_sq_norm_grad = 144.11045837402344,                  max_norm_grad = 15.62993335723877, var_grad = 138.40792846679688

>>> Round:   25 / Acc: 68.443% / Loss: 1.1548 /Time: 4.09s
======================================================================================================

= Test = round: 25 / acc: 70.310% / loss: 1.1094 / Time: 0.79s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 5.813852310180664, avg_sq_norm_grad = 138.64366149902344,                  max_norm_grad = 15.68454647064209, var_grad = 132.82980346679688
round 26: local lr = 0.01, sq_norm_avg_grad = 5.478174209594727, avg_sq_norm_grad = 135.47509765625,                  max_norm_grad = 15.566156387329102, var_grad = 129.99691772460938
round 27: local lr = 0.01, sq_norm_avg_grad = 5.258048057556152, avg_sq_norm_grad = 130.69627380371094,                  max_norm_grad = 15.29119873046875, var_grad = 125.43822479248047
round 28: local lr = 0.01, sq_norm_avg_grad = 5.168914794921875, avg_sq_norm_grad = 125.66592407226562,                  max_norm_grad = 14.918027877807617, var_grad = 120.49700927734375
round 29: local lr = 0.01, sq_norm_avg_grad = 4.979564189910889, avg_sq_norm_grad = 123.14984893798828,                  max_norm_grad = 14.770658493041992, var_grad = 118.1702880859375

>>> Round:   30 / Acc: 71.982% / Loss: 0.9896 /Time: 4.21s
======================================================================================================

= Test = round: 30 / acc: 73.630% / loss: 0.9436 / Time: 0.78s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 5.558499336242676, avg_sq_norm_grad = 120.49436950683594,                  max_norm_grad = 14.744261741638184, var_grad = 114.93586730957031
round 31: local lr = 0.01, sq_norm_avg_grad = 4.90648889541626, avg_sq_norm_grad = 118.07049560546875,                  max_norm_grad = 14.49331283569336, var_grad = 113.16400909423828
round 32: local lr = 0.01, sq_norm_avg_grad = 5.521522521972656, avg_sq_norm_grad = 113.7572250366211,                  max_norm_grad = 14.419157028198242, var_grad = 108.23570251464844
round 33: local lr = 0.01, sq_norm_avg_grad = 5.24233341217041, avg_sq_norm_grad = 112.47665405273438,                  max_norm_grad = 14.317021369934082, var_grad = 107.23432159423828
round 34: local lr = 0.01, sq_norm_avg_grad = 5.077446460723877, avg_sq_norm_grad = 112.54855346679688,                  max_norm_grad = 14.50451374053955, var_grad = 107.47110748291016

>>> Round:   35 / Acc: 74.810% / Loss: 0.8716 /Time: 4.34s
======================================================================================================

= Test = round: 35 / acc: 76.370% / loss: 0.8283 / Time: 0.83s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 5.636268138885498, avg_sq_norm_grad = 109.3515625,                  max_norm_grad = 14.388522148132324, var_grad = 103.71529388427734
round 36: local lr = 0.01, sq_norm_avg_grad = 5.985469818115234, avg_sq_norm_grad = 108.34296417236328,                  max_norm_grad = 14.473883628845215, var_grad = 102.35749816894531
round 37: local lr = 0.01, sq_norm_avg_grad = 5.665565013885498, avg_sq_norm_grad = 106.28247833251953,                  max_norm_grad = 14.390156745910645, var_grad = 100.61691284179688
round 38: local lr = 0.01, sq_norm_avg_grad = 5.388232231140137, avg_sq_norm_grad = 104.21896362304688,                  max_norm_grad = 14.294352531433105, var_grad = 98.83073425292969
round 39: local lr = 0.01, sq_norm_avg_grad = 5.630648612976074, avg_sq_norm_grad = 102.50945281982422,                  max_norm_grad = 14.1393461227417, var_grad = 96.8788070678711

>>> Round:   40 / Acc: 77.039% / Loss: 0.7818 /Time: 4.25s
======================================================================================================

= Test = round: 40 / acc: 78.550% / loss: 0.7409 / Time: 0.86s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4841, Train_acc: 0.5277, Test_loss: 1.4703, Test_acc: 0.5277
Epoch: 006, Train_loss: 1.2337, Train_acc: 0.6246, Test_loss: 1.2222, Test_acc: 0.6264
Epoch: 011, Train_loss: 1.2237, Train_acc: 0.6310, Test_loss: 1.2132, Test_acc: 0.6319
Epoch: 016, Train_loss: 1.1977, Train_acc: 0.6358, Test_loss: 1.1887, Test_acc: 0.6384
Epoch: 021, Train_loss: 1.1901, Train_acc: 0.6421, Test_loss: 1.1811, Test_acc: 0.6446
Epoch: 026, Train_loss: 1.1862, Train_acc: 0.6455, Test_loss: 1.1765, Test_acc: 0.6504
Epoch: 031, Train_loss: 1.1884, Train_acc: 0.6397, Test_loss: 1.1799, Test_acc: 0.6437
Epoch: 036, Train_loss: 1.1836, Train_acc: 0.6472, Test_loss: 1.1749, Test_acc: 0.6527
Epoch: 041, Train_loss: 1.1850, Train_acc: 0.6439, Test_loss: 1.1774, Test_acc: 0.6503
Epoch: 046, Train_loss: 1.1807, Train_acc: 0.6487, Test_loss: 1.1726, Test_acc: 0.6508
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231003235917_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231003235917_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.1764274562965584, 0.6500228809681192, 1.1684359937505635, 0.6527052549716699]
model_source_only: [2.4044624273236455, 0.3641294215352282, 2.416387263728094, 0.3631818686812576]

************************************************************************************************************************

repeat:4/5
using torch seed 13
uid: 20231004002547
FL pretrained model will be saved at ./models/lenet_mnist_20231004002547.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.186% / Loss: 2.3040 /Time: 4.31s
======================================================================================================

= Test = round: 0 / acc: 11.400% / loss: 2.3016 / Time: 0.83s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.05538386479020119, avg_sq_norm_grad = 1.713126301765442,                  max_norm_grad = 1.4731525182724, var_grad = 1.6577423810958862
round 2: local lr = 0.01, sq_norm_avg_grad = 0.06870441138744354, avg_sq_norm_grad = 2.3520102500915527,                  max_norm_grad = 1.779395341873169, var_grad = 2.2833058834075928
round 3: local lr = 0.01, sq_norm_avg_grad = 0.09594538807868958, avg_sq_norm_grad = 3.6514647006988525,                  max_norm_grad = 2.2351014614105225, var_grad = 3.5555193424224854
round 4: local lr = 0.01, sq_norm_avg_grad = 0.15766406059265137, avg_sq_norm_grad = 6.812724590301514,                  max_norm_grad = 3.0767364501953125, var_grad = 6.655060768127441

>>> Round:    5 / Acc: 19.749% / Loss: 2.2495 /Time: 4.19s
======================================================================================================

= Test = round: 5 / acc: 21.340% / loss: 2.2423 / Time: 0.79s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.31373006105422974, avg_sq_norm_grad = 16.618690490722656,                  max_norm_grad = 4.835388660430908, var_grad = 16.304960250854492
round 6: local lr = 0.01, sq_norm_avg_grad = 0.7558645009994507, avg_sq_norm_grad = 48.13163375854492,                  max_norm_grad = 8.162771224975586, var_grad = 47.375770568847656
round 7: local lr = 0.01, sq_norm_avg_grad = 1.397757649421692, avg_sq_norm_grad = 109.4369888305664,                  max_norm_grad = 12.304220199584961, var_grad = 108.03923034667969
round 8: local lr = 0.01, sq_norm_avg_grad = 1.8276493549346924, avg_sq_norm_grad = 141.2312469482422,                  max_norm_grad = 14.13976001739502, var_grad = 139.40359497070312
round 9: local lr = 0.01, sq_norm_avg_grad = 2.5070385932922363, avg_sq_norm_grad = 145.62725830078125,                  max_norm_grad = 14.745985984802246, var_grad = 143.12022399902344

>>> Round:   10 / Acc: 42.546% / Loss: 2.0189 /Time: 4.27s
======================================================================================================

= Test = round: 10 / acc: 44.860% / loss: 2.0028 / Time: 0.82s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 3.135133981704712, avg_sq_norm_grad = 150.5980987548828,                  max_norm_grad = 15.025391578674316, var_grad = 147.4629669189453
round 11: local lr = 0.01, sq_norm_avg_grad = 4.192084789276123, avg_sq_norm_grad = 150.71775817871094,                  max_norm_grad = 15.203810691833496, var_grad = 146.5256805419922
round 12: local lr = 0.01, sq_norm_avg_grad = 4.303129196166992, avg_sq_norm_grad = 149.3698272705078,                  max_norm_grad = 15.115791320800781, var_grad = 145.0666961669922
round 13: local lr = 0.01, sq_norm_avg_grad = 4.692060470581055, avg_sq_norm_grad = 150.1919403076172,                  max_norm_grad = 15.052050590515137, var_grad = 145.4998779296875
round 14: local lr = 0.01, sq_norm_avg_grad = 4.541053295135498, avg_sq_norm_grad = 153.3683319091797,                  max_norm_grad = 15.177912712097168, var_grad = 148.82728576660156

>>> Round:   15 / Acc: 55.216% / Loss: 1.7129 /Time: 4.18s
======================================================================================================

= Test = round: 15 / acc: 57.900% / loss: 1.6850 / Time: 0.83s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 5.005391597747803, avg_sq_norm_grad = 155.43484497070312,                  max_norm_grad = 15.379867553710938, var_grad = 150.42945861816406
round 16: local lr = 0.01, sq_norm_avg_grad = 4.992184638977051, avg_sq_norm_grad = 154.1532440185547,                  max_norm_grad = 15.362842559814453, var_grad = 149.1610565185547
round 17: local lr = 0.01, sq_norm_avg_grad = 4.976120471954346, avg_sq_norm_grad = 156.96697998046875,                  max_norm_grad = 15.475123405456543, var_grad = 151.99085998535156
round 18: local lr = 0.01, sq_norm_avg_grad = 5.287948131561279, avg_sq_norm_grad = 151.8004608154297,                  max_norm_grad = 15.435690879821777, var_grad = 146.51251220703125
round 19: local lr = 0.01, sq_norm_avg_grad = 5.0664191246032715, avg_sq_norm_grad = 149.82936096191406,                  max_norm_grad = 15.331915855407715, var_grad = 144.762939453125

>>> Round:   20 / Acc: 64.194% / Loss: 1.3922 /Time: 4.09s
======================================================================================================

= Test = round: 20 / acc: 66.040% / loss: 1.3531 / Time: 0.79s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 5.377274513244629, avg_sq_norm_grad = 150.9129180908203,                  max_norm_grad = 15.484232902526855, var_grad = 145.53564453125
round 21: local lr = 0.01, sq_norm_avg_grad = 5.304140567779541, avg_sq_norm_grad = 149.254638671875,                  max_norm_grad = 15.4603910446167, var_grad = 143.95050048828125
round 22: local lr = 0.01, sq_norm_avg_grad = 5.383631229400635, avg_sq_norm_grad = 143.2833709716797,                  max_norm_grad = 15.402717590332031, var_grad = 137.8997344970703
round 23: local lr = 0.01, sq_norm_avg_grad = 4.809129238128662, avg_sq_norm_grad = 139.78326416015625,                  max_norm_grad = 15.154013633728027, var_grad = 134.97413635253906
round 24: local lr = 0.01, sq_norm_avg_grad = 4.850980281829834, avg_sq_norm_grad = 139.00767517089844,                  max_norm_grad = 15.387266159057617, var_grad = 134.1566925048828

>>> Round:   25 / Acc: 68.546% / Loss: 1.1564 /Time: 4.69s
======================================================================================================

= Test = round: 25 / acc: 70.350% / loss: 1.1104 / Time: 1.15s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 5.582238674163818, avg_sq_norm_grad = 135.7864227294922,                  max_norm_grad = 15.62005615234375, var_grad = 130.2041778564453
round 26: local lr = 0.01, sq_norm_avg_grad = 5.112666130065918, avg_sq_norm_grad = 130.8991241455078,                  max_norm_grad = 15.20031452178955, var_grad = 125.78646087646484
round 27: local lr = 0.01, sq_norm_avg_grad = 4.958549976348877, avg_sq_norm_grad = 128.5887451171875,                  max_norm_grad = 15.09516429901123, var_grad = 123.63019561767578
round 28: local lr = 0.01, sq_norm_avg_grad = 4.569911479949951, avg_sq_norm_grad = 123.63893127441406,                  max_norm_grad = 14.687408447265625, var_grad = 119.06902313232422
round 29: local lr = 0.01, sq_norm_avg_grad = 4.27087926864624, avg_sq_norm_grad = 122.77173614501953,                  max_norm_grad = 14.49168586730957, var_grad = 118.5008544921875

>>> Round:   30 / Acc: 72.587% / Loss: 0.9857 /Time: 4.60s
======================================================================================================

= Test = round: 30 / acc: 74.040% / loss: 0.9400 / Time: 0.87s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 4.6591691970825195, avg_sq_norm_grad = 119.19120788574219,                  max_norm_grad = 14.49797248840332, var_grad = 114.53203582763672
round 31: local lr = 0.01, sq_norm_avg_grad = 4.545577526092529, avg_sq_norm_grad = 114.3998794555664,                  max_norm_grad = 14.12735652923584, var_grad = 109.85430145263672
round 32: local lr = 0.01, sq_norm_avg_grad = 5.1245622634887695, avg_sq_norm_grad = 114.21554565429688,                  max_norm_grad = 14.386792182922363, var_grad = 109.09098052978516
round 33: local lr = 0.01, sq_norm_avg_grad = 5.186307430267334, avg_sq_norm_grad = 111.17223358154297,                  max_norm_grad = 14.2470064163208, var_grad = 105.98592376708984
round 34: local lr = 0.01, sq_norm_avg_grad = 4.732500076293945, avg_sq_norm_grad = 110.05966186523438,                  max_norm_grad = 14.117910385131836, var_grad = 105.32716369628906

>>> Round:   35 / Acc: 74.589% / Loss: 0.8733 /Time: 4.45s
======================================================================================================

= Test = round: 35 / acc: 76.250% / loss: 0.8303 / Time: 0.84s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 5.875097751617432, avg_sq_norm_grad = 111.06405639648438,                  max_norm_grad = 14.491235733032227, var_grad = 105.18895721435547
round 36: local lr = 0.01, sq_norm_avg_grad = 5.831803321838379, avg_sq_norm_grad = 109.212158203125,                  max_norm_grad = 14.372564315795898, var_grad = 103.38035583496094
round 37: local lr = 0.01, sq_norm_avg_grad = 6.214617729187012, avg_sq_norm_grad = 107.82817077636719,                  max_norm_grad = 14.304947853088379, var_grad = 101.61355590820312
round 38: local lr = 0.01, sq_norm_avg_grad = 6.229183197021484, avg_sq_norm_grad = 104.16521453857422,                  max_norm_grad = 14.325096130371094, var_grad = 97.93603515625
round 39: local lr = 0.01, sq_norm_avg_grad = 6.185053825378418, avg_sq_norm_grad = 103.5842056274414,                  max_norm_grad = 14.309386253356934, var_grad = 97.39915466308594

>>> Round:   40 / Acc: 76.690% / Loss: 0.7896 /Time: 4.62s
======================================================================================================

= Test = round: 40 / acc: 78.170% / loss: 0.7488 / Time: 1.11s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4715, Train_acc: 0.5404, Test_loss: 1.4570, Test_acc: 0.5406
Epoch: 006, Train_loss: 1.2282, Train_acc: 0.6297, Test_loss: 1.2174, Test_acc: 0.6328
Epoch: 011, Train_loss: 1.2096, Train_acc: 0.6354, Test_loss: 1.1990, Test_acc: 0.6392
Epoch: 016, Train_loss: 1.1996, Train_acc: 0.6336, Test_loss: 1.1899, Test_acc: 0.6377
Epoch: 021, Train_loss: 1.1998, Train_acc: 0.6370, Test_loss: 1.1911, Test_acc: 0.6393
Epoch: 026, Train_loss: 1.1995, Train_acc: 0.6339, Test_loss: 1.1882, Test_acc: 0.6399
Epoch: 031, Train_loss: 1.1891, Train_acc: 0.6430, Test_loss: 1.1783, Test_acc: 0.6434
Epoch: 036, Train_loss: 1.1855, Train_acc: 0.6429, Test_loss: 1.1754, Test_acc: 0.6467
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231004002547_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231004002547_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.183418140060059, 0.6422772495381435, 1.1732000339734476, 0.6444839462281968]
model_source_only: [2.4041037496228403, 0.36026508025287707, 2.415973199339605, 0.3578491278746806]

************************************************************************************************************************

repeat:5/5
using torch seed 14
uid: 20231004004907
FL pretrained model will be saved at ./models/lenet_mnist_20231004004907.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.186% / Loss: 2.3040 /Time: 4.14s
======================================================================================================

= Test = round: 0 / acc: 11.400% / loss: 2.3016 / Time: 0.80s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.05528821051120758, avg_sq_norm_grad = 1.711428165435791,                  max_norm_grad = 1.469882607460022, var_grad = 1.6561399698257446
round 2: local lr = 0.01, sq_norm_avg_grad = 0.0686681866645813, avg_sq_norm_grad = 2.3468616008758545,                  max_norm_grad = 1.7781987190246582, var_grad = 2.278193473815918
round 3: local lr = 0.01, sq_norm_avg_grad = 0.09572894871234894, avg_sq_norm_grad = 3.6379764080047607,                  max_norm_grad = 2.228771924972534, var_grad = 3.5422475337982178
round 4: local lr = 0.01, sq_norm_avg_grad = 0.15664882957935333, avg_sq_norm_grad = 6.765092849731445,                  max_norm_grad = 3.0662803649902344, var_grad = 6.6084442138671875

>>> Round:    5 / Acc: 19.845% / Loss: 2.2500 /Time: 4.13s
======================================================================================================

= Test = round: 5 / acc: 21.560% / loss: 2.2429 / Time: 0.78s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.30988946557044983, avg_sq_norm_grad = 16.423795700073242,                  max_norm_grad = 4.807844161987305, var_grad = 16.113906860351562
round 6: local lr = 0.01, sq_norm_avg_grad = 0.7357711791992188, avg_sq_norm_grad = 47.52774429321289,                  max_norm_grad = 8.11697006225586, var_grad = 46.79197311401367
round 7: local lr = 0.01, sq_norm_avg_grad = 1.359561562538147, avg_sq_norm_grad = 108.33318328857422,                  max_norm_grad = 12.278900146484375, var_grad = 106.97362518310547
round 8: local lr = 0.01, sq_norm_avg_grad = 1.7295202016830444, avg_sq_norm_grad = 140.1369171142578,                  max_norm_grad = 14.120929718017578, var_grad = 138.4073944091797
round 9: local lr = 0.01, sq_norm_avg_grad = 2.4507431983947754, avg_sq_norm_grad = 145.38719177246094,                  max_norm_grad = 14.751547813415527, var_grad = 142.9364471435547

>>> Round:   10 / Acc: 45.736% / Loss: 2.0193 /Time: 4.05s
======================================================================================================

= Test = round: 10 / acc: 47.660% / loss: 2.0037 / Time: 0.77s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 2.8762168884277344, avg_sq_norm_grad = 146.885986328125,                  max_norm_grad = 14.907471656799316, var_grad = 144.009765625
round 11: local lr = 0.01, sq_norm_avg_grad = 4.296584606170654, avg_sq_norm_grad = 150.80319213867188,                  max_norm_grad = 15.462234497070312, var_grad = 146.50660705566406
round 12: local lr = 0.01, sq_norm_avg_grad = 4.257421970367432, avg_sq_norm_grad = 151.14292907714844,                  max_norm_grad = 15.385978698730469, var_grad = 146.88551330566406
round 13: local lr = 0.01, sq_norm_avg_grad = 4.2345428466796875, avg_sq_norm_grad = 148.07684326171875,                  max_norm_grad = 15.173896789550781, var_grad = 143.84230041503906
round 14: local lr = 0.01, sq_norm_avg_grad = 4.221577167510986, avg_sq_norm_grad = 149.7409210205078,                  max_norm_grad = 15.14248275756836, var_grad = 145.51934814453125

>>> Round:   15 / Acc: 57.109% / Loss: 1.7132 /Time: 4.03s
======================================================================================================

= Test = round: 15 / acc: 59.520% / loss: 1.6844 / Time: 0.76s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 4.97314453125, avg_sq_norm_grad = 152.59837341308594,                  max_norm_grad = 15.38661003112793, var_grad = 147.62522888183594
round 16: local lr = 0.01, sq_norm_avg_grad = 5.364898681640625, avg_sq_norm_grad = 152.7961883544922,                  max_norm_grad = 15.337416648864746, var_grad = 147.43128967285156
round 17: local lr = 0.01, sq_norm_avg_grad = 5.46858024597168, avg_sq_norm_grad = 156.31878662109375,                  max_norm_grad = 15.598677635192871, var_grad = 150.85020446777344
round 18: local lr = 0.01, sq_norm_avg_grad = 5.564753532409668, avg_sq_norm_grad = 155.82284545898438,                  max_norm_grad = 15.621826171875, var_grad = 150.25808715820312
round 19: local lr = 0.01, sq_norm_avg_grad = 5.590505123138428, avg_sq_norm_grad = 154.23806762695312,                  max_norm_grad = 15.717524528503418, var_grad = 148.64756774902344

>>> Round:   20 / Acc: 63.452% / Loss: 1.3977 /Time: 4.06s
======================================================================================================

= Test = round: 20 / acc: 65.360% / loss: 1.3574 / Time: 0.77s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 5.700020790100098, avg_sq_norm_grad = 147.73716735839844,                  max_norm_grad = 15.631697654724121, var_grad = 142.03713989257812
round 21: local lr = 0.01, sq_norm_avg_grad = 5.935727596282959, avg_sq_norm_grad = 147.28147888183594,                  max_norm_grad = 15.67840576171875, var_grad = 141.3457489013672
round 22: local lr = 0.01, sq_norm_avg_grad = 5.789395332336426, avg_sq_norm_grad = 147.71751403808594,                  max_norm_grad = 15.81067180633545, var_grad = 141.92811584472656
round 23: local lr = 0.01, sq_norm_avg_grad = 5.353594779968262, avg_sq_norm_grad = 145.39990234375,                  max_norm_grad = 15.463871955871582, var_grad = 140.0463104248047
round 24: local lr = 0.01, sq_norm_avg_grad = 5.29423713684082, avg_sq_norm_grad = 138.462646484375,                  max_norm_grad = 15.339221954345703, var_grad = 133.1684112548828

>>> Round:   25 / Acc: 68.736% / Loss: 1.1545 /Time: 4.00s
======================================================================================================

= Test = round: 25 / acc: 70.690% / loss: 1.1095 / Time: 0.77s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 5.129798889160156, avg_sq_norm_grad = 135.7449493408203,                  max_norm_grad = 15.250099182128906, var_grad = 130.61514282226562
round 26: local lr = 0.01, sq_norm_avg_grad = 4.596649169921875, avg_sq_norm_grad = 135.81683349609375,                  max_norm_grad = 15.076567649841309, var_grad = 131.22018432617188
round 27: local lr = 0.01, sq_norm_avg_grad = 4.921041488647461, avg_sq_norm_grad = 131.77618408203125,                  max_norm_grad = 15.049542427062988, var_grad = 126.85514068603516
round 28: local lr = 0.01, sq_norm_avg_grad = 4.686633110046387, avg_sq_norm_grad = 125.16207885742188,                  max_norm_grad = 14.784284591674805, var_grad = 120.47544860839844
round 29: local lr = 0.01, sq_norm_avg_grad = 4.8310747146606445, avg_sq_norm_grad = 122.76376342773438,                  max_norm_grad = 14.749258995056152, var_grad = 117.93268585205078

>>> Round:   30 / Acc: 72.135% / Loss: 0.9923 /Time: 4.04s
======================================================================================================

= Test = round: 30 / acc: 73.630% / loss: 0.9467 / Time: 0.76s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 5.271385192871094, avg_sq_norm_grad = 119.62842559814453,                  max_norm_grad = 14.917133331298828, var_grad = 114.35704040527344
round 31: local lr = 0.01, sq_norm_avg_grad = 5.758841037750244, avg_sq_norm_grad = 119.29940795898438,                  max_norm_grad = 14.943482398986816, var_grad = 113.54056549072266
round 32: local lr = 0.01, sq_norm_avg_grad = 5.132915019989014, avg_sq_norm_grad = 114.08235931396484,                  max_norm_grad = 14.76685619354248, var_grad = 108.94944763183594
round 33: local lr = 0.01, sq_norm_avg_grad = 5.201907634735107, avg_sq_norm_grad = 111.1072998046875,                  max_norm_grad = 14.57465934753418, var_grad = 105.9053955078125
round 34: local lr = 0.01, sq_norm_avg_grad = 5.696666717529297, avg_sq_norm_grad = 111.72769165039062,                  max_norm_grad = 14.545510292053223, var_grad = 106.03102111816406

>>> Round:   35 / Acc: 74.262% / Loss: 0.8763 /Time: 4.36s
======================================================================================================

= Test = round: 35 / acc: 76.030% / loss: 0.8334 / Time: 0.83s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 6.417238712310791, avg_sq_norm_grad = 112.63162231445312,                  max_norm_grad = 14.747777938842773, var_grad = 106.21438598632812
round 36: local lr = 0.01, sq_norm_avg_grad = 5.776121616363525, avg_sq_norm_grad = 107.5760498046875,                  max_norm_grad = 14.502636909484863, var_grad = 101.7999267578125
round 37: local lr = 0.01, sq_norm_avg_grad = 5.132580280303955, avg_sq_norm_grad = 104.25165557861328,                  max_norm_grad = 14.286288261413574, var_grad = 99.11907196044922
round 38: local lr = 0.01, sq_norm_avg_grad = 5.712639331817627, avg_sq_norm_grad = 104.69728088378906,                  max_norm_grad = 14.490065574645996, var_grad = 98.9846420288086
round 39: local lr = 0.01, sq_norm_avg_grad = 5.134434223175049, avg_sq_norm_grad = 100.09095764160156,                  max_norm_grad = 14.071883201599121, var_grad = 94.9565200805664

>>> Round:   40 / Acc: 77.044% / Loss: 0.7835 /Time: 4.28s
======================================================================================================

= Test = round: 40 / acc: 78.600% / loss: 0.7424 / Time: 0.82s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.4782, Train_acc: 0.5379, Test_loss: 1.4631, Test_acc: 0.5404
Epoch: 006, Train_loss: 1.2497, Train_acc: 0.6180, Test_loss: 1.2369, Test_acc: 0.6229
Epoch: 011, Train_loss: 1.2082, Train_acc: 0.6378, Test_loss: 1.1965, Test_acc: 0.6367
Epoch: 016, Train_loss: 1.2024, Train_acc: 0.6331, Test_loss: 1.1918, Test_acc: 0.6358
Epoch: 021, Train_loss: 1.1906, Train_acc: 0.6452, Test_loss: 1.1805, Test_acc: 0.6504
Epoch: 026, Train_loss: 1.1948, Train_acc: 0.6347, Test_loss: 1.1881, Test_acc: 0.6357
Epoch: 031, Train_loss: 1.1934, Train_acc: 0.6450, Test_loss: 1.1825, Test_acc: 0.6467
Epoch: 036, Train_loss: 1.1850, Train_acc: 0.6441, Test_loss: 1.1798, Test_acc: 0.6488
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231004004907_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231004004907_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.181618606047526, 0.6469720852188946, 1.1719975277140808, 0.6530385512720809]
model_source_only: [2.404257243032086, 0.3644683988406976, 2.416190484698223, 0.361626485946006]
fl_test_acc_mean 0.7787200000000001
model_source_only_test_acc_mean 0.35909343406288186
model_ft_test_acc_mean 0.6512387512498611
