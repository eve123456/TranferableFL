nohup: ignoring input
working!
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.01
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 100
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.001
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20231001182456
FL pretrained model will be saved at ./models/lenet_mnist_20231001182456.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 6.135% / Loss: 2.3068 /Time: 4.11s
======================================================================================================

= Test = round: 0 / acc: 6.050% / loss: 2.3088 / Time: 0.79s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.006512817461043596, avg_sq_norm_grad = 0.726550281047821,                  max_norm_grad = 0.9180260896682739, var_grad = 0.7200374603271484
round 2: local lr = 0.01, sq_norm_avg_grad = 0.00670666666701436, avg_sq_norm_grad = 0.7414606809616089,                  max_norm_grad = 0.9291936755180359, var_grad = 0.7347540259361267
nohup: ignoring input
working!
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.01
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 100
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
round 3: local lr = 0.01, sq_norm_avg_grad = 0.006951420567929745, avg_sq_norm_grad = 0.7592112421989441,                  max_norm_grad = 0.9423753619194031, var_grad = 0.7522598505020142
round 4: local lr = 0.01, sq_norm_avg_grad = 0.007220647297799587, avg_sq_norm_grad = 0.7803225517272949,                  max_norm_grad = 0.9575396180152893, var_grad = 0.7731019258499146

>>> Round:    5 / Acc: 8.618% / Loss: 2.3036 /Time: 4.12s
======================================================================================================

= Test = round: 5 / acc: 8.590% / loss: 2.3054 / Time: 0.78s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.007495228201150894, avg_sq_norm_grad = 0.8061327934265137,                  max_norm_grad = 0.9749578833580017, var_grad = 0.7986375689506531
round 6: local lr = 0.01, sq_norm_avg_grad = 0.007787955459207296, avg_sq_norm_grad = 0.8368145823478699,                  max_norm_grad = 0.9961475729942322, var_grad = 0.8290266394615173
round 7: local lr = 0.01, sq_norm_avg_grad = 0.008108562789857388, avg_sq_norm_grad = 0.8746745586395264,                  max_norm_grad = 1.0204969644546509, var_grad = 0.866566002368927
round 8: local lr = 0.01, sq_norm_avg_grad = 0.008624023757874966, avg_sq_norm_grad = 0.9210159182548523,                  max_norm_grad = 1.0488343238830566, var_grad = 0.9123919010162354
round 9: local lr = 0.01, sq_norm_avg_grad = 0.009371618740260601, avg_sq_norm_grad = 0.9781795144081116,                  max_norm_grad = 1.0823755264282227, var_grad = 0.9688078761100769

>>> Round:   10 / Acc: 10.087% / Loss: 2.2989 /Time: 4.92s
======================================================================================================

= Test = round: 10 / acc: 9.890% / loss: 2.3007 / Time: 1.01s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.010495420545339584, avg_sq_norm_grad = 1.0509542226791382,                  max_norm_grad = 1.1256753206253052, var_grad = 1.0404587984085083
round 11: local lr = 0.01, sq_norm_avg_grad = 0.012003270909190178, avg_sq_norm_grad = 1.1473186016082764,                  max_norm_grad = 1.1785964965820312, var_grad = 1.1353152990341187
round 12: local lr = 0.01, sq_norm_avg_grad = 0.013405866920948029, avg_sq_norm_grad = 1.2774407863616943,                  max_norm_grad = 1.2495317459106445, var_grad = 1.2640348672866821
round 13: local lr = 0.01, sq_norm_avg_grad = 0.015161619521677494, avg_sq_norm_grad = 1.4564484357833862,                  max_norm_grad = 1.3407886028289795, var_grad = 1.4412868022918701
round 14: local lr = 0.01, sq_norm_avg_grad = 0.017960261553525925, avg_sq_norm_grad = 1.7152690887451172,                  max_norm_grad = 1.4751863479614258, var_grad = 1.6973087787628174

>>> Round:   15 / Acc: 14.808% / Loss: 2.2891 /Time: 4.17s
======================================================================================================

= Test = round: 15 / acc: 14.520% / loss: 2.2911 / Time: 0.82s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 0.022303838282823563, avg_sq_norm_grad = 2.107931137084961,                  max_norm_grad = 1.6579171419143677, var_grad = 2.085627317428589
round 16: local lr = 0.01, sq_norm_avg_grad = 0.02854534238576889, avg_sq_norm_grad = 2.748436212539673,                  max_norm_grad = 1.907759189605713, var_grad = 2.719890832901001
round 17: local lr = 0.01, sq_norm_avg_grad = 0.03657228499650955, avg_sq_norm_grad = 3.914714813232422,                  max_norm_grad = 2.2891623973846436, var_grad = 3.8781425952911377
round 18: local lr = 0.01, sq_norm_avg_grad = 0.0519041083753109, avg_sq_norm_grad = 6.443682670593262,                  max_norm_grad = 2.975074291229248, var_grad = 6.391778469085693
round 19: local lr = 0.01, sq_norm_avg_grad = 0.09948161989450455, avg_sq_norm_grad = 12.973593711853027,                  max_norm_grad = 4.231192588806152, var_grad = 12.874112129211426

>>> Round:   20 / Acc: 17.518% / Loss: 2.2568 /Time: 4.09s
======================================================================================================

= Test = round: 20 / acc: 17.300% / loss: 2.2640 / Time: 0.79s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 0.2758477032184601, avg_sq_norm_grad = 32.50484848022461,                  max_norm_grad = 6.620465278625488, var_grad = 32.229000091552734
round 21: local lr = 0.01, sq_norm_avg_grad = 0.691927433013916, avg_sq_norm_grad = 75.74906921386719,                  max_norm_grad = 10.33537769317627, var_grad = 75.05714416503906
round 22: local lr = 0.01, sq_norm_avg_grad = 1.1048802137374878, avg_sq_norm_grad = 128.29225158691406,                  max_norm_grad = 13.77682876586914, var_grad = 127.18737030029297
round 23: local lr = 0.01, sq_norm_avg_grad = 1.2053654193878174, avg_sq_norm_grad = 140.97561645507812,                  max_norm_grad = 14.622142791748047, var_grad = 139.77024841308594
round 24: local lr = 0.01, sq_norm_avg_grad = 1.8553410768508911, avg_sq_norm_grad = 142.3209991455078,                  max_norm_grad = 14.9356107711792, var_grad = 140.4656524658203

>>> Round:   25 / Acc: 37.266% / Loss: 2.1346 /Time: 4.17s
======================================================================================================

= Test = round: 25 / acc: 38.380% / loss: 2.1282 / Time: 0.83s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 2.3900954723358154, avg_sq_norm_grad = 145.6995849609375,                  max_norm_grad = 15.161351203918457, var_grad = 143.3094940185547
round 26: local lr = 0.01, sq_norm_avg_grad = 3.408719062805176, avg_sq_norm_grad = 147.13555908203125,                  max_norm_grad = 15.369108200073242, var_grad = 143.72683715820312
round 27: local lr = 0.01, sq_norm_avg_grad = 3.377016544342041, avg_sq_norm_grad = 148.37806701660156,                  max_norm_grad = 15.44736099243164, var_grad = 145.0010528564453
round 28: local lr = 0.01, sq_norm_avg_grad = 3.5007834434509277, avg_sq_norm_grad = 147.5970916748047,                  max_norm_grad = 15.34432315826416, var_grad = 144.0963134765625
round 29: local lr = 0.01, sq_norm_avg_grad = 3.8153626918792725, avg_sq_norm_grad = 146.5260009765625,                  max_norm_grad = 15.291208267211914, var_grad = 142.71063232421875

>>> Round:   30 / Acc: 53.389% / Loss: 1.9363 /Time: 4.05s
======================================================================================================

= Test = round: 30 / acc: 54.790% / loss: 1.9208 / Time: 0.79s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 3.8568806648254395, avg_sq_norm_grad = 146.39039611816406,                  max_norm_grad = 15.211546897888184, var_grad = 142.53350830078125
round 31: local lr = 0.01, sq_norm_avg_grad = 4.223956108093262, avg_sq_norm_grad = 149.0703125,                  max_norm_grad = 15.375617027282715, var_grad = 144.8463592529297
round 32: local lr = 0.01, sq_norm_avg_grad = 4.3739495277404785, avg_sq_norm_grad = 150.5615234375,                  max_norm_grad = 15.273423194885254, var_grad = 146.1875762939453
round 33: local lr = 0.01, sq_norm_avg_grad = 4.636297702789307, avg_sq_norm_grad = 152.5505828857422,                  max_norm_grad = 15.352521896362305, var_grad = 147.91429138183594
round 34: local lr = 0.01, sq_norm_avg_grad = 5.030498504638672, avg_sq_norm_grad = 152.49630737304688,                  max_norm_grad = 15.436519622802734, var_grad = 147.46580505371094

>>> Round:   35 / Acc: 62.231% / Loss: 1.6218 /Time: 4.15s
======================================================================================================

= Test = round: 35 / acc: 64.030% / loss: 1.5961 / Time: 0.75s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 5.525951385498047, avg_sq_norm_grad = 155.8554229736328,                  max_norm_grad = 15.71688175201416, var_grad = 150.3294677734375
round 36: local lr = 0.01, sq_norm_avg_grad = 5.616243839263916, avg_sq_norm_grad = 151.55770874023438,                  max_norm_grad = 15.674161911010742, var_grad = 145.94146728515625
round 37: local lr = 0.01, sq_norm_avg_grad = 5.436376094818115, avg_sq_norm_grad = 150.14535522460938,                  max_norm_grad = 15.718753814697266, var_grad = 144.708984375
round 38: local lr = 0.01, sq_norm_avg_grad = 5.558024883270264, avg_sq_norm_grad = 151.78555297851562,                  max_norm_grad = 15.861762046813965, var_grad = 146.22752380371094
round 39: local lr = 0.01, sq_norm_avg_grad = 5.672637939453125, avg_sq_norm_grad = 148.89779663085938,                  max_norm_grad = 15.936283111572266, var_grad = 143.22515869140625

>>> Round:   40 / Acc: 68.806% / Loss: 1.3028 /Time: 4.13s
======================================================================================================

= Test = round: 40 / acc: 70.750% / loss: 1.2687 / Time: 0.79s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 6.238958835601807, avg_sq_norm_grad = 146.74234008789062,                  max_norm_grad = 15.972847938537598, var_grad = 140.50338745117188
round 41: local lr = 0.01, sq_norm_avg_grad = 5.918524265289307, avg_sq_norm_grad = 143.80076599121094,                  max_norm_grad = 15.8185453414917, var_grad = 137.8822479248047
round 42: local lr = 0.01, sq_norm_avg_grad = 6.203334808349609, avg_sq_norm_grad = 141.101318359375,                  max_norm_grad = 15.774443626403809, var_grad = 134.89797973632812
round 43: local lr = 0.01, sq_norm_avg_grad = 6.444360256195068, avg_sq_norm_grad = 140.07713317871094,                  max_norm_grad = 16.009201049804688, var_grad = 133.6327667236328
round 44: local lr = 0.01, sq_norm_avg_grad = 6.258682727813721, avg_sq_norm_grad = 137.00909423828125,                  max_norm_grad = 15.994897842407227, var_grad = 130.7504119873047

>>> Round:   45 / Acc: 72.862% / Loss: 1.0641 /Time: 4.10s
======================================================================================================

= Test = round: 45 / acc: 74.730% / loss: 1.0266 / Time: 0.81s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 6.50582218170166, avg_sq_norm_grad = 132.50155639648438,                  max_norm_grad = 16.03125762939453, var_grad = 125.99573516845703
round 46: local lr = 0.01, sq_norm_avg_grad = 6.309779167175293, avg_sq_norm_grad = 128.5918426513672,                  max_norm_grad = 16.057950973510742, var_grad = 122.28206634521484
round 47: local lr = 0.01, sq_norm_avg_grad = 6.547740459442139, avg_sq_norm_grad = 126.39966583251953,                  max_norm_grad = 16.201152801513672, var_grad = 119.8519287109375
round 48: local lr = 0.01, sq_norm_avg_grad = 7.1352152824401855, avg_sq_norm_grad = 122.62840270996094,                  max_norm_grad = 16.155773162841797, var_grad = 115.4931869506836
round 49: local lr = 0.01, sq_norm_avg_grad = 7.34173583984375, avg_sq_norm_grad = 121.25050354003906,                  max_norm_grad = 16.22499656677246, var_grad = 113.90876770019531

>>> Round:   50 / Acc: 75.264% / Loss: 0.9053 /Time: 4.24s
======================================================================================================

= Test = round: 50 / acc: 77.130% / loss: 0.8669 / Time: 0.80s
======================================================================================================

round 50: local lr = 0.01, sq_norm_avg_grad = 7.645462989807129, avg_sq_norm_grad = 119.95327758789062,                  max_norm_grad = 16.331090927124023, var_grad = 112.30781555175781
round 51: local lr = 0.01, sq_norm_avg_grad = 7.745625019073486, avg_sq_norm_grad = 118.84088897705078,                  max_norm_grad = 16.404733657836914, var_grad = 111.09526062011719
round 52: local lr = 0.01, sq_norm_avg_grad = 8.065569877624512, avg_sq_norm_grad = 115.09308624267578,                  max_norm_grad = 16.10529327392578, var_grad = 107.02751922607422
round 53: local lr = 0.01, sq_norm_avg_grad = 7.257417678833008, avg_sq_norm_grad = 113.44590759277344,                  max_norm_grad = 15.891406059265137, var_grad = 106.18849182128906
round 54: local lr = 0.01, sq_norm_avg_grad = 7.707231044769287, avg_sq_norm_grad = 111.4828109741211,                  max_norm_grad = 15.967121124267578, var_grad = 103.77558135986328

>>> Round:   55 / Acc: 77.520% / Loss: 0.7979 /Time: 4.03s
======================================================================================================

= Test = round: 55 / acc: 79.170% / loss: 0.7612 / Time: 0.77s
======================================================================================================

round 55: local lr = 0.01, sq_norm_avg_grad = 8.747694969177246, avg_sq_norm_grad = 112.49295806884766,                  max_norm_grad = 16.422603607177734, var_grad = 103.7452621459961
round 56: local lr = 0.01, sq_norm_avg_grad = 10.187809944152832, avg_sq_norm_grad = 114.35548400878906,                  max_norm_grad = 16.687767028808594, var_grad = 104.16767120361328
round 57: local lr = 0.01, sq_norm_avg_grad = 9.423455238342285, avg_sq_norm_grad = 110.59375,                  max_norm_grad = 16.64263916015625, var_grad = 101.17029571533203
round 58: local lr = 0.01, sq_norm_avg_grad = 9.004303932189941, avg_sq_norm_grad = 108.08135223388672,                  max_norm_grad = 16.531862258911133, var_grad = 99.0770492553711
round 59: local lr = 0.01, sq_norm_avg_grad = 9.076555252075195, avg_sq_norm_grad = 105.85750579833984,                  max_norm_grad = 16.39898109436035, var_grad = 96.78095245361328

>>> Round:   60 / Acc: 79.664% / Loss: 0.7161 /Time: 4.11s
======================================================================================================

= Test = round: 60 / acc: 81.430% / loss: 0.6802 / Time: 0.81s
======================================================================================================

round 60: local lr = 0.01, sq_norm_avg_grad = 8.18024730682373, avg_sq_norm_grad = 102.73890686035156,                  max_norm_grad = 15.958231925964355, var_grad = 94.55866241455078
round 61: local lr = 0.01, sq_norm_avg_grad = 8.2691650390625, avg_sq_norm_grad = 101.29601287841797,                  max_norm_grad = 15.705613136291504, var_grad = 93.02684783935547
round 62: local lr = 0.01, sq_norm_avg_grad = 8.456729888916016, avg_sq_norm_grad = 101.11204528808594,                  max_norm_grad = 15.810257911682129, var_grad = 92.65531921386719
round 63: local lr = 0.01, sq_norm_avg_grad = 8.027649879455566, avg_sq_norm_grad = 98.4572525024414,                  max_norm_grad = 15.592185974121094, var_grad = 90.42960357666016
round 64: local lr = 0.01, sq_norm_avg_grad = 7.922369003295898, avg_sq_norm_grad = 97.74107360839844,                  max_norm_grad = 15.629676818847656, var_grad = 89.8187026977539

>>> Round:   65 / Acc: 80.852% / Loss: 0.6573 /Time: 4.12s
======================================================================================================

= Test = round: 65 / acc: 82.600% / loss: 0.6204 / Time: 0.76s
======================================================================================================

round 65: local lr = 0.01, sq_norm_avg_grad = 8.241471290588379, avg_sq_norm_grad = 96.87036895751953,                  max_norm_grad = 15.761374473571777, var_grad = 88.62889862060547
round 66: local lr = 0.01, sq_norm_avg_grad = 8.556303024291992, avg_sq_norm_grad = 95.98944854736328,                  max_norm_grad = 15.787080764770508, var_grad = 87.43314361572266
round 67: local lr = 0.01, sq_norm_avg_grad = 8.436233520507812, avg_sq_norm_grad = 94.30745697021484,                  max_norm_grad = 15.680275917053223, var_grad = 85.87122344970703
round 68: local lr = 0.01, sq_norm_avg_grad = 8.992249488830566, avg_sq_norm_grad = 93.75113677978516,                  max_norm_grad = 15.78670597076416, var_grad = 84.7588882446289
round 69: local lr = 0.01, sq_norm_avg_grad = 7.4532294273376465, avg_sq_norm_grad = 88.6109848022461,                  max_norm_grad = 15.072556495666504, var_grad = 81.15775299072266

>>> Round:   70 / Acc: 82.347% / Loss: 0.6110 /Time: 4.16s
======================================================================================================

= Test = round: 70 / acc: 83.920% / loss: 0.5742 / Time: 0.78s
======================================================================================================

round 70: local lr = 0.01, sq_norm_avg_grad = 6.793631553649902, avg_sq_norm_grad = 85.26417541503906,                  max_norm_grad = 14.57622241973877, var_grad = 78.47054290771484
round 71: local lr = 0.01, sq_norm_avg_grad = 6.254795551300049, avg_sq_norm_grad = 83.54299926757812,                  max_norm_grad = 14.417308807373047, var_grad = 77.28820037841797
round 72: local lr = 0.01, sq_norm_avg_grad = 5.94658088684082, avg_sq_norm_grad = 82.44782257080078,                  max_norm_grad = 14.282247543334961, var_grad = 76.5012435913086
round 73: local lr = 0.01, sq_norm_avg_grad = 5.200287342071533, avg_sq_norm_grad = 79.36000061035156,                  max_norm_grad = 14.018253326416016, var_grad = 74.15971374511719
round 74: local lr = 0.01, sq_norm_avg_grad = 5.189270496368408, avg_sq_norm_grad = 79.44682312011719,                  max_norm_grad = 14.136347770690918, var_grad = 74.25755310058594

>>> Round:   75 / Acc: 83.362% / Loss: 0.5689 /Time: 4.18s
======================================================================================================

= Test = round: 75 / acc: 84.910% / loss: 0.5328 / Time: 0.80s
======================================================================================================

round 75: local lr = 0.01, sq_norm_avg_grad = 6.198219299316406, avg_sq_norm_grad = 81.2161636352539,                  max_norm_grad = 14.531874656677246, var_grad = 75.0179443359375
round 76: local lr = 0.01, sq_norm_avg_grad = 5.591428756713867, avg_sq_norm_grad = 78.7489013671875,                  max_norm_grad = 14.29251766204834, var_grad = 73.157470703125
round 77: local lr = 0.01, sq_norm_avg_grad = 5.135551452636719, avg_sq_norm_grad = 75.7923583984375,                  max_norm_grad = 13.99686050415039, var_grad = 70.65680694580078
round 78: local lr = 0.01, sq_norm_avg_grad = 5.230316162109375, avg_sq_norm_grad = 75.63167572021484,                  max_norm_grad = 13.839262962341309, var_grad = 70.40135955810547
round 79: local lr = 0.01, sq_norm_avg_grad = 4.549546241760254, avg_sq_norm_grad = 71.98641204833984,                  max_norm_grad = 13.288585662841797, var_grad = 67.4368667602539

>>> Round:   80 / Acc: 84.668% / Loss: 0.5309 /Time: 5.40s
======================================================================================================

= Test = round: 80 / acc: 86.190% / loss: 0.4956 / Time: 1.00s
======================================================================================================

round 80: local lr = 0.01, sq_norm_avg_grad = 4.092458724975586, avg_sq_norm_grad = 70.7454833984375,                  max_norm_grad = 13.1065092086792, var_grad = 66.65302276611328
round 81: local lr = 0.01, sq_norm_avg_grad = 3.9417614936828613, avg_sq_norm_grad = 69.5697021484375,                  max_norm_grad = 13.131422996520996, var_grad = 65.62793731689453
round 82: local lr = 0.01, sq_norm_avg_grad = 3.762540102005005, avg_sq_norm_grad = 67.2366714477539,                  max_norm_grad = 13.162099838256836, var_grad = 63.4741325378418
round 83: local lr = 0.01, sq_norm_avg_grad = 4.482024669647217, avg_sq_norm_grad = 68.89159393310547,                  max_norm_grad = 13.475728034973145, var_grad = 64.4095687866211
round 84: local lr = 0.01, sq_norm_avg_grad = 3.8931756019592285, avg_sq_norm_grad = 66.39918518066406,                  max_norm_grad = 12.992396354675293, var_grad = 62.50600814819336

>>> Round:   85 / Acc: 85.179% / Loss: 0.5062 /Time: 5.34s
======================================================================================================

= Test = round: 85 / acc: 86.640% / loss: 0.4710 / Time: 1.02s
======================================================================================================

round 85: local lr = 0.01, sq_norm_avg_grad = 4.030547142028809, avg_sq_norm_grad = 66.6964111328125,                  max_norm_grad = 12.980829238891602, var_grad = 62.665863037109375
round 86: local lr = 0.01, sq_norm_avg_grad = 4.051477909088135, avg_sq_norm_grad = 66.33846282958984,                  max_norm_grad = 13.064870834350586, var_grad = 62.286983489990234
round 87: local lr = 0.01, sq_norm_avg_grad = 4.162935733795166, avg_sq_norm_grad = 66.11959075927734,                  max_norm_grad = 13.071176528930664, var_grad = 61.9566535949707
round 88: local lr = 0.01, sq_norm_avg_grad = 4.43230676651001, avg_sq_norm_grad = 65.93743133544922,                  max_norm_grad = 13.137696266174316, var_grad = 61.505123138427734
round 89: local lr = 0.01, sq_norm_avg_grad = 5.056194305419922, avg_sq_norm_grad = 66.85082244873047,                  max_norm_grad = 13.518477439880371, var_grad = 61.79462814331055

>>> Round:   90 / Acc: 85.293% / Loss: 0.4916 /Time: 5.41s
======================================================================================================

= Test = round: 90 / acc: 86.800% / loss: 0.4557 / Time: 0.99s
======================================================================================================

round 90: local lr = 0.01, sq_norm_avg_grad = 4.715732097625732, avg_sq_norm_grad = 64.76952362060547,                  max_norm_grad = 13.195425987243652, var_grad = 60.05379104614258
round 91: local lr = 0.01, sq_norm_avg_grad = 4.1142497062683105, avg_sq_norm_grad = 63.09428405761719,                  max_norm_grad = 12.793022155761719, var_grad = 58.98003387451172
round 92: local lr = 0.01, sq_norm_avg_grad = 4.084158420562744, avg_sq_norm_grad = 62.4333610534668,                  max_norm_grad = 12.736519813537598, var_grad = 58.34920120239258
round 93: local lr = 0.01, sq_norm_avg_grad = 4.023228168487549, avg_sq_norm_grad = 61.16379165649414,                  max_norm_grad = 12.598684310913086, var_grad = 57.14056396484375
round 94: local lr = 0.01, sq_norm_avg_grad = 3.400212287902832, avg_sq_norm_grad = 58.4684944152832,                  max_norm_grad = 12.148977279663086, var_grad = 55.06828308105469

>>> Round:   95 / Acc: 86.083% / Loss: 0.4718 /Time: 5.40s
======================================================================================================

= Test = round: 95 / acc: 87.420% / loss: 0.4372 / Time: 1.00s
======================================================================================================

round 95: local lr = 0.01, sq_norm_avg_grad = 4.018723487854004, avg_sq_norm_grad = 59.08449172973633,                  max_norm_grad = 12.284372329711914, var_grad = 55.06576919555664
round 96: local lr = 0.01, sq_norm_avg_grad = 3.505859136581421, avg_sq_norm_grad = 57.354736328125,                  max_norm_grad = 11.762408256530762, var_grad = 53.848876953125
round 97: local lr = 0.01, sq_norm_avg_grad = 3.2456769943237305, avg_sq_norm_grad = 56.31536865234375,                  max_norm_grad = 11.619027137756348, var_grad = 53.0696907043457
round 98: local lr = 0.01, sq_norm_avg_grad = 3.227842092514038, avg_sq_norm_grad = 56.1183967590332,                  max_norm_grad = 11.932830810546875, var_grad = 52.89055633544922
round 99: local lr = 0.01, sq_norm_avg_grad = 3.2126529216766357, avg_sq_norm_grad = 55.474159240722656,                  max_norm_grad = 11.853538513183594, var_grad = 52.261505126953125

>>> Round:  100 / Acc: 86.804% / Loss: 0.4476 /Time: 4.98s
======================================================================================================

= Test = round: 100 / acc: 88.380% / loss: 0.4127 / Time: 0.89s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.3028, Train_acc: 0.6055, Test_loss: 1.2864, Test_acc: 0.6076
Epoch: 006, Train_loss: 1.1208, Train_acc: 0.6684, Test_loss: 1.1064, Test_acc: 0.6739
Epoch: 011, Train_loss: 1.1018, Train_acc: 0.6675, Test_loss: 1.0966, Test_acc: 0.6715
Epoch: 016, Train_loss: 1.0757, Train_acc: 0.6801, Test_loss: 1.0675, Test_acc: 0.6879
Epoch: 021, Train_loss: 1.0906, Train_acc: 0.6728, Test_loss: 1.0793, Test_acc: 0.6780
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001182456_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001182456_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.0757022151094224, 0.6801240656938018, 1.067515139016373, 0.687923564048439]
model_source_only: [2.423722427991792, 0.40329824918221724, 2.4557193472787757, 0.40339962226419285]
fl_test_acc_mean 0.8824
model_source_only_test_acc_mean 0.40339962226419285
model_ft_test_acc_mean 0.687923564048439
