nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.1
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928194505
FL pretrained model will be saved at ./models/lenet_mnist_20230928194505.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 5.015% / Loss: 2.3083 /Time: 5.36s
======================================================================================================

= Test = round: 0 / acc: 4.760% / loss: 2.3084 / Time: 0.84s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.01855696551501751, avg_sq_norm_grad = 1.1051887273788452,                  max_norm_grad = 1.21155846118927, var_grad = 1.0866317749023438
round 2: local lr = 0.01, sq_norm_avg_grad = 0.0152758052572608, avg_sq_norm_grad = 0.9649885296821594,                  max_norm_grad = 1.114930510520935, var_grad = 0.9497127532958984
round 3: local lr = 0.01, sq_norm_avg_grad = 0.013363981619477272, avg_sq_norm_grad = 0.8823003172874451,                  max_norm_grad = 1.050817608833313, var_grad = 0.8689363598823547
round 4: local lr = 0.01, sq_norm_avg_grad = 0.012074383907020092, avg_sq_norm_grad = 0.8287620544433594,                  max_norm_grad = 1.007601261138916, var_grad = 0.8166876435279846

>>> Round:    5 / Acc: 9.934% / Loss: 2.3010 /Time: 4.57s
======================================================================================================

= Test = round: 5 / acc: 10.870% / loss: 2.3009 / Time: 1.17s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.01133816223591566, avg_sq_norm_grad = 0.7931086421012878,                  max_norm_grad = 0.9794961214065552, var_grad = 0.7817704677581787
round 6: local lr = 0.01, sq_norm_avg_grad = 0.010947429575026035, avg_sq_norm_grad = 0.7691510319709778,                  max_norm_grad = 0.9625197052955627, var_grad = 0.7582036256790161
round 7: local lr = 0.01, sq_norm_avg_grad = 0.010805265046656132, avg_sq_norm_grad = 0.7525869011878967,                  max_norm_grad = 0.9508347511291504, var_grad = 0.7417816519737244
round 8: local lr = 0.01, sq_norm_avg_grad = 0.010856334120035172, avg_sq_norm_grad = 0.7415278553962708,                  max_norm_grad = 0.9434534907341003, var_grad = 0.7306715250015259
round 9: local lr = 0.01, sq_norm_avg_grad = 0.011069935746490955, avg_sq_norm_grad = 0.7348251342773438,                  max_norm_grad = 0.9393717646598816, var_grad = 0.7237551808357239

>>> Round:   10 / Acc: 12.229% / Loss: 2.2959 /Time: 4.32s
======================================================================================================

= Test = round: 10 / acc: 13.270% / loss: 2.2957 / Time: 0.85s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.011407286860048771, avg_sq_norm_grad = 0.7311456203460693,                  max_norm_grad = 0.936754584312439, var_grad = 0.7197383046150208
round 11: local lr = 0.01, sq_norm_avg_grad = 0.011891248635947704, avg_sq_norm_grad = 0.7302118539810181,                  max_norm_grad = 0.9364722371101379, var_grad = 0.7183206081390381
round 12: local lr = 0.01, sq_norm_avg_grad = 0.012509339489042759, avg_sq_norm_grad = 0.7317884564399719,                  max_norm_grad = 0.9380502700805664, var_grad = 0.7192791104316711
round 13: local lr = 0.01, sq_norm_avg_grad = 0.013240271247923374, avg_sq_norm_grad = 0.7358118295669556,                  max_norm_grad = 0.9404128193855286, var_grad = 0.7225715517997742
round 14: local lr = 0.01, sq_norm_avg_grad = 0.014109435491263866, avg_sq_norm_grad = 0.7417041063308716,                  max_norm_grad = 0.9450810551643372, var_grad = 0.7275946736335754

>>> Round:   15 / Acc: 17.157% / Loss: 2.2898 /Time: 4.47s
======================================================================================================

= Test = round: 15 / acc: 18.280% / loss: 2.2895 / Time: 0.84s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 0.015147465281188488, avg_sq_norm_grad = 0.7498223781585693,                  max_norm_grad = 0.9504425525665283, var_grad = 0.7346749305725098
round 16: local lr = 0.01, sq_norm_avg_grad = 0.016360286623239517, avg_sq_norm_grad = 0.7599599957466125,                  max_norm_grad = 0.9580594897270203, var_grad = 0.7435997128486633
round 17: local lr = 0.01, sq_norm_avg_grad = 0.01771789975464344, avg_sq_norm_grad = 0.7724754214286804,                  max_norm_grad = 0.967556893825531, var_grad = 0.7547575235366821
round 18: local lr = 0.01, sq_norm_avg_grad = 0.019258039072155952, avg_sq_norm_grad = 0.7867494821548462,                  max_norm_grad = 0.9776614904403687, var_grad = 0.7674914598464966
round 19: local lr = 0.01, sq_norm_avg_grad = 0.021022306755185127, avg_sq_norm_grad = 0.8035638332366943,                  max_norm_grad = 0.9881232976913452, var_grad = 0.7825415134429932

>>> Round:   20 / Acc: 23.046% / Loss: 2.2812 /Time: 4.54s
======================================================================================================

= Test = round: 20 / acc: 23.960% / loss: 2.2808 / Time: 0.94s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 0.02305780164897442, avg_sq_norm_grad = 0.822320282459259,                  max_norm_grad = 1.0041477680206299, var_grad = 0.7992624640464783
round 21: local lr = 0.01, sq_norm_avg_grad = 0.02540571428835392, avg_sq_norm_grad = 0.8441412448883057,                  max_norm_grad = 1.0227757692337036, var_grad = 0.8187355399131775
round 22: local lr = 0.01, sq_norm_avg_grad = 0.02809026651084423, avg_sq_norm_grad = 0.8689970970153809,                  max_norm_grad = 1.0449272394180298, var_grad = 0.8409068584442139
round 23: local lr = 0.01, sq_norm_avg_grad = 0.03115835413336754, avg_sq_norm_grad = 0.897973895072937,                  max_norm_grad = 1.0678496360778809, var_grad = 0.8668155670166016
round 24: local lr = 0.01, sq_norm_avg_grad = 0.034705907106399536, avg_sq_norm_grad = 0.9308880567550659,                  max_norm_grad = 1.092686653137207, var_grad = 0.8961821794509888

>>> Round:   25 / Acc: 29.677% / Loss: 2.2687 /Time: 4.33s
======================================================================================================

= Test = round: 25 / acc: 30.740% / loss: 2.2679 / Time: 0.83s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 0.038770806044340134, avg_sq_norm_grad = 0.9676345586776733,                  max_norm_grad = 1.1205123662948608, var_grad = 0.9288637638092041
round 26: local lr = 0.01, sq_norm_avg_grad = 0.04358643293380737, avg_sq_norm_grad = 1.009714126586914,                  max_norm_grad = 1.1504980325698853, var_grad = 0.9661276936531067
round 27: local lr = 0.01, sq_norm_avg_grad = 0.04901272431015968, avg_sq_norm_grad = 1.0572046041488647,                  max_norm_grad = 1.1839162111282349, var_grad = 1.0081918239593506
round 28: local lr = 0.01, sq_norm_avg_grad = 0.05527905374765396, avg_sq_norm_grad = 1.1095836162567139,                  max_norm_grad = 1.2201157808303833, var_grad = 1.054304599761963
round 29: local lr = 0.01, sq_norm_avg_grad = 0.0626542940735817, avg_sq_norm_grad = 1.1689285039901733,                  max_norm_grad = 1.2597163915634155, var_grad = 1.1062742471694946

>>> Round:   30 / Acc: 38.155% / Loss: 2.2496 /Time: 4.52s
======================================================================================================

= Test = round: 30 / acc: 39.290% / loss: 2.2483 / Time: 0.82s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 0.07111538201570511, avg_sq_norm_grad = 1.2350860834121704,                  max_norm_grad = 1.301619529724121, var_grad = 1.163970708847046
round 31: local lr = 0.01, sq_norm_avg_grad = 0.0809033066034317, avg_sq_norm_grad = 1.3108984231948853,                  max_norm_grad = 1.3497331142425537, var_grad = 1.2299951314926147
round 32: local lr = 0.01, sq_norm_avg_grad = 0.092422716319561, avg_sq_norm_grad = 1.3941949605941772,                  max_norm_grad = 1.4000872373580933, var_grad = 1.3017722368240356
round 33: local lr = 0.01, sq_norm_avg_grad = 0.10599017143249512, avg_sq_norm_grad = 1.4903963804244995,                  max_norm_grad = 1.455673336982727, var_grad = 1.3844062089920044
round 34: local lr = 0.01, sq_norm_avg_grad = 0.12178707122802734, avg_sq_norm_grad = 1.5994863510131836,                  max_norm_grad = 1.516811490058899, var_grad = 1.4776992797851562

>>> Round:   35 / Acc: 43.911% / Loss: 2.2198 /Time: 4.63s
======================================================================================================

= Test = round: 35 / acc: 44.700% / loss: 2.2177 / Time: 0.88s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 0.14044401049613953, avg_sq_norm_grad = 1.7233898639678955,                  max_norm_grad = 1.582850456237793, var_grad = 1.5829458236694336
round 36: local lr = 0.01, sq_norm_avg_grad = 0.16231146454811096, avg_sq_norm_grad = 1.860870599746704,                  max_norm_grad = 1.6560306549072266, var_grad = 1.6985591650009155
round 37: local lr = 0.01, sq_norm_avg_grad = 0.18808285892009735, avg_sq_norm_grad = 2.015212297439575,                  max_norm_grad = 1.7387923002243042, var_grad = 1.8271294832229614
round 38: local lr = 0.01, sq_norm_avg_grad = 0.21853554248809814, avg_sq_norm_grad = 2.197453260421753,                  max_norm_grad = 1.8334921598434448, var_grad = 1.9789177179336548
round 39: local lr = 0.01, sq_norm_avg_grad = 0.25455498695373535, avg_sq_norm_grad = 2.404179573059082,                  max_norm_grad = 1.9358046054840088, var_grad = 2.1496245861053467

>>> Round:   40 / Acc: 48.862% / Loss: 2.1724 /Time: 5.28s
======================================================================================================

= Test = round: 40 / acc: 49.750% / loss: 2.1690 / Time: 1.13s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 0.29763877391815186, avg_sq_norm_grad = 2.6505966186523438,                  max_norm_grad = 2.0499377250671387, var_grad = 2.3529577255249023
round 41: local lr = 0.01, sq_norm_avg_grad = 0.34806838631629944, avg_sq_norm_grad = 2.9223809242248535,                  max_norm_grad = 2.170246124267578, var_grad = 2.574312448501587
round 42: local lr = 0.01, sq_norm_avg_grad = 0.4076194167137146, avg_sq_norm_grad = 3.23738956451416,                  max_norm_grad = 2.299243688583374, var_grad = 2.829770088195801
round 43: local lr = 0.010029035620391369, sq_norm_avg_grad = 0.476722776889801, avg_sq_norm_grad = 3.59246826171875,                  max_norm_grad = 2.4356741905212402, var_grad = 3.1157455444335938
round 44: local lr = 0.010472908616065979, sq_norm_avg_grad = 0.5572577118873596, avg_sq_norm_grad = 4.021378517150879,                  max_norm_grad = 2.5922763347625732, var_grad = 3.464120864868164

>>> Round:   45 / Acc: 55.109% / Loss: 2.0943 /Time: 5.43s
======================================================================================================

= Test = round: 45 / acc: 56.420% / loss: 2.0888 / Time: 1.08s
======================================================================================================

round 45: local lr = 0.010755176655948162, sq_norm_avg_grad = 0.6553186774253845, avg_sq_norm_grad = 4.604910373687744,                  max_norm_grad = 2.7845265865325928, var_grad = 3.949591636657715
round 46: local lr = 0.010870176367461681, sq_norm_avg_grad = 0.7737723588943481, avg_sq_norm_grad = 5.379759311676025,                  max_norm_grad = 3.0193610191345215, var_grad = 4.605987071990967
round 47: local lr = 0.01069588866084814, sq_norm_avg_grad = 0.9179607033729553, avg_sq_norm_grad = 6.486246109008789,                  max_norm_grad = 3.30895733833313, var_grad = 5.5682854652404785
round 48: local lr = 0.010392672382295132, sq_norm_avg_grad = 1.087679386138916, avg_sq_norm_grad = 7.909697532653809,                  max_norm_grad = 3.6316840648651123, var_grad = 6.822018146514893
round 49: local lr = 0.010089010931551456, sq_norm_avg_grad = 1.2787140607833862, avg_sq_norm_grad = 9.5787992477417,                  max_norm_grad = 3.9534904956817627, var_grad = 8.300085067749023

>>> Round:   50 / Acc: 58.980% / Loss: 1.9303 /Time: 5.13s
======================================================================================================

= Test = round: 50 / acc: 60.090% / loss: 1.9200 / Time: 1.02s
======================================================================================================

round 50: local lr = 0.010089010931551456, sq_norm_avg_grad = 1.4961012601852417, avg_sq_norm_grad = 11.41613483428955,                  max_norm_grad = 4.263174057006836, var_grad = 9.92003345489502
round 51: local lr = 0.010089010931551456, sq_norm_avg_grad = 1.7373998165130615, avg_sq_norm_grad = 13.709869384765625,                  max_norm_grad = 4.600011348724365, var_grad = 11.972469329833984
round 52: local lr = 0.010089010931551456, sq_norm_avg_grad = 2.0169663429260254, avg_sq_norm_grad = 15.806254386901855,                  max_norm_grad = 4.886905670166016, var_grad = 13.789287567138672
round 53: local lr = 0.010893176309764385, sq_norm_avg_grad = 2.3729779720306396, avg_sq_norm_grad = 16.463621139526367,                  max_norm_grad = 5.003352165222168, var_grad = 14.090642929077148
round 54: local lr = 0.012653982266783714, sq_norm_avg_grad = 2.795814275741577, avg_sq_norm_grad = 16.698110580444336,                  max_norm_grad = 5.048667907714844, var_grad = 13.90229606628418

>>> Round:   55 / Acc: 64.142% / Loss: 1.8310 /Time: 4.90s
======================================================================================================

= Test = round: 55 / acc: 65.180% / loss: 1.8181 / Time: 1.05s
======================================================================================================

round 55: local lr = 0.014959888532757759, sq_norm_avg_grad = 3.2873826026916504, avg_sq_norm_grad = 16.607646942138672,                  max_norm_grad = 5.0191969871521, var_grad = 13.32026481628418
round 56: local lr = 0.02711707353591919, sq_norm_avg_grad = 4.111011028289795, avg_sq_norm_grad = 11.45755672454834,                  max_norm_grad = 4.464666366577148, var_grad = 7.346545696258545
round 57: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.021850042045116425, avg_sq_norm_grad = 0.7081977128982544,                  max_norm_grad = 0.9528213143348694, var_grad = 0.6863476634025574
round 58: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.014683658257126808, avg_sq_norm_grad = 0.6711634397506714,                  max_norm_grad = 0.9193772077560425, var_grad = 0.6564797759056091
round 59: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.011241461150348186, avg_sq_norm_grad = 0.6534973978996277,                  max_norm_grad = 0.897219181060791, var_grad = 0.642255961894989

>>> Round:   60 / Acc: 9.810% / Loss: 2.3054 /Time: 5.15s
======================================================================================================

= Test = round: 60 / acc: 9.310% / loss: 2.3049 / Time: 0.85s
======================================================================================================

round 60: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.008703328669071198, avg_sq_norm_grad = 0.6468395590782166,                  max_norm_grad = 0.888884961605072, var_grad = 0.6381362080574036
round 61: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.007451073732227087, avg_sq_norm_grad = 0.6491342186927795,                  max_norm_grad = 0.8933864831924438, var_grad = 0.6416831612586975
round 62: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.0075216940604150295, avg_sq_norm_grad = 0.6584925651550293,                  max_norm_grad = 0.9064965844154358, var_grad = 0.6509708762168884
round 63: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.008818604052066803, avg_sq_norm_grad = 0.6731729507446289,                  max_norm_grad = 0.9244650602340698, var_grad = 0.6643543243408203
round 64: local lr = 0.02711707353591919, sq_norm_avg_grad = 0.012065742164850235, avg_sq_norm_grad = 0.697891354560852,                  max_norm_grad = 0.9482458829879761, var_grad = 0.6858255863189697

>>> Round:   65 / Acc: 17.369% / Loss: 2.2899 /Time: 4.98s
======================================================================================================

= Test = round: 65 / acc: 17.580% / loss: 2.2893 / Time: 1.02s
======================================================================================================

Training early stopped. Model saved at ./models/lenet_mnist_20230928194505.pt.

>>> Round:  200 / Acc: 17.369% / Loss: 2.2899 /Time: 5.09s
======================================================================================================

= Test = round: 200 / acc: 17.580% / loss: 2.2893 / Time: 0.95s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.7018, Train_acc: 0.4489, Test_loss: 1.6983, Test_acc: 0.4502
Epoch: 006, Train_loss: 1.2997, Train_acc: 0.5807, Test_loss: 1.2925, Test_acc: 0.5778
Epoch: 011, Train_loss: 1.2164, Train_acc: 0.6090, Test_loss: 1.2106, Test_acc: 0.6123
Epoch: 016, Train_loss: 1.1662, Train_acc: 0.6203, Test_loss: 1.1652, Test_acc: 0.6223
Epoch: 021, Train_loss: 1.1297, Train_acc: 0.6332, Test_loss: 1.1311, Test_acc: 0.6358
Epoch: 026, Train_loss: 1.0985, Train_acc: 0.6418, Test_loss: 1.1019, Test_acc: 0.6429
Epoch: 031, Train_loss: 1.0811, Train_acc: 0.6469, Test_loss: 1.0913, Test_acc: 0.6478
Epoch: 036, Train_loss: 1.0631, Train_acc: 0.6533, Test_loss: 1.0765, Test_acc: 0.6520
Epoch: 041, Train_loss: 1.0458, Train_acc: 0.6589, Test_loss: 1.0597, Test_acc: 0.6615
Epoch: 046, Train_loss: 1.0310, Train_acc: 0.6623, Test_loss: 1.0518, Test_acc: 0.6610
Epoch: 051, Train_loss: 1.0164, Train_acc: 0.6660, Test_loss: 1.0388, Test_acc: 0.6620
Epoch: 056, Train_loss: 1.0044, Train_acc: 0.6725, Test_loss: 1.0309, Test_acc: 0.6680
Epoch: 061, Train_loss: 1.0001, Train_acc: 0.6705, Test_loss: 1.0304, Test_acc: 0.6649
Epoch: 066, Train_loss: 0.9885, Train_acc: 0.6747, Test_loss: 1.0172, Test_acc: 0.6685
Epoch: 071, Train_loss: 0.9859, Train_acc: 0.6753, Test_loss: 1.0156, Test_acc: 0.6703
Epoch: 076, Train_loss: 0.9720, Train_acc: 0.6813, Test_loss: 1.0059, Test_acc: 0.6779
Epoch: 081, Train_loss: 0.9686, Train_acc: 0.6842, Test_loss: 1.0090, Test_acc: 0.6778
Epoch: 086, Train_loss: 0.9676, Train_acc: 0.6847, Test_loss: 1.0047, Test_acc: 0.6763
Epoch: 091, Train_loss: 0.9561, Train_acc: 0.6883, Test_loss: 0.9976, Test_acc: 0.6791
Epoch: 096, Train_loss: 0.9485, Train_acc: 0.6903, Test_loss: 0.9939, Test_acc: 0.6790
Epoch: 101, Train_loss: 0.9460, Train_acc: 0.6896, Test_loss: 0.9901, Test_acc: 0.6798
Epoch: 106, Train_loss: 0.9457, Train_acc: 0.6915, Test_loss: 0.9880, Test_acc: 0.6804
Epoch: 111, Train_loss: 0.9410, Train_acc: 0.6918, Test_loss: 0.9899, Test_acc: 0.6825
Epoch: 116, Train_loss: 0.9385, Train_acc: 0.6910, Test_loss: 0.9877, Test_acc: 0.6828
Epoch: 121, Train_loss: 0.9353, Train_acc: 0.6937, Test_loss: 0.9860, Test_acc: 0.6826
Epoch: 126, Train_loss: 0.9324, Train_acc: 0.6954, Test_loss: 0.9856, Test_acc: 0.6869
Epoch: 131, Train_loss: 0.9267, Train_acc: 0.6951, Test_loss: 0.9807, Test_acc: 0.6833
Epoch: 136, Train_loss: 0.9367, Train_acc: 0.6927, Test_loss: 0.9910, Test_acc: 0.6808
Epoch: 141, Train_loss: 0.9250, Train_acc: 0.6966, Test_loss: 0.9788, Test_acc: 0.6875
Epoch: 146, Train_loss: 0.9215, Train_acc: 0.6988, Test_loss: 0.9823, Test_acc: 0.6873
Epoch: 151, Train_loss: 0.9268, Train_acc: 0.6959, Test_loss: 0.9843, Test_acc: 0.6843
Epoch: 156, Train_loss: 0.9204, Train_acc: 0.6971, Test_loss: 0.9824, Test_acc: 0.6836
Epoch: 161, Train_loss: 0.9193, Train_acc: 0.7007, Test_loss: 0.9783, Test_acc: 0.6858
Epoch: 166, Train_loss: 0.9153, Train_acc: 0.6993, Test_loss: 0.9772, Test_acc: 0.6858
Epoch: 171, Train_loss: 0.9122, Train_acc: 0.7016, Test_loss: 0.9772, Test_acc: 0.6861
Epoch: 176, Train_loss: 0.9083, Train_acc: 0.7027, Test_loss: 0.9775, Test_acc: 0.6893
Epoch: 181, Train_loss: 0.9115, Train_acc: 0.7012, Test_loss: 0.9793, Test_acc: 0.6850
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20230928194505_model_ft.pt.
Model saved at ./models/ft_checkpoints/20230928194505_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [0.9081048567190697, 0.7051575397027169, 0.9720550436878744, 0.6899233418509054]
model_source_only: [2.211144654981682, 0.2399789834070609, 2.212486736218991, 0.24075102766359294]
fl_test_acc_mean 0.6518
model_source_only_test_acc_mean 0.24075102766359294
model_ft_test_acc_mean 0.6899233418509054
