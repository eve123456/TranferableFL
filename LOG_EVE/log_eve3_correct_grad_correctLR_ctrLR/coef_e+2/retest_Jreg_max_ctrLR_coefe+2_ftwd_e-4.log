nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928193549
FL pretrained model will be saved at ./models/lenet_mnist_20230928193549.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 

nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 100.0
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928193632
FL pretrained model will be saved at ./models/lenet_mnist_20230928193632.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 16.535% / Loss: 2.3023 /Time: 4.14s
======================================================================================================

= Test = round: 0 / acc: 16.650% / loss: 2.3007 / Time: 0.78s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.05081027373671532, avg_sq_norm_grad = 1.94959557056427,                  max_norm_grad = 1.6001619100570679, var_grad = 1.8987853527069092
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
round 2: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
round 3: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
round 4: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.

>>> Round:    5 / Acc: 10.000% / Loss: nan /Time: 4.20s
======================================================================================================

= Test = round: 5 / acc: 9.800% / loss: nan / Time: 0.77s
======================================================================================================

WARNING:root:NaN or Inf found in input tensor.
round 5: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
round 6: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
round 7: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
round 8: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
round 9: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.

>>> Round:   10 / Acc: 10.000% / Loss: nan /Time: 4.14s
======================================================================================================

= Test = round: 10 / acc: 9.800% / loss: nan / Time: 0.81s
======================================================================================================

WARNING:root:NaN or Inf found in input tensor.
round 10: local lr = 0.01, sq_norm_avg_grad = nan, avg_sq_norm_grad = nan,                  max_norm_grad = nan, var_grad = nan
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
Training early stopped. Model saved at ./models/lenet_mnist_20230928193632.pt.
WARNING:root:NaN or Inf found in input tensor.

>>> Round:  200 / Acc: 10.000% / Loss: nan /Time: 4.32s
======================================================================================================

= Test = round: 200 / acc: 9.800% / loss: nan / Time: 0.83s
======================================================================================================

WARNING:root:NaN or Inf found in input tensor.
>>> Training model_ft
Epoch: 001, Train_loss: 1.5950, Train_acc: 0.5043, Test_loss: 1.5864, Test_acc: 0.5057
Epoch: 006, Train_loss: 1.1888, Train_acc: 0.6213, Test_loss: 1.1787, Test_acc: 0.6262
Epoch: 011, Train_loss: 1.1060, Train_acc: 0.6511, Test_loss: 1.0992, Test_acc: 0.6544
Epoch: 016, Train_loss: 1.0546, Train_acc: 0.6678, Test_loss: 1.0500, Test_acc: 0.6730
Epoch: 021, Train_loss: 1.0184, Train_acc: 0.6793, Test_loss: 1.0171, Test_acc: 0.6823
Epoch: 026, Train_loss: 0.9913, Train_acc: 0.6853, Test_loss: 0.9937, Test_acc: 0.6825
Epoch: 031, Train_loss: 0.9638, Train_acc: 0.6920, Test_loss: 0.9695, Test_acc: 0.6934
Epoch: 036, Train_loss: 0.9468, Train_acc: 0.6991, Test_loss: 0.9563, Test_acc: 0.6961
Epoch: 041, Train_loss: 0.9323, Train_acc: 0.7040, Test_loss: 0.9472, Test_acc: 0.6980
Epoch: 046, Train_loss: 0.9197, Train_acc: 0.7081, Test_loss: 0.9397, Test_acc: 0.7010
Epoch: 051, Train_loss: 0.9101, Train_acc: 0.7103, Test_loss: 0.9342, Test_acc: 0.7004
Epoch: 056, Train_loss: 0.8977, Train_acc: 0.7151, Test_loss: 0.9230, Test_acc: 0.7036
Epoch: 061, Train_loss: 0.8946, Train_acc: 0.7150, Test_loss: 0.9209, Test_acc: 0.7040
Epoch: 066, Train_loss: 0.8866, Train_acc: 0.7185, Test_loss: 0.9145, Test_acc: 0.7096
Epoch: 071, Train_loss: 0.8797, Train_acc: 0.7204, Test_loss: 0.9084, Test_acc: 0.7105
Epoch: 076, Train_loss: 0.8736, Train_acc: 0.7228, Test_loss: 0.9068, Test_acc: 0.7111
Epoch: 081, Train_loss: 0.8671, Train_acc: 0.7247, Test_loss: 0.9022, Test_acc: 0.7104
Epoch: 086, Train_loss: 0.8610, Train_acc: 0.7276, Test_loss: 0.8965, Test_acc: 0.7125
Epoch: 091, Train_loss: 0.8555, Train_acc: 0.7290, Test_loss: 0.8955, Test_acc: 0.7141
Epoch: 096, Train_loss: 0.8506, Train_acc: 0.7302, Test_loss: 0.8918, Test_acc: 0.7125
Epoch: 101, Train_loss: 0.8470, Train_acc: 0.7333, Test_loss: 0.8891, Test_acc: 0.7170
Epoch: 106, Train_loss: 0.8436, Train_acc: 0.7319, Test_loss: 0.8849, Test_acc: 0.7174
Epoch: 111, Train_loss: 0.8394, Train_acc: 0.7345, Test_loss: 0.8850, Test_acc: 0.7183
Epoch: 116, Train_loss: 0.8356, Train_acc: 0.7349, Test_loss: 0.8833, Test_acc: 0.7178
Epoch: 121, Train_loss: 0.8343, Train_acc: 0.7354, Test_loss: 0.8827, Test_acc: 0.7160
Epoch: 126, Train_loss: 0.8310, Train_acc: 0.7380, Test_loss: 0.8830, Test_acc: 0.7207
Epoch: 131, Train_loss: 0.8285, Train_acc: 0.7388, Test_loss: 0.8810, Test_acc: 0.7206
Epoch: 136, Train_loss: 0.8263, Train_acc: 0.7392, Test_loss: 0.8817, Test_acc: 0.7177
Epoch: 141, Train_loss: 0.8231, Train_acc: 0.7397, Test_loss: 0.8796, Test_acc: 0.7191
Epoch: 146, Train_loss: 0.8201, Train_acc: 0.7406, Test_loss: 0.8774, Test_acc: 0.7197
Epoch: 151, Train_loss: 0.8183, Train_acc: 0.7426, Test_loss: 0.8762, Test_acc: 0.7208
Epoch: 156, Train_loss: 0.8158, Train_acc: 0.7416, Test_loss: 0.8762, Test_acc: 0.7224
Epoch: 161, Train_loss: 0.8168, Train_acc: 0.7404, Test_loss: 0.8765, Test_acc: 0.7204
Epoch: 166, Train_loss: 0.8110, Train_acc: 0.7445, Test_loss: 0.8734, Test_acc: 0.7227
Epoch: 171, Train_loss: 0.8111, Train_acc: 0.7428, Test_loss: 0.8722, Test_acc: 0.7214
Epoch: 176, Train_loss: 0.8096, Train_acc: 0.7439, Test_loss: 0.8706, Test_acc: 0.7205
Epoch: 181, Train_loss: 0.8100, Train_acc: 0.7440, Test_loss: 0.8751, Test_acc: 0.7230
Epoch: 186, Train_loss: 0.8079, Train_acc: 0.7446, Test_loss: 0.8741, Test_acc: 0.7225
Epoch: 191, Train_loss: 0.8040, Train_acc: 0.7455, Test_loss: 0.8702, Test_acc: 0.7246
Epoch: 196, Train_loss: 0.8038, Train_acc: 0.7461, Test_loss: 0.8725, Test_acc: 0.7226
Model saved at ./models/ft_checkpoints/20230928193632_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [0.8015342528827191, 0.7460890493381468, 0.8686642761350195, 0.7231418731252083]
model_source_only: [2.3015079867025365, 0.13298079693564516, 2.301627660669124, 0.1339851127652483]
fl_test_acc_mean 0.172
model_source_only_test_acc_mean 0.1339851127652483
model_ft_test_acc_mean 0.7231418731252083
