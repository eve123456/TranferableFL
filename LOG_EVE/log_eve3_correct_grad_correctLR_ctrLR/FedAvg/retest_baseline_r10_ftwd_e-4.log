nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 10
	           opt_lr : False
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.0
	  reg_J_norm_coef : 0.0
	          reg_max : False
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928195653
FL pretrained model will be saved at ./models/lenet_mnist_20230928195653.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 10.568% / Loss: 2.3061 /Time: 4.85s
======================================================================================================

= Test = round: 0 / acc: 10.330% / loss: 2.3071 / Time: 0.98s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.0882846862077713, avg_sq_norm_grad = 3.6597964763641357,                  max_norm_grad = 2.452883243560791, var_grad = 3.571511745452881
round 2: local lr = 0.01, sq_norm_avg_grad = 0.11578315496444702, avg_sq_norm_grad = 6.238926887512207,                  max_norm_grad = 3.219756603240967, var_grad = 6.123143672943115
round 3: local lr = 0.01, sq_norm_avg_grad = 0.1978387087583542, avg_sq_norm_grad = 13.446849822998047,                  max_norm_grad = 4.71575403213501, var_grad = 13.249011039733887
round 4: local lr = 0.01, sq_norm_avg_grad = 0.5514348745346069, avg_sq_norm_grad = 36.31397247314453,                  max_norm_grad = 7.759219169616699, var_grad = 35.76253890991211

>>> Round:    5 / Acc: 19.009% / Loss: 2.2326 /Time: 4.50s
======================================================================================================

= Test = round: 5 / acc: 19.140% / loss: 2.2292 / Time: 0.86s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 1.4364855289459229, avg_sq_norm_grad = 87.48762512207031,                  max_norm_grad = 11.975972175598145, var_grad = 86.05113983154297
round 6: local lr = 0.01, sq_norm_avg_grad = 2.1303634643554688, avg_sq_norm_grad = 154.29031372070312,                  max_norm_grad = 15.660059928894043, var_grad = 152.15994262695312
round 7: local lr = 0.01, sq_norm_avg_grad = 1.4324352741241455, avg_sq_norm_grad = 187.981201171875,                  max_norm_grad = 16.679170608520508, var_grad = 186.54876708984375
round 8: local lr = 0.01, sq_norm_avg_grad = 1.4903053045272827, avg_sq_norm_grad = 193.8425750732422,                  max_norm_grad = 17.21661376953125, var_grad = 192.35226440429688
round 9: local lr = 0.01, sq_norm_avg_grad = 1.3766030073165894, avg_sq_norm_grad = 191.6626739501953,                  max_norm_grad = 16.876890182495117, var_grad = 190.28607177734375

>>> Round:   10 / Acc: 62.756% / Loss: 1.9946 /Time: 5.70s
======================================================================================================

= Test = round: 10 / acc: 63.770% / loss: 1.9849 / Time: 0.93s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.8434, Train_acc: 0.4026, Test_loss: 1.8367, Test_acc: 0.4053
Epoch: 006, Train_loss: 1.2786, Train_acc: 0.5969, Test_loss: 1.2650, Test_acc: 0.5974
Epoch: 011, Train_loss: 1.1677, Train_acc: 0.6319, Test_loss: 1.1587, Test_acc: 0.6330
Epoch: 016, Train_loss: 1.1151, Train_acc: 0.6441, Test_loss: 1.1105, Test_acc: 0.6452
Epoch: 021, Train_loss: 1.0854, Train_acc: 0.6523, Test_loss: 1.0863, Test_acc: 0.6526
Epoch: 026, Train_loss: 1.0396, Train_acc: 0.6709, Test_loss: 1.0444, Test_acc: 0.6664
Epoch: 031, Train_loss: 1.0049, Train_acc: 0.6833, Test_loss: 1.0115, Test_acc: 0.6811
Epoch: 036, Train_loss: 0.9903, Train_acc: 0.6864, Test_loss: 0.9967, Test_acc: 0.6866
Epoch: 041, Train_loss: 0.9788, Train_acc: 0.6890, Test_loss: 0.9891, Test_acc: 0.6859
Epoch: 046, Train_loss: 0.9533, Train_acc: 0.6998, Test_loss: 0.9597, Test_acc: 0.6983
Epoch: 051, Train_loss: 0.9445, Train_acc: 0.7020, Test_loss: 0.9533, Test_acc: 0.7014
Epoch: 056, Train_loss: 0.9487, Train_acc: 0.6977, Test_loss: 0.9608, Test_acc: 0.6961
Epoch: 061, Train_loss: 0.9173, Train_acc: 0.7099, Test_loss: 0.9295, Test_acc: 0.7056
Epoch: 066, Train_loss: 0.9174, Train_acc: 0.7099, Test_loss: 0.9341, Test_acc: 0.7057
Epoch: 071, Train_loss: 0.9217, Train_acc: 0.7057, Test_loss: 0.9417, Test_acc: 0.7014
Epoch: 076, Train_loss: 0.8949, Train_acc: 0.7155, Test_loss: 0.9161, Test_acc: 0.7093
Epoch: 081, Train_loss: 0.8909, Train_acc: 0.7169, Test_loss: 0.9146, Test_acc: 0.7091
Epoch: 086, Train_loss: 0.8796, Train_acc: 0.7221, Test_loss: 0.9055, Test_acc: 0.7090
Epoch: 091, Train_loss: 0.8934, Train_acc: 0.7162, Test_loss: 0.9191, Test_acc: 0.7050
Epoch: 096, Train_loss: 0.8859, Train_acc: 0.7155, Test_loss: 0.9112, Test_acc: 0.7100
Epoch: 101, Train_loss: 0.8793, Train_acc: 0.7190, Test_loss: 0.9052, Test_acc: 0.7108
Epoch: 106, Train_loss: 0.8900, Train_acc: 0.7137, Test_loss: 0.9203, Test_acc: 0.7060
Epoch: 111, Train_loss: 0.8727, Train_acc: 0.7207, Test_loss: 0.9030, Test_acc: 0.7135
Epoch: 116, Train_loss: 0.8623, Train_acc: 0.7229, Test_loss: 0.8964, Test_acc: 0.7160
Epoch: 121, Train_loss: 0.8577, Train_acc: 0.7254, Test_loss: 0.8887, Test_acc: 0.7154
Epoch: 126, Train_loss: 0.8808, Train_acc: 0.7161, Test_loss: 0.9200, Test_acc: 0.7057
Epoch: 131, Train_loss: 0.8507, Train_acc: 0.7285, Test_loss: 0.8855, Test_acc: 0.7179
Epoch: 136, Train_loss: 0.8359, Train_acc: 0.7324, Test_loss: 0.8691, Test_acc: 0.7227
Epoch: 141, Train_loss: 0.8460, Train_acc: 0.7271, Test_loss: 0.8816, Test_acc: 0.7216
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20230928195653_model_ft.pt.
Model saved at ./models/ft_checkpoints/20230928195653_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [0.8275912469937533, 0.7362248097489873, 0.8644700296228958, 0.726141539828908]
model_source_only: [2.2898609585560865, 0.23386044304333825, 2.2887413174347695, 0.23164092878569048]
fl_test_acc_mean 0.5934
model_source_only_test_acc_mean 0.23164092878569048
model_ft_test_acc_mean 0.726141539828908
