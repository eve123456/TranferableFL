nohup: ignoring input
Using device: cuda:0
>>> Arguments:
	               algo : fedavgtl
	              alpha : 13.2316427
	         batch_size : 64
	  clients_per_round : 100
	            dataset : mnist_all_data_1_equal_niid
	             device : cuda:0
	                dis : 
	     early_stopping : 10
	         eval_every : 5
	      ft_batch_size : 128
	         ft_dataset : mnist-m
	          ft_epochs : 200
	              ft_lr : 0.001
	              ft_wd : 0.0
	                gpu : True
	        input_shape : (1, 28, 28)
	             last_k : 1
	                 lr : 0.01
	              model : lenet
	             n_init : 10
	          noaverage : False
	               noft : False
	            noprint : False
	          num_class : 10
	          num_epoch : 1
	          num_round : 200
	             opt_lr : False
	pretrain_model_path : ./models/lenet_mnist_20230928195653.pt
	              reg_J : True
	         reg_J_coef : 0.01
	             repeat : 1
	               seed : 0
	                 wd : 0.0
load pretrained model from ./models/lenet_mnist_20230928195653.pt

************************************************************************************************************************

uid: 20231001164106
>>> Training model_ft
>>> Evaluating model_source_only
model_source_only: [2.289860950098432, 0.23386044304333825, 2.2887413174347695, 0.23164092878569048]
model_source_only_test_acc_mean 0.23164092878569048
>>> Training model_ft
Epoch: 001, Train_loss: 1.8399, Train_acc: 0.3994, Test_loss: 1.8316, Test_acc: 0.3983
Epoch: 006, Train_loss: 1.2726, Train_acc: 0.5969, Test_loss: 1.2585, Test_acc: 0.5989
Epoch: 011, Train_loss: 1.1538, Train_acc: 0.6318, Test_loss: 1.1480, Test_acc: 0.6316
Epoch: 016, Train_loss: 1.1110, Train_acc: 0.6460, Test_loss: 1.1012, Test_acc: 0.6524
Epoch: 021, Train_loss: 1.0461, Train_acc: 0.6702, Test_loss: 1.0499, Test_acc: 0.6693
Epoch: 026, Train_loss: 1.0396, Train_acc: 0.6713, Test_loss: 1.0421, Test_acc: 0.6714
Epoch: 031, Train_loss: 0.9903, Train_acc: 0.6854, Test_loss: 0.9979, Test_acc: 0.6851
Epoch: 036, Train_loss: 0.9730, Train_acc: 0.6904, Test_loss: 0.9827, Test_acc: 0.6921
Epoch: 041, Train_loss: 0.9717, Train_acc: 0.6898, Test_loss: 0.9904, Test_acc: 0.6861
Epoch: 046, Train_loss: 0.9540, Train_acc: 0.6958, Test_loss: 0.9690, Test_acc: 0.6950
Epoch: 051, Train_loss: 0.9346, Train_acc: 0.7024, Test_loss: 0.9520, Test_acc: 0.7004
Epoch: 056, Train_loss: 0.9168, Train_acc: 0.7093, Test_loss: 0.9361, Test_acc: 0.7061
Epoch: 061, Train_loss: 0.9137, Train_acc: 0.7108, Test_loss: 0.9373, Test_acc: 0.7044
Epoch: 066, Train_loss: 0.9378, Train_acc: 0.6984, Test_loss: 0.9614, Test_acc: 0.6941
Epoch: 071, Train_loss: 0.9266, Train_acc: 0.6987, Test_loss: 0.9525, Test_acc: 0.6976
Epoch: 076, Train_loss: 0.8912, Train_acc: 0.7147, Test_loss: 0.9193, Test_acc: 0.7094
Epoch: 081, Train_loss: 0.8720, Train_acc: 0.7218, Test_loss: 0.9073, Test_acc: 0.7137
Epoch: 086, Train_loss: 0.8851, Train_acc: 0.7148, Test_loss: 0.9219, Test_acc: 0.7079
Epoch: 091, Train_loss: 0.8866, Train_acc: 0.7158, Test_loss: 0.9203, Test_acc: 0.7079
Epoch: 096, Train_loss: 0.8611, Train_acc: 0.7252, Test_loss: 0.9017, Test_acc: 0.7176
Epoch: 101, Train_loss: 0.8484, Train_acc: 0.7305, Test_loss: 0.8907, Test_acc: 0.7207
Epoch: 106, Train_loss: 0.8404, Train_acc: 0.7305, Test_loss: 0.8856, Test_acc: 0.7223
Epoch: 111, Train_loss: 0.8516, Train_acc: 0.7257, Test_loss: 0.8958, Test_acc: 0.7178
Epoch: 116, Train_loss: 0.8630, Train_acc: 0.7185, Test_loss: 0.9162, Test_acc: 0.7111
Epoch: 121, Train_loss: 0.8373, Train_acc: 0.7305, Test_loss: 0.8881, Test_acc: 0.7186
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001164106_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001164106_model_ft.pt.
>>> Fine-tuning done!
model_ft: [0.8363361940747837, 0.7322926730055422, 0.8872639731544479, 0.7195867125874903]
model_ft_test_acc_mean 0.7195867125874903
