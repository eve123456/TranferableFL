nohup: ignoring input
working!
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.01
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 20
	           opt_lr : False
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.0
	  reg_J_norm_coef : 0.0
	          reg_max : False
	           repeat : 5
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20231001224252
FL pretrained model will be saved at ./models/lenet_mnist_20231001224252.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.339% / Loss: 2.3076 /Time: 4.22s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3073 / Time: 0.79s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.04649993032217026, avg_sq_norm_grad = 2.374821186065674,                  max_norm_grad = 1.7651070356369019, var_grad = 2.3283212184906006
round 2: local lr = 0.01, sq_norm_avg_grad = 0.06179196760058403, avg_sq_norm_grad = 3.3171966075897217,                  max_norm_grad = 2.0954697132110596, var_grad = 3.2554047107696533
round 3: local lr = 0.01, sq_norm_avg_grad = 0.0882161483168602, avg_sq_norm_grad = 5.347885608673096,                  max_norm_grad = 2.6725687980651855, var_grad = 5.259669303894043
round 4: local lr = 0.01, sq_norm_avg_grad = 0.1690225899219513, avg_sq_norm_grad = 10.501168251037598,                  max_norm_grad = 3.7638168334960938, var_grad = 10.332145690917969

>>> Round:    5 / Acc: 11.801% / Loss: 2.2820 /Time: 4.30s
======================================================================================================

= Test = round: 5 / acc: 11.630% / loss: 2.2817 / Time: 0.82s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.4216349720954895, avg_sq_norm_grad = 25.764257431030273,                  max_norm_grad = 6.043346405029297, var_grad = 25.342622756958008
round 6: local lr = 0.01, sq_norm_avg_grad = 1.0433731079101562, avg_sq_norm_grad = 64.17333221435547,                  max_norm_grad = 9.608203887939453, var_grad = 63.12995910644531
round 7: local lr = 0.01, sq_norm_avg_grad = 1.4101568460464478, avg_sq_norm_grad = 127.26390075683594,                  max_norm_grad = 13.441803932189941, var_grad = 125.85374450683594
round 8: local lr = 0.01, sq_norm_avg_grad = 1.0906894207000732, avg_sq_norm_grad = 185.3712615966797,                  max_norm_grad = 16.000354766845703, var_grad = 184.28057861328125
round 9: local lr = 0.01, sq_norm_avg_grad = 0.7979417443275452, avg_sq_norm_grad = 193.5194549560547,                  max_norm_grad = 16.147212982177734, var_grad = 192.7215118408203

>>> Round:   10 / Acc: 44.292% / Loss: 2.1107 /Time: 4.89s
======================================================================================================

= Test = round: 10 / acc: 44.910% / loss: 2.1041 / Time: 0.83s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.8371949195861816, avg_sq_norm_grad = 197.6552734375,                  max_norm_grad = 16.2427978515625, var_grad = 196.81808471679688
round 11: local lr = 0.01, sq_norm_avg_grad = 0.897293746471405, avg_sq_norm_grad = 197.4879608154297,                  max_norm_grad = 16.35031509399414, var_grad = 196.59066772460938
round 12: local lr = 0.01, sq_norm_avg_grad = 0.97254478931427, avg_sq_norm_grad = 197.66896057128906,                  max_norm_grad = 16.202070236206055, var_grad = 196.6964111328125
round 13: local lr = 0.01, sq_norm_avg_grad = 1.2107449769973755, avg_sq_norm_grad = 201.35609436035156,                  max_norm_grad = 16.462928771972656, var_grad = 200.14535522460938
round 14: local lr = 0.01, sq_norm_avg_grad = 1.3022950887680054, avg_sq_norm_grad = 200.43638610839844,                  max_norm_grad = 16.41275978088379, var_grad = 199.13409423828125

>>> Round:   15 / Acc: 60.244% / Loss: 1.8784 /Time: 4.26s
======================================================================================================

= Test = round: 15 / acc: 62.080% / loss: 1.8602 / Time: 0.82s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.3011105060577393, avg_sq_norm_grad = 199.05398559570312,                  max_norm_grad = 16.390409469604492, var_grad = 197.75286865234375
round 16: local lr = 0.01, sq_norm_avg_grad = 1.4951093196868896, avg_sq_norm_grad = 201.0573272705078,                  max_norm_grad = 16.408071517944336, var_grad = 199.56222534179688
round 17: local lr = 0.01, sq_norm_avg_grad = 1.7607269287109375, avg_sq_norm_grad = 200.81753540039062,                  max_norm_grad = 16.39895248413086, var_grad = 199.0568084716797
round 18: local lr = 0.01, sq_norm_avg_grad = 1.941857099533081, avg_sq_norm_grad = 203.67535400390625,                  max_norm_grad = 16.492109298706055, var_grad = 201.73348999023438
round 19: local lr = 0.01, sq_norm_avg_grad = 1.8539797067642212, avg_sq_norm_grad = 205.03585815429688,                  max_norm_grad = 16.824466705322266, var_grad = 203.181884765625

>>> Round:   20 / Acc: 63.996% / Loss: 1.5757 /Time: 4.42s
======================================================================================================

= Test = round: 20 / acc: 66.040% / loss: 1.5441 / Time: 0.83s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.7622, Train_acc: 0.4223, Test_loss: 1.7533, Test_acc: 0.4265
Epoch: 006, Train_loss: 1.4440, Train_acc: 0.5612, Test_loss: 1.4247, Test_acc: 0.5708
Epoch: 011, Train_loss: 1.4070, Train_acc: 0.5669, Test_loss: 1.3862, Test_acc: 0.5788
Epoch: 016, Train_loss: 1.3878, Train_acc: 0.5897, Test_loss: 1.3702, Test_acc: 0.5989
Epoch: 021, Train_loss: 1.3821, Train_acc: 0.5883, Test_loss: 1.3607, Test_acc: 0.6013
Epoch: 026, Train_loss: 1.3802, Train_acc: 0.5881, Test_loss: 1.3648, Test_acc: 0.5945
Epoch: 031, Train_loss: 1.3769, Train_acc: 0.5914, Test_loss: 1.3612, Test_acc: 0.5982
Epoch: 036, Train_loss: 1.3754, Train_acc: 0.5962, Test_loss: 1.3551, Test_acc: 0.6005
Epoch: 041, Train_loss: 1.3697, Train_acc: 0.5991, Test_loss: 1.3495, Test_acc: 0.6095
Epoch: 046, Train_loss: 1.3776, Train_acc: 0.5925, Test_loss: 1.3579, Test_acc: 0.6058
Epoch: 051, Train_loss: 1.3693, Train_acc: 0.5960, Test_loss: 1.3488, Test_acc: 0.6076
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001224252_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001224252_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.364619837706599, 0.6010576091930645, 1.3467760311340096, 0.6070436618153539]
model_source_only: [2.3042743871443188, 0.2875374993644175, 2.3015704552023535, 0.2904121764248417]

************************************************************************************************************************

uid: 20231001225905
FL pretrained model will be saved at ./models/lenet_mnist_20231001225905.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.339% / Loss: 2.3076 /Time: 4.99s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3073 / Time: 1.13s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.0464748777449131, avg_sq_norm_grad = 2.37581205368042,                  max_norm_grad = 1.7662819623947144, var_grad = 2.3293371200561523
round 2: local lr = 0.01, sq_norm_avg_grad = 0.06163004785776138, avg_sq_norm_grad = 3.318070650100708,                  max_norm_grad = 2.095278024673462, var_grad = 3.2564406394958496
round 3: local lr = 0.01, sq_norm_avg_grad = 0.0879354178905487, avg_sq_norm_grad = 5.353631019592285,                  max_norm_grad = 2.674698829650879, var_grad = 5.265695571899414
round 4: local lr = 0.01, sq_norm_avg_grad = 0.16766570508480072, avg_sq_norm_grad = 10.504544258117676,                  max_norm_grad = 3.7659049034118652, var_grad = 10.336878776550293

>>> Round:    5 / Acc: 11.659% / Loss: 2.2815 /Time: 5.02s
======================================================================================================

= Test = round: 5 / acc: 11.530% / loss: 2.2811 / Time: 0.96s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.41113898158073425, avg_sq_norm_grad = 25.656173706054688,                  max_norm_grad = 6.041937828063965, var_grad = 25.24503517150879
round 6: local lr = 0.01, sq_norm_avg_grad = 1.020595669746399, avg_sq_norm_grad = 64.14688110351562,                  max_norm_grad = 9.607892036437988, var_grad = 63.126285552978516
round 7: local lr = 0.01, sq_norm_avg_grad = 1.4035714864730835, avg_sq_norm_grad = 127.06710815429688,                  max_norm_grad = 13.476277351379395, var_grad = 125.66353607177734
round 8: local lr = 0.01, sq_norm_avg_grad = 1.1319795846939087, avg_sq_norm_grad = 180.9980010986328,                  max_norm_grad = 15.964573860168457, var_grad = 179.86602783203125
round 9: local lr = 0.01, sq_norm_avg_grad = 0.7116787433624268, avg_sq_norm_grad = 193.33770751953125,                  max_norm_grad = 16.231536865234375, var_grad = 192.6260223388672

>>> Round:   10 / Acc: 44.587% / Loss: 2.1134 /Time: 5.32s
======================================================================================================

= Test = round: 10 / acc: 44.700% / loss: 2.1051 / Time: 1.09s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 1.0463383197784424, avg_sq_norm_grad = 194.35519409179688,                  max_norm_grad = 16.173728942871094, var_grad = 193.30885314941406
round 11: local lr = 0.01, sq_norm_avg_grad = 0.865959107875824, avg_sq_norm_grad = 198.9271240234375,                  max_norm_grad = 16.409549713134766, var_grad = 198.06117248535156
round 12: local lr = 0.01, sq_norm_avg_grad = 1.0101042985916138, avg_sq_norm_grad = 196.03720092773438,                  max_norm_grad = 16.16054916381836, var_grad = 195.027099609375
round 13: local lr = 0.01, sq_norm_avg_grad = 1.196186900138855, avg_sq_norm_grad = 196.3114471435547,                  max_norm_grad = 16.32866096496582, var_grad = 195.11526489257812
round 14: local lr = 0.01, sq_norm_avg_grad = 1.2644630670547485, avg_sq_norm_grad = 197.64755249023438,                  max_norm_grad = 16.421672821044922, var_grad = 196.38308715820312

>>> Round:   15 / Acc: 58.094% / Loss: 1.8793 /Time: 5.51s
======================================================================================================

= Test = round: 15 / acc: 59.930% / loss: 1.8609 / Time: 1.24s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.4638136625289917, avg_sq_norm_grad = 198.3964080810547,                  max_norm_grad = 16.28929901123047, var_grad = 196.93260192871094
round 16: local lr = 0.01, sq_norm_avg_grad = 1.556115746498108, avg_sq_norm_grad = 199.9864044189453,                  max_norm_grad = 16.410112380981445, var_grad = 198.43028259277344
round 17: local lr = 0.01, sq_norm_avg_grad = 1.6777054071426392, avg_sq_norm_grad = 202.25941467285156,                  max_norm_grad = 16.46946144104004, var_grad = 200.5817108154297
round 18: local lr = 0.01, sq_norm_avg_grad = 1.796555757522583, avg_sq_norm_grad = 201.9754180908203,                  max_norm_grad = 16.695650100708008, var_grad = 200.17886352539062
round 19: local lr = 0.01, sq_norm_avg_grad = 1.9737924337387085, avg_sq_norm_grad = 202.5551300048828,                  max_norm_grad = 16.917770385742188, var_grad = 200.5813446044922

>>> Round:   20 / Acc: 64.450% / Loss: 1.5756 /Time: 5.68s
======================================================================================================

= Test = round: 20 / acc: 66.590% / loss: 1.5433 / Time: 1.09s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.7462, Train_acc: 0.4257, Test_loss: 1.7335, Test_acc: 0.4355
Epoch: 006, Train_loss: 1.4396, Train_acc: 0.5504, Test_loss: 1.4209, Test_acc: 0.5626
Epoch: 011, Train_loss: 1.3976, Train_acc: 0.5828, Test_loss: 1.3777, Test_acc: 0.5969
Epoch: 016, Train_loss: 1.3989, Train_acc: 0.5879, Test_loss: 1.3807, Test_acc: 0.5916
Epoch: 021, Train_loss: 1.3759, Train_acc: 0.5892, Test_loss: 1.3571, Test_acc: 0.6018
Epoch: 026, Train_loss: 1.3934, Train_acc: 0.5799, Test_loss: 1.3721, Test_acc: 0.5915
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001225905_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001225905_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.375876315728953, 0.5892273012321825, 1.357124213006361, 0.6018220197755805]
model_source_only: [2.3045657641952197, 0.28792732326570736, 2.3018961776271025, 0.2904121764248417]

************************************************************************************************************************

uid: 20231001231429
FL pretrained model will be saved at ./models/lenet_mnist_20231001231429.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.339% / Loss: 2.3076 /Time: 5.48s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3073 / Time: 0.93s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.046592939645051956, avg_sq_norm_grad = 2.3782854080200195,                  max_norm_grad = 1.7668617963790894, var_grad = 2.3316924571990967
round 2: local lr = 0.01, sq_norm_avg_grad = 0.06174891069531441, avg_sq_norm_grad = 3.316060781478882,                  max_norm_grad = 2.095289707183838, var_grad = 3.2543118000030518
round 3: local lr = 0.01, sq_norm_avg_grad = 0.08815032243728638, avg_sq_norm_grad = 5.351396560668945,                  max_norm_grad = 2.6742208003997803, var_grad = 5.263246059417725
round 4: local lr = 0.01, sq_norm_avg_grad = 0.17083916068077087, avg_sq_norm_grad = 10.54221248626709,                  max_norm_grad = 3.7646594047546387, var_grad = 10.371373176574707

>>> Round:    5 / Acc: 11.751% / Loss: 2.2819 /Time: 4.51s
======================================================================================================

= Test = round: 5 / acc: 11.590% / loss: 2.2815 / Time: 0.88s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.42117801308631897, avg_sq_norm_grad = 25.791683197021484,                  max_norm_grad = 6.047359466552734, var_grad = 25.37050437927246
round 6: local lr = 0.01, sq_norm_avg_grad = 1.0513980388641357, avg_sq_norm_grad = 64.52753448486328,                  max_norm_grad = 9.635565757751465, var_grad = 63.47613525390625
round 7: local lr = 0.01, sq_norm_avg_grad = 1.3396046161651611, avg_sq_norm_grad = 127.25238037109375,                  max_norm_grad = 13.466809272766113, var_grad = 125.91277313232422
round 8: local lr = 0.01, sq_norm_avg_grad = 0.9184821248054504, avg_sq_norm_grad = 183.323974609375,                  max_norm_grad = 15.78502368927002, var_grad = 182.40548706054688
round 9: local lr = 0.01, sq_norm_avg_grad = 0.8086941242218018, avg_sq_norm_grad = 191.4397735595703,                  max_norm_grad = 16.00826644897461, var_grad = 190.63107299804688

>>> Round:   10 / Acc: 45.854% / Loss: 2.1120 /Time: 5.32s
======================================================================================================

= Test = round: 10 / acc: 46.140% / loss: 2.1044 / Time: 1.03s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.845160722732544, avg_sq_norm_grad = 194.75711059570312,                  max_norm_grad = 16.151975631713867, var_grad = 193.91195678710938
round 11: local lr = 0.01, sq_norm_avg_grad = 1.0072393417358398, avg_sq_norm_grad = 199.92710876464844,                  max_norm_grad = 16.360944747924805, var_grad = 198.9198760986328
round 12: local lr = 0.01, sq_norm_avg_grad = 1.2994227409362793, avg_sq_norm_grad = 196.7714080810547,                  max_norm_grad = 16.34553337097168, var_grad = 195.47198486328125
round 13: local lr = 0.01, sq_norm_avg_grad = 1.2383368015289307, avg_sq_norm_grad = 197.31515502929688,                  max_norm_grad = 16.3979434967041, var_grad = 196.07681274414062
round 14: local lr = 0.01, sq_norm_avg_grad = 1.1784274578094482, avg_sq_norm_grad = 196.08602905273438,                  max_norm_grad = 16.369300842285156, var_grad = 194.90760803222656

>>> Round:   15 / Acc: 57.430% / Loss: 1.8804 /Time: 5.43s
======================================================================================================

= Test = round: 15 / acc: 59.120% / loss: 1.8621 / Time: 0.97s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.4562700986862183, avg_sq_norm_grad = 200.59800720214844,                  max_norm_grad = 16.532976150512695, var_grad = 199.14173889160156
round 16: local lr = 0.01, sq_norm_avg_grad = 1.5899912118911743, avg_sq_norm_grad = 198.25108337402344,                  max_norm_grad = 16.19191551208496, var_grad = 196.6610870361328
round 17: local lr = 0.01, sq_norm_avg_grad = 1.7228277921676636, avg_sq_norm_grad = 204.5514678955078,                  max_norm_grad = 16.859569549560547, var_grad = 202.82864379882812
round 18: local lr = 0.01, sq_norm_avg_grad = 1.8318283557891846, avg_sq_norm_grad = 206.32630920410156,                  max_norm_grad = 16.98058319091797, var_grad = 204.49447631835938
round 19: local lr = 0.01, sq_norm_avg_grad = 1.9545292854309082, avg_sq_norm_grad = 208.2468719482422,                  max_norm_grad = 16.997861862182617, var_grad = 206.29234313964844

>>> Round:   20 / Acc: 64.284% / Loss: 1.5788 /Time: 5.86s
======================================================================================================

= Test = round: 20 / acc: 66.530% / loss: 1.5477 / Time: 1.26s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.7686, Train_acc: 0.4271, Test_loss: 1.7603, Test_acc: 0.4303
Epoch: 006, Train_loss: 1.4404, Train_acc: 0.5710, Test_loss: 1.4213, Test_acc: 0.5797
Epoch: 011, Train_loss: 1.4056, Train_acc: 0.5640, Test_loss: 1.3851, Test_acc: 0.5787
Epoch: 016, Train_loss: 1.3869, Train_acc: 0.5888, Test_loss: 1.3692, Test_acc: 0.5995
Epoch: 021, Train_loss: 1.3826, Train_acc: 0.5898, Test_loss: 1.3659, Test_acc: 0.5960
Epoch: 026, Train_loss: 1.3776, Train_acc: 0.5773, Test_loss: 1.3588, Test_acc: 0.5872
Epoch: 031, Train_loss: 1.3701, Train_acc: 0.5954, Test_loss: 1.3493, Test_acc: 0.6074
Epoch: 036, Train_loss: 1.3656, Train_acc: 0.5955, Test_loss: 1.3471, Test_acc: 0.6037
Epoch: 041, Train_loss: 1.3746, Train_acc: 0.5931, Test_loss: 1.3519, Test_acc: 0.6058
Epoch: 046, Train_loss: 1.3673, Train_acc: 0.5959, Test_loss: 1.3480, Test_acc: 0.6025
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001231429_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001231429_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.3624955107180927, 0.5988203589769665, 1.3447456504090496, 0.6088212420842128]
model_source_only: [2.305648772106868, 0.2872832663853155, 2.3030757145965883, 0.2886345961559827]

************************************************************************************************************************

uid: 20231001234059
FL pretrained model will be saved at ./models/lenet_mnist_20231001234059.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.339% / Loss: 2.3076 /Time: 5.30s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3073 / Time: 0.94s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.04646097123622894, avg_sq_norm_grad = 2.375552177429199,                  max_norm_grad = 1.7644977569580078, var_grad = 2.3290913105010986
round 2: local lr = 0.01, sq_norm_avg_grad = 0.0615992471575737, avg_sq_norm_grad = 3.3162927627563477,                  max_norm_grad = 2.094104766845703, var_grad = 3.2546935081481934
round 3: local lr = 0.01, sq_norm_avg_grad = 0.08783328533172607, avg_sq_norm_grad = 5.337522983551025,                  max_norm_grad = 2.6703484058380127, var_grad = 5.24968957901001
round 4: local lr = 0.01, sq_norm_avg_grad = 0.16824382543563843, avg_sq_norm_grad = 10.512980461120605,                  max_norm_grad = 3.7609171867370605, var_grad = 10.34473705291748

>>> Round:    5 / Acc: 11.836% / Loss: 2.2815 /Time: 5.35s
======================================================================================================

= Test = round: 5 / acc: 11.660% / loss: 2.2812 / Time: 1.32s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.4105474054813385, avg_sq_norm_grad = 25.729957580566406,                  max_norm_grad = 6.042769908905029, var_grad = 25.31941032409668
round 6: local lr = 0.01, sq_norm_avg_grad = 1.0346170663833618, avg_sq_norm_grad = 64.43888854980469,                  max_norm_grad = 9.624683380126953, var_grad = 63.40427017211914
round 7: local lr = 0.01, sq_norm_avg_grad = 1.4203104972839355, avg_sq_norm_grad = 127.96221160888672,                  max_norm_grad = 13.485386848449707, var_grad = 126.54190063476562
round 8: local lr = 0.01, sq_norm_avg_grad = 0.8383256793022156, avg_sq_norm_grad = 184.90614318847656,                  max_norm_grad = 15.870070457458496, var_grad = 184.06781005859375
round 9: local lr = 0.01, sq_norm_avg_grad = 0.8414169549942017, avg_sq_norm_grad = 194.18223571777344,                  max_norm_grad = 16.068769454956055, var_grad = 193.3408203125

>>> Round:   10 / Acc: 45.339% / Loss: 2.1136 /Time: 4.86s
======================================================================================================

= Test = round: 10 / acc: 46.090% / loss: 2.1051 / Time: 0.98s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.9403526186943054, avg_sq_norm_grad = 194.47483825683594,                  max_norm_grad = 16.33858299255371, var_grad = 193.53448486328125
round 11: local lr = 0.01, sq_norm_avg_grad = 0.9390625953674316, avg_sq_norm_grad = 195.6656036376953,                  max_norm_grad = 16.117027282714844, var_grad = 194.72654724121094
round 12: local lr = 0.01, sq_norm_avg_grad = 1.1271988153457642, avg_sq_norm_grad = 196.53114318847656,                  max_norm_grad = 16.279949188232422, var_grad = 195.40394592285156
round 13: local lr = 0.01, sq_norm_avg_grad = 1.1325597763061523, avg_sq_norm_grad = 195.9385528564453,                  max_norm_grad = 16.142715454101562, var_grad = 194.80599975585938
round 14: local lr = 0.01, sq_norm_avg_grad = 1.292797327041626, avg_sq_norm_grad = 198.6209259033203,                  max_norm_grad = 16.225067138671875, var_grad = 197.328125

>>> Round:   15 / Acc: 59.000% / Loss: 1.8799 /Time: 5.29s
======================================================================================================

= Test = round: 15 / acc: 60.810% / loss: 1.8613 / Time: 1.10s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.3527663946151733, avg_sq_norm_grad = 200.96499633789062,                  max_norm_grad = 16.664033889770508, var_grad = 199.6122283935547
round 16: local lr = 0.01, sq_norm_avg_grad = 1.6335963010787964, avg_sq_norm_grad = 204.3688507080078,                  max_norm_grad = 16.62550163269043, var_grad = 202.73526000976562
round 17: local lr = 0.01, sq_norm_avg_grad = 1.6826770305633545, avg_sq_norm_grad = 201.43345642089844,                  max_norm_grad = 16.724353790283203, var_grad = 199.7507781982422
round 18: local lr = 0.01, sq_norm_avg_grad = 1.856225848197937, avg_sq_norm_grad = 201.6000213623047,                  max_norm_grad = 16.716623306274414, var_grad = 199.74378967285156
round 19: local lr = 0.01, sq_norm_avg_grad = 2.038585662841797, avg_sq_norm_grad = 203.755615234375,                  max_norm_grad = 16.814577102661133, var_grad = 201.71702575683594

>>> Round:   20 / Acc: 64.579% / Loss: 1.5784 /Time: 5.18s
======================================================================================================

= Test = round: 20 / acc: 66.660% / loss: 1.5475 / Time: 1.01s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.7604, Train_acc: 0.4273, Test_loss: 1.7504, Test_acc: 0.4318
Epoch: 006, Train_loss: 1.4506, Train_acc: 0.5505, Test_loss: 1.4363, Test_acc: 0.5617
Epoch: 011, Train_loss: 1.4010, Train_acc: 0.5851, Test_loss: 1.3815, Test_acc: 0.6004
Epoch: 016, Train_loss: 1.4048, Train_acc: 0.5583, Test_loss: 1.3848, Test_acc: 0.5738
Epoch: 021, Train_loss: 1.3838, Train_acc: 0.5920, Test_loss: 1.3662, Test_acc: 0.6037
Epoch: 026, Train_loss: 1.3798, Train_acc: 0.5923, Test_loss: 1.3586, Test_acc: 0.6014
Epoch: 031, Train_loss: 1.3700, Train_acc: 0.5932, Test_loss: 1.3525, Test_acc: 0.6023
Epoch: 036, Train_loss: 1.3708, Train_acc: 0.5924, Test_loss: 1.3512, Test_acc: 0.6065
Epoch: 041, Train_loss: 1.3649, Train_acc: 0.5886, Test_loss: 1.3435, Test_acc: 0.6022
Epoch: 046, Train_loss: 1.3746, Train_acc: 0.5921, Test_loss: 1.3565, Test_acc: 0.6004
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001234059_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001234059_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.3648645306734293, 0.5886340909476111, 1.3434752147471345, 0.6021553160759916]
model_source_only: [2.3056635112290147, 0.2861137946814461, 2.3029777914957794, 0.2881902010887679]

************************************************************************************************************************

uid: 20231002000017
FL pretrained model will be saved at ./models/lenet_mnist_20231002000017.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.339% / Loss: 2.3076 /Time: 4.83s
======================================================================================================

= Test = round: 0 / acc: 11.350% / loss: 2.3073 / Time: 0.88s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.046575140208005905, avg_sq_norm_grad = 2.3780548572540283,                  max_norm_grad = 1.7660313844680786, var_grad = 2.331479787826538
round 2: local lr = 0.01, sq_norm_avg_grad = 0.0616900660097599, avg_sq_norm_grad = 3.3168840408325195,                  max_norm_grad = 2.09485125541687, var_grad = 3.2551939487457275
round 3: local lr = 0.01, sq_norm_avg_grad = 0.08815065771341324, avg_sq_norm_grad = 5.363248348236084,                  max_norm_grad = 2.6771185398101807, var_grad = 5.275097846984863
round 4: local lr = 0.01, sq_norm_avg_grad = 0.16882577538490295, avg_sq_norm_grad = 10.530695915222168,                  max_norm_grad = 3.761478900909424, var_grad = 10.361869812011719

>>> Round:    5 / Acc: 11.661% / Loss: 2.2821 /Time: 4.55s
======================================================================================================

= Test = round: 5 / acc: 11.520% / loss: 2.2818 / Time: 0.88s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.4197913110256195, avg_sq_norm_grad = 25.74239158630371,                  max_norm_grad = 6.0416693687438965, var_grad = 25.322599411010742
round 6: local lr = 0.01, sq_norm_avg_grad = 1.0405488014221191, avg_sq_norm_grad = 64.257080078125,                  max_norm_grad = 9.614627838134766, var_grad = 63.216529846191406
round 7: local lr = 0.01, sq_norm_avg_grad = 1.3368264436721802, avg_sq_norm_grad = 127.3260726928711,                  max_norm_grad = 13.404512405395508, var_grad = 125.98924255371094
round 8: local lr = 0.01, sq_norm_avg_grad = 0.880212664604187, avg_sq_norm_grad = 184.5930938720703,                  max_norm_grad = 16.03414535522461, var_grad = 183.71287536621094
round 9: local lr = 0.01, sq_norm_avg_grad = 0.941918134689331, avg_sq_norm_grad = 193.26683044433594,                  max_norm_grad = 16.101057052612305, var_grad = 192.3249053955078

>>> Round:   10 / Acc: 44.336% / Loss: 2.1128 /Time: 5.32s
======================================================================================================

= Test = round: 10 / acc: 44.690% / loss: 2.1049 / Time: 0.84s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 0.9174478054046631, avg_sq_norm_grad = 195.8529052734375,                  max_norm_grad = 16.328306198120117, var_grad = 194.93545532226562
round 11: local lr = 0.01, sq_norm_avg_grad = 1.118324637413025, avg_sq_norm_grad = 197.71627807617188,                  max_norm_grad = 16.13811683654785, var_grad = 196.5979461669922
round 12: local lr = 0.01, sq_norm_avg_grad = 1.0857253074645996, avg_sq_norm_grad = 196.11448669433594,                  max_norm_grad = 16.277721405029297, var_grad = 195.0287628173828
round 13: local lr = 0.01, sq_norm_avg_grad = 1.1501595973968506, avg_sq_norm_grad = 194.756591796875,                  max_norm_grad = 16.324222564697266, var_grad = 193.60643005371094
round 14: local lr = 0.01, sq_norm_avg_grad = 1.2093051671981812, avg_sq_norm_grad = 201.00909423828125,                  max_norm_grad = 16.613204956054688, var_grad = 199.79978942871094

>>> Round:   15 / Acc: 59.493% / Loss: 1.8786 /Time: 4.94s
======================================================================================================

= Test = round: 15 / acc: 61.040% / loss: 1.8610 / Time: 1.11s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.3150122165679932, avg_sq_norm_grad = 199.58355712890625,                  max_norm_grad = 16.374235153198242, var_grad = 198.26853942871094
round 16: local lr = 0.01, sq_norm_avg_grad = 1.6128880977630615, avg_sq_norm_grad = 200.6433563232422,                  max_norm_grad = 16.414295196533203, var_grad = 199.0304718017578
round 17: local lr = 0.01, sq_norm_avg_grad = 1.926095962524414, avg_sq_norm_grad = 203.32432556152344,                  max_norm_grad = 16.847700119018555, var_grad = 201.39822387695312
round 18: local lr = 0.01, sq_norm_avg_grad = 1.902025580406189, avg_sq_norm_grad = 201.39434814453125,                  max_norm_grad = 16.686552047729492, var_grad = 199.49232482910156
round 19: local lr = 0.01, sq_norm_avg_grad = 2.1066694259643555, avg_sq_norm_grad = 207.20242309570312,                  max_norm_grad = 16.935672760009766, var_grad = 205.0957489013672

>>> Round:   20 / Acc: 64.013% / Loss: 1.5811 /Time: 4.44s
======================================================================================================

= Test = round: 20 / acc: 66.400% / loss: 1.5498 / Time: 0.91s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.7526, Train_acc: 0.4392, Test_loss: 1.7419, Test_acc: 0.4491
Epoch: 006, Train_loss: 1.4373, Train_acc: 0.5646, Test_loss: 1.4200, Test_acc: 0.5730
Epoch: 011, Train_loss: 1.3977, Train_acc: 0.5856, Test_loss: 1.3763, Test_acc: 0.5966
Epoch: 016, Train_loss: 1.3955, Train_acc: 0.5769, Test_loss: 1.3796, Test_acc: 0.5842
Epoch: 021, Train_loss: 1.3973, Train_acc: 0.5768, Test_loss: 1.3760, Test_acc: 0.5869
Epoch: 026, Train_loss: 1.3789, Train_acc: 0.5875, Test_loss: 1.3584, Test_acc: 0.6006
Epoch: 031, Train_loss: 1.3723, Train_acc: 0.5884, Test_loss: 1.3538, Test_acc: 0.6010
Epoch: 036, Train_loss: 1.3768, Train_acc: 0.5880, Test_loss: 1.3597, Test_acc: 0.6007
Epoch: 041, Train_loss: 1.3838, Train_acc: 0.5814, Test_loss: 1.3631, Test_acc: 0.5929
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002000017_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002000017_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.3665847442216428, 0.6009389671361502, 1.3456822374690547, 0.6123764026219309]
model_source_only: [2.30218016031049, 0.28460534567210727, 2.2993305898642435, 0.28696811465392735]
fl_test_acc_mean 0.6523000000000001
model_source_only_test_acc_mean 0.28892345294967225
model_ft_test_acc_mean 0.6064437284746139
