nohup: ignoring input
working!
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.01
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 10
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 10
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 5
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20231002153553
FL pretrained model will be saved at ./models/lenet_mnist_20231002153553.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 7.863% / Loss: 2.2998 /Time: 4.27s
======================================================================================================

= Test = round: 0 / acc: 7.100% / loss: 2.3006 / Time: 0.83s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.017384175211191177, avg_sq_norm_grad = 1.2360204458236694,                  max_norm_grad = 1.4163206815719604, var_grad = 1.2186362743377686
round 2: local lr = 0.01, sq_norm_avg_grad = 0.019507262855768204, avg_sq_norm_grad = 1.3622846603393555,                  max_norm_grad = 1.5074422359466553, var_grad = 1.3427773714065552
round 3: local lr = 0.01, sq_norm_avg_grad = 0.02231970801949501, avg_sq_norm_grad = 1.526294469833374,                  max_norm_grad = 1.6198301315307617, var_grad = 1.5039747953414917
round 4: local lr = 0.01, sq_norm_avg_grad = 0.026159385219216347, avg_sq_norm_grad = 1.7414406538009644,                  max_norm_grad = 1.7515579462051392, var_grad = 1.7152812480926514

>>> Round:    5 / Acc: 11.865% / Loss: 2.2865 /Time: 4.13s
======================================================================================================

= Test = round: 5 / acc: 10.590% / loss: 2.2875 / Time: 0.80s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.0312315933406353, avg_sq_norm_grad = 2.034853219985962,                  max_norm_grad = 1.912495732307434, var_grad = 2.0036215782165527
round 6: local lr = 0.01, sq_norm_avg_grad = 0.03802180662751198, avg_sq_norm_grad = 2.445873737335205,                  max_norm_grad = 2.113687038421631, var_grad = 2.4078519344329834
round 7: local lr = 0.01, sq_norm_avg_grad = 0.04697439819574356, avg_sq_norm_grad = 3.0503814220428467,                  max_norm_grad = 2.371096611022949, var_grad = 3.0034070014953613
round 8: local lr = 0.01, sq_norm_avg_grad = 0.05924563854932785, avg_sq_norm_grad = 3.992044687271118,                  max_norm_grad = 2.717761278152466, var_grad = 3.9327991008758545
round 9: local lr = 0.01, sq_norm_avg_grad = 0.0784863606095314, avg_sq_norm_grad = 5.623237133026123,                  max_norm_grad = 3.2072386741638184, var_grad = 5.544750690460205

>>> Round:   10 / Acc: 20.308% / Loss: 2.2477 /Time: 4.21s
======================================================================================================

= Test = round: 10 / acc: 18.620% / loss: 2.2500 / Time: 0.84s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 2.1105, Train_acc: 0.2582, Test_loss: 2.1102, Test_acc: 0.2624
Epoch: 006, Train_loss: 1.9161, Train_acc: 0.3771, Test_loss: 1.9154, Test_acc: 0.3738
Epoch: 011, Train_loss: 1.8962, Train_acc: 0.3774, Test_loss: 1.8949, Test_acc: 0.3772
Epoch: 016, Train_loss: 1.8796, Train_acc: 0.3995, Test_loss: 1.8785, Test_acc: 0.3973
Epoch: 021, Train_loss: 1.8812, Train_acc: 0.3946, Test_loss: 1.8782, Test_acc: 0.3945
Epoch: 026, Train_loss: 1.8761, Train_acc: 0.3951, Test_loss: 1.8759, Test_acc: 0.3937
Epoch: 031, Train_loss: 1.8768, Train_acc: 0.3980, Test_loss: 1.8770, Test_acc: 0.3941
Epoch: 036, Train_loss: 1.8730, Train_acc: 0.4024, Test_loss: 1.8724, Test_acc: 0.4021
Epoch: 041, Train_loss: 1.8715, Train_acc: 0.4041, Test_loss: 1.8697, Test_acc: 0.4040
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002153553_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002153553_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.8696624792498435, 0.4064846358536296, 1.8683004313317315, 0.404066214865015]
model_source_only: [2.294881775238516, 0.12421823358926119, 2.2955538839542156, 0.12043106321519831]

************************************************************************************************************************

uid: 20231002154917
FL pretrained model will be saved at ./models/lenet_mnist_20231002154917.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 7.863% / Loss: 2.2998 /Time: 5.10s
======================================================================================================

= Test = round: 0 / acc: 7.100% / loss: 2.3006 / Time: 1.06s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.01739642582833767, avg_sq_norm_grad = 1.236229419708252,                  max_norm_grad = 1.416704773902893, var_grad = 1.2188329696655273
round 2: local lr = 0.01, sq_norm_avg_grad = 0.01952281780540943, avg_sq_norm_grad = 1.3639929294586182,                  max_norm_grad = 1.5086818933486938, var_grad = 1.3444701433181763
round 3: local lr = 0.01, sq_norm_avg_grad = 0.022360440343618393, avg_sq_norm_grad = 1.5290725231170654,                  max_norm_grad = 1.6209897994995117, var_grad = 1.5067120790481567
round 4: local lr = 0.01, sq_norm_avg_grad = 0.026207750663161278, avg_sq_norm_grad = 1.7454566955566406,                  max_norm_grad = 1.7532762289047241, var_grad = 1.71924889087677

>>> Round:    5 / Acc: 11.917% / Loss: 2.2864 /Time: 5.15s
======================================================================================================

= Test = round: 5 / acc: 10.600% / loss: 2.2874 / Time: 0.93s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.031273581087589264, avg_sq_norm_grad = 2.039647102355957,                  max_norm_grad = 1.915610432624817, var_grad = 2.008373498916626
round 6: local lr = 0.01, sq_norm_avg_grad = 0.03810633718967438, avg_sq_norm_grad = 2.4544506072998047,                  max_norm_grad = 2.1174752712249756, var_grad = 2.416344165802002
round 7: local lr = 0.01, sq_norm_avg_grad = 0.04708437994122505, avg_sq_norm_grad = 3.058152675628662,                  max_norm_grad = 2.374005079269409, var_grad = 3.011068344116211
round 8: local lr = 0.01, sq_norm_avg_grad = 0.05933922901749611, avg_sq_norm_grad = 3.9996256828308105,                  max_norm_grad = 2.720214605331421, var_grad = 3.94028639793396
round 9: local lr = 0.01, sq_norm_avg_grad = 0.07856953889131546, avg_sq_norm_grad = 5.6331787109375,                  max_norm_grad = 3.210862636566162, var_grad = 5.554609298706055

>>> Round:   10 / Acc: 20.046% / Loss: 2.2476 /Time: 5.54s
======================================================================================================

= Test = round: 10 / acc: 18.330% / loss: 2.2498 / Time: 0.97s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 2.1130, Train_acc: 0.2494, Test_loss: 2.1134, Test_acc: 0.2453
Epoch: 006, Train_loss: 1.9098, Train_acc: 0.3842, Test_loss: 1.9076, Test_acc: 0.3843
Epoch: 011, Train_loss: 1.8876, Train_acc: 0.3957, Test_loss: 1.8854, Test_acc: 0.3934
Epoch: 016, Train_loss: 1.8723, Train_acc: 0.4107, Test_loss: 1.8690, Test_acc: 0.4094
Epoch: 021, Train_loss: 1.8616, Train_acc: 0.4069, Test_loss: 1.8594, Test_acc: 0.4084
Epoch: 026, Train_loss: 1.8616, Train_acc: 0.4166, Test_loss: 1.8593, Test_acc: 0.4137
Epoch: 031, Train_loss: 1.8610, Train_acc: 0.4075, Test_loss: 1.8586, Test_acc: 0.4095
Epoch: 036, Train_loss: 1.8593, Train_acc: 0.4203, Test_loss: 1.8567, Test_acc: 0.4202
Epoch: 041, Train_loss: 1.8565, Train_acc: 0.4160, Test_loss: 1.8537, Test_acc: 0.4168
Epoch: 046, Train_loss: 1.8622, Train_acc: 0.3970, Test_loss: 1.8590, Test_acc: 0.3963
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002154917_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002154917_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.8565454798426375, 0.4159929492720462, 1.853705359424383, 0.41684257304743916]
model_source_only: [2.294782773516679, 0.12245555160082032, 2.2954543354961294, 0.11943117431396512]

************************************************************************************************************************

uid: 20231002161602
FL pretrained model will be saved at ./models/lenet_mnist_20231002161602.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 7.863% / Loss: 2.2998 /Time: 7.21s
======================================================================================================

= Test = round: 0 / acc: 7.100% / loss: 2.3006 / Time: 1.63s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.017378069460392, avg_sq_norm_grad = 1.2360812425613403,                  max_norm_grad = 1.4166018962860107, var_grad = 1.2187031507492065
round 2: local lr = 0.01, sq_norm_avg_grad = 0.019498735666275024, avg_sq_norm_grad = 1.3629634380340576,                  max_norm_grad = 1.5082981586456299, var_grad = 1.343464732170105
round 3: local lr = 0.01, sq_norm_avg_grad = 0.022350842133164406, avg_sq_norm_grad = 1.5276604890823364,                  max_norm_grad = 1.6209948062896729, var_grad = 1.5053097009658813
round 4: local lr = 0.01, sq_norm_avg_grad = 0.02617466263473034, avg_sq_norm_grad = 1.7442840337753296,                  max_norm_grad = 1.7532514333724976, var_grad = 1.718109369277954

>>> Round:    5 / Acc: 11.860% / Loss: 2.2864 /Time: 6.97s
======================================================================================================

= Test = round: 5 / acc: 10.590% / loss: 2.2874 / Time: 1.57s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.031290020793676376, avg_sq_norm_grad = 2.0402398109436035,                  max_norm_grad = 1.9170746803283691, var_grad = 2.0089497566223145
round 6: local lr = 0.01, sq_norm_avg_grad = 0.038129791617393494, avg_sq_norm_grad = 2.4557220935821533,                  max_norm_grad = 2.118298053741455, var_grad = 2.4175922870635986
round 7: local lr = 0.01, sq_norm_avg_grad = 0.04707249999046326, avg_sq_norm_grad = 3.062147378921509,                  max_norm_grad = 2.37557053565979, var_grad = 3.0150749683380127
round 8: local lr = 0.01, sq_norm_avg_grad = 0.05921545997262001, avg_sq_norm_grad = 4.000890254974365,                  max_norm_grad = 2.723428249359131, var_grad = 3.9416747093200684
round 9: local lr = 0.01, sq_norm_avg_grad = 0.07840552181005478, avg_sq_norm_grad = 5.641129016876221,                  max_norm_grad = 3.2133755683898926, var_grad = 5.562723636627197

>>> Round:   10 / Acc: 20.611% / Loss: 2.2476 /Time: 6.62s
======================================================================================================

= Test = round: 10 / acc: 18.920% / loss: 2.2499 / Time: 1.20s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 2.1123, Train_acc: 0.2642, Test_loss: 2.1118, Test_acc: 0.2637
Epoch: 006, Train_loss: 1.9245, Train_acc: 0.3810, Test_loss: 1.9232, Test_acc: 0.3751
Epoch: 011, Train_loss: 1.8911, Train_acc: 0.3913, Test_loss: 1.8911, Test_acc: 0.3897
Epoch: 016, Train_loss: 1.8842, Train_acc: 0.3975, Test_loss: 1.8818, Test_acc: 0.3946
Epoch: 021, Train_loss: 1.8788, Train_acc: 0.4088, Test_loss: 1.8780, Test_acc: 0.4120
Epoch: 026, Train_loss: 1.8726, Train_acc: 0.4085, Test_loss: 1.8710, Test_acc: 0.4085
Epoch: 031, Train_loss: 1.8721, Train_acc: 0.4059, Test_loss: 1.8706, Test_acc: 0.4043
Epoch: 036, Train_loss: 1.8704, Train_acc: 0.4105, Test_loss: 1.8692, Test_acc: 0.4106
Epoch: 041, Train_loss: 1.8685, Train_acc: 0.4069, Test_loss: 1.8669, Test_acc: 0.4048
Epoch: 046, Train_loss: 1.8685, Train_acc: 0.4012, Test_loss: 1.8674, Test_acc: 0.3975
Epoch: 051, Train_loss: 1.8690, Train_acc: 0.4078, Test_loss: 1.8673, Test_acc: 0.4068
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002161602_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002161602_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.868497303088995, 0.40116269215776, 1.8673889149614022, 0.3975113876235974]
model_source_only: [2.294803815907041, 0.12499788139184081, 2.295472844761354, 0.12143095211643151]

************************************************************************************************************************

uid: 20231002165741
FL pretrained model will be saved at ./models/lenet_mnist_20231002165741.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 7.863% / Loss: 2.2998 /Time: 6.11s
======================================================================================================

= Test = round: 0 / acc: 7.100% / loss: 2.3006 / Time: 1.19s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.017368391156196594, avg_sq_norm_grad = 1.2367595434188843,                  max_norm_grad = 1.4171395301818848, var_grad = 1.219391107559204
round 2: local lr = 0.01, sq_norm_avg_grad = 0.019489189609885216, avg_sq_norm_grad = 1.3641821146011353,                  max_norm_grad = 1.5091944932937622, var_grad = 1.3446929454803467
round 3: local lr = 0.01, sq_norm_avg_grad = 0.02230893075466156, avg_sq_norm_grad = 1.5261073112487793,                  max_norm_grad = 1.6198148727416992, var_grad = 1.5037983655929565
round 4: local lr = 0.01, sq_norm_avg_grad = 0.026148753240704536, avg_sq_norm_grad = 1.7422116994857788,                  max_norm_grad = 1.751815915107727, var_grad = 1.7160629034042358

>>> Round:    5 / Acc: 11.965% / Loss: 2.2864 /Time: 6.21s
======================================================================================================

= Test = round: 5 / acc: 10.650% / loss: 2.2874 / Time: 1.13s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.03121320530772209, avg_sq_norm_grad = 2.036237955093384,                  max_norm_grad = 1.9144054651260376, var_grad = 2.0050246715545654
round 6: local lr = 0.01, sq_norm_avg_grad = 0.03795390576124191, avg_sq_norm_grad = 2.4484808444976807,                  max_norm_grad = 2.1163740158081055, var_grad = 2.410526990890503
round 7: local lr = 0.01, sq_norm_avg_grad = 0.046896375715732574, avg_sq_norm_grad = 3.0507304668426514,                  max_norm_grad = 2.3722665309906006, var_grad = 3.0038340091705322
round 8: local lr = 0.01, sq_norm_avg_grad = 0.058982811868190765, avg_sq_norm_grad = 3.9862968921661377,                  max_norm_grad = 2.7213597297668457, var_grad = 3.927314043045044
round 9: local lr = 0.01, sq_norm_avg_grad = 0.07810754328966141, avg_sq_norm_grad = 5.603356838226318,                  max_norm_grad = 3.2071685791015625, var_grad = 5.525249481201172

>>> Round:   10 / Acc: 20.421% / Loss: 2.2476 /Time: 6.45s
======================================================================================================

= Test = round: 10 / acc: 18.730% / loss: 2.2499 / Time: 1.19s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 2.1201, Train_acc: 0.2558, Test_loss: 2.1207, Test_acc: 0.2566
Epoch: 006, Train_loss: 1.9206, Train_acc: 0.3765, Test_loss: 1.9198, Test_acc: 0.3741
Epoch: 011, Train_loss: 1.8926, Train_acc: 0.3853, Test_loss: 1.8930, Test_acc: 0.3787
Epoch: 016, Train_loss: 1.8840, Train_acc: 0.4000, Test_loss: 1.8833, Test_acc: 0.3960
Epoch: 021, Train_loss: 1.8795, Train_acc: 0.3994, Test_loss: 1.8793, Test_acc: 0.3973
Epoch: 026, Train_loss: 1.8786, Train_acc: 0.4053, Test_loss: 1.8783, Test_acc: 0.3981
Epoch: 031, Train_loss: 1.8810, Train_acc: 0.3865, Test_loss: 1.8805, Test_acc: 0.3836
Epoch: 036, Train_loss: 1.8777, Train_acc: 0.4027, Test_loss: 1.8771, Test_acc: 0.4033
Epoch: 041, Train_loss: 1.8762, Train_acc: 0.4021, Test_loss: 1.8764, Test_acc: 0.3983
Epoch: 046, Train_loss: 1.8778, Train_acc: 0.4062, Test_loss: 1.8774, Test_acc: 0.4001
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002165741_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002165741_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.8761661116772714, 0.4021118286130744, 1.8763635588041903, 0.3982890789912232]
model_source_only: [2.2947537846606543, 0.12470975068219183, 2.295435407614393, 0.12120875458282412]

************************************************************************************************************************

uid: 20231002172920
FL pretrained model will be saved at ./models/lenet_mnist_20231002172920.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 7.863% / Loss: 2.2998 /Time: 6.24s
======================================================================================================

= Test = round: 0 / acc: 7.100% / loss: 2.3006 / Time: 1.23s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.017372503876686096, avg_sq_norm_grad = 1.2355798482894897,                  max_norm_grad = 1.4164624214172363, var_grad = 1.2182073593139648
round 2: local lr = 0.01, sq_norm_avg_grad = 0.019477859139442444, avg_sq_norm_grad = 1.3627146482467651,                  max_norm_grad = 1.5071208477020264, var_grad = 1.3432368040084839
round 3: local lr = 0.01, sq_norm_avg_grad = 0.022273259237408638, avg_sq_norm_grad = 1.526618480682373,                  max_norm_grad = 1.6198877096176147, var_grad = 1.504345178604126
round 4: local lr = 0.01, sq_norm_avg_grad = 0.026097992435097694, avg_sq_norm_grad = 1.7438362836837769,                  max_norm_grad = 1.752624750137329, var_grad = 1.7177382707595825

>>> Round:    5 / Acc: 11.832% / Loss: 2.2865 /Time: 6.30s
======================================================================================================

= Test = round: 5 / acc: 10.590% / loss: 2.2875 / Time: 1.00s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.031164899468421936, avg_sq_norm_grad = 2.0388312339782715,                  max_norm_grad = 1.9147084951400757, var_grad = 2.0076663494110107
round 6: local lr = 0.01, sq_norm_avg_grad = 0.03797738254070282, avg_sq_norm_grad = 2.4510576725006104,                  max_norm_grad = 2.115200996398926, var_grad = 2.4130802154541016
round 7: local lr = 0.01, sq_norm_avg_grad = 0.046872351318597794, avg_sq_norm_grad = 3.057356595993042,                  max_norm_grad = 2.37335205078125, var_grad = 3.010484218597412
round 8: local lr = 0.01, sq_norm_avg_grad = 0.05895210802555084, avg_sq_norm_grad = 4.004358768463135,                  max_norm_grad = 2.722069501876831, var_grad = 3.945406675338745
round 9: local lr = 0.01, sq_norm_avg_grad = 0.07810141146183014, avg_sq_norm_grad = 5.638086795806885,                  max_norm_grad = 3.212644338607788, var_grad = 5.559985160827637

>>> Round:   10 / Acc: 20.535% / Loss: 2.2477 /Time: 6.26s
======================================================================================================

= Test = round: 10 / acc: 18.880% / loss: 2.2500 / Time: 1.14s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 2.1159, Train_acc: 0.2458, Test_loss: 2.1137, Test_acc: 0.2435
Epoch: 006, Train_loss: 1.9227, Train_acc: 0.3765, Test_loss: 1.9228, Test_acc: 0.3723
Epoch: 011, Train_loss: 1.8993, Train_acc: 0.3759, Test_loss: 1.8994, Test_acc: 0.3677
Epoch: 016, Train_loss: 1.8855, Train_acc: 0.3956, Test_loss: 1.8850, Test_acc: 0.3916
Epoch: 021, Train_loss: 1.8879, Train_acc: 0.3751, Test_loss: 1.8872, Test_acc: 0.3750
Epoch: 026, Train_loss: 1.8794, Train_acc: 0.4022, Test_loss: 1.8795, Test_acc: 0.3964
Epoch: 031, Train_loss: 1.8802, Train_acc: 0.3952, Test_loss: 1.8803, Test_acc: 0.3895
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002172920_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002172920_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.8790456232011377, 0.3966034473991966, 1.8786402241334215, 0.3934007332518609]
model_source_only: [2.294887174155938, 0.12499788139184081, 2.295560677175243, 0.12043106321519831]
fl_test_acc_mean 0.16286
model_source_only_test_acc_mean 0.1205866014887235
model_ft_test_acc_mean 0.4020219975558271
