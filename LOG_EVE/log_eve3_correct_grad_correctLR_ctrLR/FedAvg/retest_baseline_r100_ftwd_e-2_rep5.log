nohup: ignoring input
working!
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.01
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 100
	           opt_lr : False
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.0
	  reg_J_norm_coef : 0.0
	          reg_max : False
	           repeat : 5
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20231001224015
FL pretrained model will be saved at ./models/lenet_mnist_20231001224015.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 14.788% / Loss: 2.3056 /Time: 3.94s
======================================================================================================

= Test = round: 0 / acc: 14.820% / loss: 2.3074 / Time: 0.74s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.057485226541757584, avg_sq_norm_grad = 2.7780568599700928,                  max_norm_grad = 1.9711836576461792, var_grad = 2.720571517944336
round 2: local lr = 0.01, sq_norm_avg_grad = 0.0778258889913559, avg_sq_norm_grad = 4.134388446807861,                  max_norm_grad = 2.439711809158325, var_grad = 4.056562423706055
round 3: local lr = 0.01, sq_norm_avg_grad = 0.12579181790351868, avg_sq_norm_grad = 7.200523376464844,                  max_norm_grad = 3.2501094341278076, var_grad = 7.074731349945068
round 4: local lr = 0.01, sq_norm_avg_grad = 0.27479878067970276, avg_sq_norm_grad = 15.805578231811523,                  max_norm_grad = 4.821617603302002, var_grad = 15.530779838562012

>>> Round:    5 / Acc: 20.006% / Loss: 2.2808 /Time: 3.89s
======================================================================================================

= Test = round: 5 / acc: 20.210% / loss: 2.2825 / Time: 0.79s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.9462019801139832, avg_sq_norm_grad = 40.587852478027344,                  max_norm_grad = 7.805569171905518, var_grad = 39.64165115356445
round 6: local lr = 0.01, sq_norm_avg_grad = 2.2439565658569336, avg_sq_norm_grad = 90.7959976196289,                  max_norm_grad = 11.588948249816895, var_grad = 88.55204010009766
round 7: local lr = 0.01, sq_norm_avg_grad = 3.126729965209961, avg_sq_norm_grad = 154.7465362548828,                  max_norm_grad = 15.082111358642578, var_grad = 151.61981201171875
round 8: local lr = 0.01, sq_norm_avg_grad = 2.2244467735290527, avg_sq_norm_grad = 191.3074493408203,                  max_norm_grad = 16.429264068603516, var_grad = 189.0830078125
round 9: local lr = 0.01, sq_norm_avg_grad = 1.7222570180892944, avg_sq_norm_grad = 195.33209228515625,                  max_norm_grad = 16.416202545166016, var_grad = 193.60983276367188

>>> Round:   10 / Acc: 51.592% / Loss: 2.0493 /Time: 4.15s
======================================================================================================

= Test = round: 10 / acc: 53.440% / loss: 2.0400 / Time: 0.78s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 1.8334124088287354, avg_sq_norm_grad = 199.36924743652344,                  max_norm_grad = 16.626325607299805, var_grad = 197.53582763671875
round 11: local lr = 0.01, sq_norm_avg_grad = 1.8688451051712036, avg_sq_norm_grad = 198.44712829589844,                  max_norm_grad = 16.470991134643555, var_grad = 196.57827758789062
round 12: local lr = 0.01, sq_norm_avg_grad = 2.1161715984344482, avg_sq_norm_grad = 199.0782012939453,                  max_norm_grad = 16.64396095275879, var_grad = 196.9620361328125
round 13: local lr = 0.01, sq_norm_avg_grad = 2.4063150882720947, avg_sq_norm_grad = 203.25303649902344,                  max_norm_grad = 16.837541580200195, var_grad = 200.8467254638672
round 14: local lr = 0.01, sq_norm_avg_grad = 2.660285472869873, avg_sq_norm_grad = 201.36314392089844,                  max_norm_grad = 16.787378311157227, var_grad = 198.70286560058594

>>> Round:   15 / Acc: 61.487% / Loss: 1.7664 /Time: 4.27s
======================================================================================================

= Test = round: 15 / acc: 63.400% / loss: 1.7427 / Time: 0.81s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 2.5579512119293213, avg_sq_norm_grad = 199.09095764160156,                  max_norm_grad = 16.675308227539062, var_grad = 196.5330047607422
round 16: local lr = 0.01, sq_norm_avg_grad = 2.6614484786987305, avg_sq_norm_grad = 199.26815795898438,                  max_norm_grad = 16.673227310180664, var_grad = 196.60670471191406
round 17: local lr = 0.01, sq_norm_avg_grad = 2.5620391368865967, avg_sq_norm_grad = 194.80311584472656,                  max_norm_grad = 16.4165096282959, var_grad = 192.24107360839844
round 18: local lr = 0.01, sq_norm_avg_grad = 2.73649001121521, avg_sq_norm_grad = 195.14385986328125,                  max_norm_grad = 16.516836166381836, var_grad = 192.40736389160156
round 19: local lr = 0.01, sq_norm_avg_grad = 2.4493954181671143, avg_sq_norm_grad = 193.25254821777344,                  max_norm_grad = 16.27315330505371, var_grad = 190.8031463623047

>>> Round:   20 / Acc: 70.554% / Loss: 1.4422 /Time: 4.31s
======================================================================================================

= Test = round: 20 / acc: 72.740% / loss: 1.4063 / Time: 0.81s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 2.5897045135498047, avg_sq_norm_grad = 193.47952270507812,                  max_norm_grad = 16.20934295654297, var_grad = 190.8898162841797
round 21: local lr = 0.01, sq_norm_avg_grad = 2.6058144569396973, avg_sq_norm_grad = 190.52194213867188,                  max_norm_grad = 16.190549850463867, var_grad = 187.91612243652344
round 22: local lr = 0.01, sq_norm_avg_grad = 2.3312418460845947, avg_sq_norm_grad = 186.865966796875,                  max_norm_grad = 16.2729434967041, var_grad = 184.53472900390625
round 23: local lr = 0.01, sq_norm_avg_grad = 2.2100770473480225, avg_sq_norm_grad = 186.0516815185547,                  max_norm_grad = 16.22593116760254, var_grad = 183.8415985107422
round 24: local lr = 0.01, sq_norm_avg_grad = 2.199045181274414, avg_sq_norm_grad = 182.5731201171875,                  max_norm_grad = 16.378894805908203, var_grad = 180.3740692138672

>>> Round:   25 / Acc: 73.542% / Loss: 1.1762 /Time: 4.29s
======================================================================================================

= Test = round: 25 / acc: 75.850% / loss: 1.1354 / Time: 0.81s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 2.1399245262145996, avg_sq_norm_grad = 177.05892944335938,                  max_norm_grad = 16.347869873046875, var_grad = 174.91900634765625
round 26: local lr = 0.01, sq_norm_avg_grad = 1.8443143367767334, avg_sq_norm_grad = 176.37953186035156,                  max_norm_grad = 16.355518341064453, var_grad = 174.53521728515625
round 27: local lr = 0.01, sq_norm_avg_grad = 1.7594964504241943, avg_sq_norm_grad = 170.6994171142578,                  max_norm_grad = 16.23369789123535, var_grad = 168.93992614746094
round 28: local lr = 0.01, sq_norm_avg_grad = 1.7770140171051025, avg_sq_norm_grad = 163.13278198242188,                  max_norm_grad = 16.223957061767578, var_grad = 161.35577392578125
round 29: local lr = 0.01, sq_norm_avg_grad = 1.639525055885315, avg_sq_norm_grad = 161.9143524169922,                  max_norm_grad = 16.03350257873535, var_grad = 160.2748260498047

>>> Round:   30 / Acc: 76.238% / Loss: 0.9882 /Time: 4.91s
======================================================================================================

= Test = round: 30 / acc: 78.400% / loss: 0.9470 / Time: 0.90s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 1.6935151815414429, avg_sq_norm_grad = 160.75526428222656,                  max_norm_grad = 16.35630989074707, var_grad = 159.06175231933594
round 31: local lr = 0.01, sq_norm_avg_grad = 1.6344472169876099, avg_sq_norm_grad = 157.1437530517578,                  max_norm_grad = 16.22360610961914, var_grad = 155.50930786132812
round 32: local lr = 0.01, sq_norm_avg_grad = 1.6328562498092651, avg_sq_norm_grad = 154.91339111328125,                  max_norm_grad = 16.285911560058594, var_grad = 153.28053283691406
round 33: local lr = 0.01, sq_norm_avg_grad = 1.6292117834091187, avg_sq_norm_grad = 153.82916259765625,                  max_norm_grad = 16.474525451660156, var_grad = 152.199951171875
round 34: local lr = 0.01, sq_norm_avg_grad = 1.5541988611221313, avg_sq_norm_grad = 147.69566345214844,                  max_norm_grad = 15.830120086669922, var_grad = 146.14146423339844

>>> Round:   35 / Acc: 77.745% / Loss: 0.8625 /Time: 4.92s
======================================================================================================

= Test = round: 35 / acc: 79.970% / loss: 0.8234 / Time: 1.00s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 1.5690618753433228, avg_sq_norm_grad = 145.91258239746094,                  max_norm_grad = 16.002517700195312, var_grad = 144.34352111816406
round 36: local lr = 0.01, sq_norm_avg_grad = 1.2990162372589111, avg_sq_norm_grad = 142.40391540527344,                  max_norm_grad = 16.03426742553711, var_grad = 141.1049041748047
round 37: local lr = 0.01, sq_norm_avg_grad = 1.2849984169006348, avg_sq_norm_grad = 140.2169647216797,                  max_norm_grad = 15.846733093261719, var_grad = 138.9319610595703
round 38: local lr = 0.01, sq_norm_avg_grad = 1.3078609704971313, avg_sq_norm_grad = 139.25973510742188,                  max_norm_grad = 15.975493431091309, var_grad = 137.95187377929688
round 39: local lr = 0.01, sq_norm_avg_grad = 1.1777068376541138, avg_sq_norm_grad = 136.20980834960938,                  max_norm_grad = 15.75815486907959, var_grad = 135.0321044921875

>>> Round:   40 / Acc: 79.380% / Loss: 0.7747 /Time: 5.25s
======================================================================================================

= Test = round: 40 / acc: 81.380% / loss: 0.7380 / Time: 0.90s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 1.5705163478851318, avg_sq_norm_grad = 134.3896484375,                  max_norm_grad = 16.052621841430664, var_grad = 132.8191375732422
round 41: local lr = 0.01, sq_norm_avg_grad = 1.1560451984405518, avg_sq_norm_grad = 132.50697326660156,                  max_norm_grad = 15.797918319702148, var_grad = 131.35092163085938
round 42: local lr = 0.01, sq_norm_avg_grad = 1.1823666095733643, avg_sq_norm_grad = 132.02499389648438,                  max_norm_grad = 15.500088691711426, var_grad = 130.84262084960938
round 43: local lr = 0.01, sq_norm_avg_grad = 1.151397466659546, avg_sq_norm_grad = 128.74415588378906,                  max_norm_grad = 15.658735275268555, var_grad = 127.59275817871094
round 44: local lr = 0.01, sq_norm_avg_grad = 1.2556689977645874, avg_sq_norm_grad = 126.63954162597656,                  max_norm_grad = 15.351053237915039, var_grad = 125.38387298583984

>>> Round:   45 / Acc: 80.982% / Loss: 0.7045 /Time: 5.26s
======================================================================================================

= Test = round: 45 / acc: 82.650% / loss: 0.6692 / Time: 1.02s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 0.8579444289207458, avg_sq_norm_grad = 124.52808380126953,                  max_norm_grad = 15.122361183166504, var_grad = 123.6701431274414
round 46: local lr = 0.01, sq_norm_avg_grad = 0.9103835821151733, avg_sq_norm_grad = 123.24645233154297,                  max_norm_grad = 15.313529014587402, var_grad = 122.33606719970703
round 47: local lr = 0.01, sq_norm_avg_grad = 0.9458329081535339, avg_sq_norm_grad = 121.30302429199219,                  max_norm_grad = 14.989012718200684, var_grad = 120.35719299316406
round 48: local lr = 0.01, sq_norm_avg_grad = 1.0124531984329224, avg_sq_norm_grad = 117.52745819091797,                  max_norm_grad = 15.079559326171875, var_grad = 116.51500701904297
round 49: local lr = 0.01, sq_norm_avg_grad = 0.8899482488632202, avg_sq_norm_grad = 114.73033142089844,                  max_norm_grad = 14.460586547851562, var_grad = 113.84038543701172

>>> Round:   50 / Acc: 81.917% / Loss: 0.6546 /Time: 4.89s
======================================================================================================

= Test = round: 50 / acc: 83.510% / loss: 0.6196 / Time: 0.90s
======================================================================================================

round 50: local lr = 0.01, sq_norm_avg_grad = 0.6747230291366577, avg_sq_norm_grad = 114.0138168334961,                  max_norm_grad = 14.44010066986084, var_grad = 113.33909606933594
round 51: local lr = 0.01, sq_norm_avg_grad = 0.6461467146873474, avg_sq_norm_grad = 112.134765625,                  max_norm_grad = 14.56486701965332, var_grad = 111.48861694335938
round 52: local lr = 0.01, sq_norm_avg_grad = 0.8094444870948792, avg_sq_norm_grad = 110.47621154785156,                  max_norm_grad = 14.342114448547363, var_grad = 109.66676330566406
round 53: local lr = 0.01, sq_norm_avg_grad = 0.6627382040023804, avg_sq_norm_grad = 109.27944946289062,                  max_norm_grad = 13.997414588928223, var_grad = 108.61671447753906
round 54: local lr = 0.01, sq_norm_avg_grad = 0.7190787196159363, avg_sq_norm_grad = 105.86078643798828,                  max_norm_grad = 14.127808570861816, var_grad = 105.14170837402344

>>> Round:   55 / Acc: 82.694% / Loss: 0.6165 /Time: 5.02s
======================================================================================================

= Test = round: 55 / acc: 84.150% / loss: 0.5823 / Time: 0.99s
======================================================================================================

round 55: local lr = 0.01, sq_norm_avg_grad = 0.7238401174545288, avg_sq_norm_grad = 105.21675872802734,                  max_norm_grad = 14.0931396484375, var_grad = 104.492919921875
round 56: local lr = 0.01, sq_norm_avg_grad = 0.810855507850647, avg_sq_norm_grad = 104.85157012939453,                  max_norm_grad = 14.385077476501465, var_grad = 104.04071807861328
round 57: local lr = 0.01, sq_norm_avg_grad = 0.839672327041626, avg_sq_norm_grad = 103.43460845947266,                  max_norm_grad = 14.086810111999512, var_grad = 102.59493255615234
round 58: local lr = 0.01, sq_norm_avg_grad = 0.615010678768158, avg_sq_norm_grad = 102.01039123535156,                  max_norm_grad = 13.731815338134766, var_grad = 101.39537811279297
round 59: local lr = 0.01, sq_norm_avg_grad = 0.6408565044403076, avg_sq_norm_grad = 100.51978302001953,                  max_norm_grad = 13.998021125793457, var_grad = 99.8789291381836

>>> Round:   60 / Acc: 83.469% / Loss: 0.5820 /Time: 4.93s
======================================================================================================

= Test = round: 60 / acc: 84.900% / loss: 0.5490 / Time: 0.97s
======================================================================================================

round 60: local lr = 0.01, sq_norm_avg_grad = 0.629520058631897, avg_sq_norm_grad = 99.56985473632812,                  max_norm_grad = 13.72337818145752, var_grad = 98.94033813476562
round 61: local lr = 0.01, sq_norm_avg_grad = 0.4907039403915405, avg_sq_norm_grad = 98.19142150878906,                  max_norm_grad = 13.405344009399414, var_grad = 97.70071411132812
round 62: local lr = 0.01, sq_norm_avg_grad = 0.5788446068763733, avg_sq_norm_grad = 96.36293029785156,                  max_norm_grad = 13.373501777648926, var_grad = 95.78408813476562
round 63: local lr = 0.01, sq_norm_avg_grad = 0.4800088107585907, avg_sq_norm_grad = 94.77130889892578,                  max_norm_grad = 13.262077331542969, var_grad = 94.29129791259766
round 64: local lr = 0.01, sq_norm_avg_grad = 0.5051599740982056, avg_sq_norm_grad = 93.82865142822266,                  max_norm_grad = 13.267068862915039, var_grad = 93.32349395751953

>>> Round:   65 / Acc: 84.214% / Loss: 0.5543 /Time: 5.49s
======================================================================================================

= Test = round: 65 / acc: 85.480% / loss: 0.5214 / Time: 0.99s
======================================================================================================

round 65: local lr = 0.01, sq_norm_avg_grad = 0.49350765347480774, avg_sq_norm_grad = 92.25154113769531,                  max_norm_grad = 13.293526649475098, var_grad = 91.7580337524414
round 66: local lr = 0.01, sq_norm_avg_grad = 0.5455842018127441, avg_sq_norm_grad = 90.80811309814453,                  max_norm_grad = 13.081710815429688, var_grad = 90.26252746582031
round 67: local lr = 0.01, sq_norm_avg_grad = 0.5110338926315308, avg_sq_norm_grad = 90.38152313232422,                  max_norm_grad = 13.118782043457031, var_grad = 89.87049102783203
round 68: local lr = 0.01, sq_norm_avg_grad = 0.6260724663734436, avg_sq_norm_grad = 89.62689208984375,                  max_norm_grad = 13.268471717834473, var_grad = 89.00081634521484
round 69: local lr = 0.01, sq_norm_avg_grad = 0.40673065185546875, avg_sq_norm_grad = 88.35295104980469,                  max_norm_grad = 12.576541900634766, var_grad = 87.94622039794922

>>> Round:   70 / Acc: 84.808% / Loss: 0.5300 /Time: 5.66s
======================================================================================================

= Test = round: 70 / acc: 85.980% / loss: 0.4978 / Time: 1.01s
======================================================================================================

round 70: local lr = 0.01, sq_norm_avg_grad = 0.5051771402359009, avg_sq_norm_grad = 87.42739868164062,                  max_norm_grad = 12.884166717529297, var_grad = 86.9222183227539
round 71: local lr = 0.01, sq_norm_avg_grad = 0.610395073890686, avg_sq_norm_grad = 87.13853454589844,                  max_norm_grad = 12.73737621307373, var_grad = 86.52813720703125
round 72: local lr = 0.01, sq_norm_avg_grad = 0.5759507417678833, avg_sq_norm_grad = 86.91447448730469,                  max_norm_grad = 12.87255573272705, var_grad = 86.3385238647461
round 73: local lr = 0.01, sq_norm_avg_grad = 0.3878655731678009, avg_sq_norm_grad = 84.38341522216797,                  max_norm_grad = 12.392918586730957, var_grad = 83.99555206298828
round 74: local lr = 0.01, sq_norm_avg_grad = 0.3567482829093933, avg_sq_norm_grad = 83.72160339355469,                  max_norm_grad = 12.605547904968262, var_grad = 83.36485290527344

>>> Round:   75 / Acc: 85.345% / Loss: 0.5075 /Time: 5.60s
======================================================================================================

= Test = round: 75 / acc: 86.550% / loss: 0.4755 / Time: 0.95s
======================================================================================================

round 75: local lr = 0.01, sq_norm_avg_grad = 0.4106261730194092, avg_sq_norm_grad = 83.67997741699219,                  max_norm_grad = 12.433835983276367, var_grad = 83.26934814453125
round 76: local lr = 0.01, sq_norm_avg_grad = 0.4140909016132355, avg_sq_norm_grad = 83.45014190673828,                  max_norm_grad = 12.48758602142334, var_grad = 83.03604888916016
round 77: local lr = 0.01, sq_norm_avg_grad = 0.4117419123649597, avg_sq_norm_grad = 81.41268157958984,                  max_norm_grad = 12.598128318786621, var_grad = 81.00093841552734
round 78: local lr = 0.01, sq_norm_avg_grad = 0.3869849741458893, avg_sq_norm_grad = 80.63038635253906,                  max_norm_grad = 12.344914436340332, var_grad = 80.24340057373047
round 79: local lr = 0.01, sq_norm_avg_grad = 0.3095770478248596, avg_sq_norm_grad = 79.66600799560547,                  max_norm_grad = 11.865862846374512, var_grad = 79.35643005371094

>>> Round:   80 / Acc: 85.893% / Loss: 0.4887 /Time: 5.63s
======================================================================================================

= Test = round: 80 / acc: 87.030% / loss: 0.4570 / Time: 1.09s
======================================================================================================

round 80: local lr = 0.01, sq_norm_avg_grad = 0.3112254738807678, avg_sq_norm_grad = 78.8839111328125,                  max_norm_grad = 11.940201759338379, var_grad = 78.57268524169922
round 81: local lr = 0.01, sq_norm_avg_grad = 0.37449976801872253, avg_sq_norm_grad = 78.75142669677734,                  max_norm_grad = 12.252833366394043, var_grad = 78.3769302368164
round 82: local lr = 0.01, sq_norm_avg_grad = 0.3470909297466278, avg_sq_norm_grad = 77.41371154785156,                  max_norm_grad = 12.373668670654297, var_grad = 77.06661987304688
round 83: local lr = 0.01, sq_norm_avg_grad = 0.375301718711853, avg_sq_norm_grad = 76.83971405029297,                  max_norm_grad = 12.282364845275879, var_grad = 76.46440887451172
round 84: local lr = 0.01, sq_norm_avg_grad = 0.3217523694038391, avg_sq_norm_grad = 75.38133239746094,                  max_norm_grad = 11.849638938903809, var_grad = 75.05957794189453

>>> Round:   85 / Acc: 86.306% / Loss: 0.4719 /Time: 5.11s
======================================================================================================

= Test = round: 85 / acc: 87.390% / loss: 0.4402 / Time: 1.04s
======================================================================================================

round 85: local lr = 0.01, sq_norm_avg_grad = 0.4055121839046478, avg_sq_norm_grad = 75.71820068359375,                  max_norm_grad = 11.844002723693848, var_grad = 75.31269073486328
round 86: local lr = 0.01, sq_norm_avg_grad = 0.31690043210983276, avg_sq_norm_grad = 74.38714599609375,                  max_norm_grad = 12.014813423156738, var_grad = 74.07024383544922
round 87: local lr = 0.01, sq_norm_avg_grad = 0.3028129041194916, avg_sq_norm_grad = 73.15763854980469,                  max_norm_grad = 11.855110168457031, var_grad = 72.85482788085938
round 88: local lr = 0.01, sq_norm_avg_grad = 0.2587107717990875, avg_sq_norm_grad = 72.76878356933594,                  max_norm_grad = 11.7017183303833, var_grad = 72.51007080078125
round 89: local lr = 0.01, sq_norm_avg_grad = 0.487259179353714, avg_sq_norm_grad = 73.244873046875,                  max_norm_grad = 12.152900695800781, var_grad = 72.75761413574219

>>> Round:   90 / Acc: 86.688% / Loss: 0.4572 /Time: 4.83s
======================================================================================================

= Test = round: 90 / acc: 87.800% / loss: 0.4258 / Time: 0.88s
======================================================================================================

round 90: local lr = 0.01, sq_norm_avg_grad = 0.44537439942359924, avg_sq_norm_grad = 72.26466369628906,                  max_norm_grad = 11.9142427444458, var_grad = 71.81929016113281
round 91: local lr = 0.01, sq_norm_avg_grad = 0.36068999767303467, avg_sq_norm_grad = 72.21062469482422,                  max_norm_grad = 11.669662475585938, var_grad = 71.84993743896484
round 92: local lr = 0.01, sq_norm_avg_grad = 0.4216811954975128, avg_sq_norm_grad = 71.82186126708984,                  max_norm_grad = 11.561579704284668, var_grad = 71.40017700195312
round 93: local lr = 0.01, sq_norm_avg_grad = 0.3149465322494507, avg_sq_norm_grad = 70.88138580322266,                  max_norm_grad = 11.617959976196289, var_grad = 70.56643676757812
round 94: local lr = 0.01, sq_norm_avg_grad = 0.25677162408828735, avg_sq_norm_grad = 69.2615737915039,                  max_norm_grad = 11.358908653259277, var_grad = 69.00479888916016

>>> Round:   95 / Acc: 87.114% / Loss: 0.4427 /Time: 4.82s
======================================================================================================

= Test = round: 95 / acc: 88.260% / loss: 0.4120 / Time: 1.00s
======================================================================================================

round 95: local lr = 0.01, sq_norm_avg_grad = 0.28321322798728943, avg_sq_norm_grad = 68.07555389404297,                  max_norm_grad = 11.268280982971191, var_grad = 67.79234313964844
round 96: local lr = 0.01, sq_norm_avg_grad = 0.24620597064495087, avg_sq_norm_grad = 67.57369232177734,                  max_norm_grad = 10.90659236907959, var_grad = 67.32748413085938
round 97: local lr = 0.01, sq_norm_avg_grad = 0.34941038489341736, avg_sq_norm_grad = 68.31438446044922,                  max_norm_grad = 11.283567428588867, var_grad = 67.96497344970703
round 98: local lr = 0.01, sq_norm_avg_grad = 0.27251383662223816, avg_sq_norm_grad = 67.10386657714844,                  max_norm_grad = 11.288636207580566, var_grad = 66.83135223388672
round 99: local lr = 0.01, sq_norm_avg_grad = 0.20013561844825745, avg_sq_norm_grad = 65.55487823486328,                  max_norm_grad = 11.023934364318848, var_grad = 65.35474395751953

>>> Round:  100 / Acc: 87.448% / Loss: 0.4293 /Time: 5.30s
======================================================================================================

= Test = round: 100 / acc: 88.560% / loss: 0.3982 / Time: 0.99s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.3309, Train_acc: 0.5972, Test_loss: 1.3013, Test_acc: 0.6060
Epoch: 006, Train_loss: 1.1323, Train_acc: 0.6592, Test_loss: 1.1111, Test_acc: 0.6598
Epoch: 011, Train_loss: 1.0978, Train_acc: 0.6672, Test_loss: 1.0835, Test_acc: 0.6686
Epoch: 016, Train_loss: 1.0769, Train_acc: 0.6738, Test_loss: 1.0630, Test_acc: 0.6790
Epoch: 021, Train_loss: 1.0835, Train_acc: 0.6736, Test_loss: 1.0671, Test_acc: 0.6790
Epoch: 026, Train_loss: 1.0829, Train_acc: 0.6691, Test_loss: 1.0652, Test_acc: 0.6787
Epoch: 031, Train_loss: 1.0675, Train_acc: 0.6841, Test_loss: 1.0518, Test_acc: 0.6869
Epoch: 036, Train_loss: 1.0696, Train_acc: 0.6750, Test_loss: 1.0549, Test_acc: 0.6769
Epoch: 041, Train_loss: 1.0677, Train_acc: 0.6795, Test_loss: 1.0483, Test_acc: 0.6891
Epoch: 046, Train_loss: 1.0667, Train_acc: 0.6787, Test_loss: 1.0490, Test_acc: 0.6807
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001224015_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001224015_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.0633970338917067, 0.6823613159098998, 1.044451732865149, 0.6881457615820464]
model_source_only: [2.566278791163336, 0.40075591939119676, 2.5982423840516304, 0.3984001777580269]

************************************************************************************************************************

uid: 20231001233602
FL pretrained model will be saved at ./models/lenet_mnist_20231001233602.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 14.788% / Loss: 2.3056 /Time: 5.59s
======================================================================================================

= Test = round: 0 / acc: 14.820% / loss: 2.3074 / Time: 1.08s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.057505492120981216, avg_sq_norm_grad = 2.7807235717773438,                  max_norm_grad = 1.9712485074996948, var_grad = 2.7232179641723633
round 2: local lr = 0.01, sq_norm_avg_grad = 0.0779363214969635, avg_sq_norm_grad = 4.1356329917907715,                  max_norm_grad = 2.4393723011016846, var_grad = 4.05769681930542
round 3: local lr = 0.01, sq_norm_avg_grad = 0.1255258023738861, avg_sq_norm_grad = 7.201785087585449,                  max_norm_grad = 3.2523229122161865, var_grad = 7.076259136199951
round 4: local lr = 0.01, sq_norm_avg_grad = 0.2733001708984375, avg_sq_norm_grad = 15.798013687133789,                  max_norm_grad = 4.820320129394531, var_grad = 15.524713516235352

>>> Round:    5 / Acc: 19.577% / Loss: 2.2800 /Time: 5.21s
======================================================================================================

= Test = round: 5 / acc: 19.950% / loss: 2.2817 / Time: 1.03s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.933297336101532, avg_sq_norm_grad = 40.40365982055664,                  max_norm_grad = 7.773437976837158, var_grad = 39.47036361694336
round 6: local lr = 0.01, sq_norm_avg_grad = 2.2162671089172363, avg_sq_norm_grad = 90.74390411376953,                  max_norm_grad = 11.580459594726562, var_grad = 88.52763366699219
round 7: local lr = 0.01, sq_norm_avg_grad = 2.9674296379089355, avg_sq_norm_grad = 154.2442169189453,                  max_norm_grad = 15.007762908935547, var_grad = 151.27679443359375
round 8: local lr = 0.01, sq_norm_avg_grad = 2.2238614559173584, avg_sq_norm_grad = 188.70132446289062,                  max_norm_grad = 16.254241943359375, var_grad = 186.4774627685547
round 9: local lr = 0.01, sq_norm_avg_grad = 1.82163405418396, avg_sq_norm_grad = 196.93460083007812,                  max_norm_grad = 16.517826080322266, var_grad = 195.1129608154297

>>> Round:   10 / Acc: 46.494% / Loss: 2.0515 /Time: 5.48s
======================================================================================================

= Test = round: 10 / acc: 48.050% / loss: 2.0414 / Time: 0.97s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 1.976507544517517, avg_sq_norm_grad = 194.01097106933594,                  max_norm_grad = 16.427038192749023, var_grad = 192.0344696044922
round 11: local lr = 0.01, sq_norm_avg_grad = 1.892329454421997, avg_sq_norm_grad = 197.59866333007812,                  max_norm_grad = 16.549556732177734, var_grad = 195.70632934570312
round 12: local lr = 0.01, sq_norm_avg_grad = 1.9814584255218506, avg_sq_norm_grad = 196.446044921875,                  max_norm_grad = 16.465709686279297, var_grad = 194.46458435058594
round 13: local lr = 0.01, sq_norm_avg_grad = 2.080380916595459, avg_sq_norm_grad = 197.78329467773438,                  max_norm_grad = 16.40204429626465, var_grad = 195.70291137695312
round 14: local lr = 0.01, sq_norm_avg_grad = 2.2663042545318604, avg_sq_norm_grad = 194.740966796875,                  max_norm_grad = 16.366872787475586, var_grad = 192.4746551513672

>>> Round:   15 / Acc: 65.028% / Loss: 1.7627 /Time: 4.95s
======================================================================================================

= Test = round: 15 / acc: 67.240% / loss: 1.7384 / Time: 1.22s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 2.124413251876831, avg_sq_norm_grad = 195.3963165283203,                  max_norm_grad = 16.200902938842773, var_grad = 193.2718963623047
round 16: local lr = 0.01, sq_norm_avg_grad = 2.293114185333252, avg_sq_norm_grad = 196.83914184570312,                  max_norm_grad = 16.344850540161133, var_grad = 194.5460205078125
round 17: local lr = 0.01, sq_norm_avg_grad = 2.546967029571533, avg_sq_norm_grad = 195.7689971923828,                  max_norm_grad = 16.49431037902832, var_grad = 193.22203063964844
round 18: local lr = 0.01, sq_norm_avg_grad = 2.39418363571167, avg_sq_norm_grad = 193.5664825439453,                  max_norm_grad = 16.271442413330078, var_grad = 191.17230224609375
round 19: local lr = 0.01, sq_norm_avg_grad = 2.369560718536377, avg_sq_norm_grad = 191.93402099609375,                  max_norm_grad = 16.16870880126953, var_grad = 189.564453125

>>> Round:   20 / Acc: 70.111% / Loss: 1.4376 /Time: 5.27s
======================================================================================================

= Test = round: 20 / acc: 72.630% / loss: 1.4017 / Time: 0.96s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 2.2154970169067383, avg_sq_norm_grad = 189.2234649658203,                  max_norm_grad = 16.039453506469727, var_grad = 187.00796508789062
round 21: local lr = 0.01, sq_norm_avg_grad = 2.4893763065338135, avg_sq_norm_grad = 192.3194122314453,                  max_norm_grad = 16.374061584472656, var_grad = 189.8300323486328
round 22: local lr = 0.01, sq_norm_avg_grad = 2.4001998901367188, avg_sq_norm_grad = 189.66217041015625,                  max_norm_grad = 16.484840393066406, var_grad = 187.261962890625
round 23: local lr = 0.01, sq_norm_avg_grad = 2.160057544708252, avg_sq_norm_grad = 182.75714111328125,                  max_norm_grad = 16.374177932739258, var_grad = 180.59707641601562
round 24: local lr = 0.01, sq_norm_avg_grad = 2.176143169403076, avg_sq_norm_grad = 180.23687744140625,                  max_norm_grad = 16.40865707397461, var_grad = 178.06072998046875

>>> Round:   25 / Acc: 73.633% / Loss: 1.1719 /Time: 5.65s
======================================================================================================

= Test = round: 25 / acc: 76.050% / loss: 1.1305 / Time: 0.95s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 1.9523953199386597, avg_sq_norm_grad = 176.8814239501953,                  max_norm_grad = 16.434160232543945, var_grad = 174.9290313720703
round 26: local lr = 0.01, sq_norm_avg_grad = 1.8740277290344238, avg_sq_norm_grad = 174.65362548828125,                  max_norm_grad = 16.41091537475586, var_grad = 172.77960205078125
round 27: local lr = 0.01, sq_norm_avg_grad = 1.9406051635742188, avg_sq_norm_grad = 169.864013671875,                  max_norm_grad = 16.038509368896484, var_grad = 167.92340087890625
round 28: local lr = 0.01, sq_norm_avg_grad = 2.0015032291412354, avg_sq_norm_grad = 165.16981506347656,                  max_norm_grad = 16.335079193115234, var_grad = 163.16830444335938
round 29: local lr = 0.01, sq_norm_avg_grad = 2.0088744163513184, avg_sq_norm_grad = 164.71490478515625,                  max_norm_grad = 16.547962188720703, var_grad = 162.70602416992188

>>> Round:   30 / Acc: 76.247% / Loss: 0.9873 /Time: 5.15s
======================================================================================================

= Test = round: 30 / acc: 78.510% / loss: 0.9464 / Time: 1.23s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 1.7073439359664917, avg_sq_norm_grad = 161.03936767578125,                  max_norm_grad = 16.291847229003906, var_grad = 159.33203125
round 31: local lr = 0.01, sq_norm_avg_grad = 1.5612465143203735, avg_sq_norm_grad = 158.0985565185547,                  max_norm_grad = 16.31039047241211, var_grad = 156.5373077392578
round 32: local lr = 0.01, sq_norm_avg_grad = 1.4845079183578491, avg_sq_norm_grad = 156.27479553222656,                  max_norm_grad = 16.131546020507812, var_grad = 154.790283203125
round 33: local lr = 0.01, sq_norm_avg_grad = 1.4447730779647827, avg_sq_norm_grad = 152.369873046875,                  max_norm_grad = 16.364240646362305, var_grad = 150.9250946044922
round 34: local lr = 0.01, sq_norm_avg_grad = 1.2782353162765503, avg_sq_norm_grad = 146.07989501953125,                  max_norm_grad = 15.968990325927734, var_grad = 144.80166625976562

>>> Round:   35 / Acc: 77.934% / Loss: 0.8616 /Time: 5.42s
======================================================================================================

= Test = round: 35 / acc: 80.210% / loss: 0.8223 / Time: 0.96s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 1.425484538078308, avg_sq_norm_grad = 145.5653839111328,                  max_norm_grad = 15.946962356567383, var_grad = 144.139892578125
round 36: local lr = 0.01, sq_norm_avg_grad = 1.391568660736084, avg_sq_norm_grad = 144.05694580078125,                  max_norm_grad = 16.12057113647461, var_grad = 142.66537475585938
round 37: local lr = 0.01, sq_norm_avg_grad = 1.224382996559143, avg_sq_norm_grad = 140.38980102539062,                  max_norm_grad = 15.856233596801758, var_grad = 139.16542053222656
round 38: local lr = 0.01, sq_norm_avg_grad = 1.3873652219772339, avg_sq_norm_grad = 139.505615234375,                  max_norm_grad = 15.955608367919922, var_grad = 138.11825561523438
round 39: local lr = 0.01, sq_norm_avg_grad = 1.2567408084869385, avg_sq_norm_grad = 137.47923278808594,                  max_norm_grad = 15.7413330078125, var_grad = 136.2224884033203

>>> Round:   40 / Acc: 79.533% / Loss: 0.7703 /Time: 4.79s
======================================================================================================

= Test = round: 40 / acc: 81.660% / loss: 0.7325 / Time: 0.93s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 1.2743604183197021, avg_sq_norm_grad = 136.97689819335938,                  max_norm_grad = 15.797441482543945, var_grad = 135.70254516601562
round 41: local lr = 0.01, sq_norm_avg_grad = 1.3598660230636597, avg_sq_norm_grad = 134.39808654785156,                  max_norm_grad = 15.198869705200195, var_grad = 133.03822326660156
round 42: local lr = 0.01, sq_norm_avg_grad = 1.1144812107086182, avg_sq_norm_grad = 129.8943634033203,                  max_norm_grad = 15.437787055969238, var_grad = 128.77987670898438
round 43: local lr = 0.01, sq_norm_avg_grad = 1.2319060564041138, avg_sq_norm_grad = 127.77320861816406,                  max_norm_grad = 15.664928436279297, var_grad = 126.54130554199219
round 44: local lr = 0.01, sq_norm_avg_grad = 0.9722183346748352, avg_sq_norm_grad = 125.59366607666016,                  max_norm_grad = 15.465448379516602, var_grad = 124.62144470214844

>>> Round:   45 / Acc: 80.762% / Loss: 0.7040 /Time: 5.53s
======================================================================================================

= Test = round: 45 / acc: 82.640% / loss: 0.6683 / Time: 1.15s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 0.909112274646759, avg_sq_norm_grad = 124.96076202392578,                  max_norm_grad = 14.919089317321777, var_grad = 124.05165100097656
round 46: local lr = 0.01, sq_norm_avg_grad = 1.081889033317566, avg_sq_norm_grad = 123.02944946289062,                  max_norm_grad = 15.168037414550781, var_grad = 121.94756317138672
round 47: local lr = 0.01, sq_norm_avg_grad = 0.7486739754676819, avg_sq_norm_grad = 119.01431274414062,                  max_norm_grad = 14.740628242492676, var_grad = 118.26564025878906
round 48: local lr = 0.01, sq_norm_avg_grad = 0.8014956712722778, avg_sq_norm_grad = 117.05545806884766,                  max_norm_grad = 14.791824340820312, var_grad = 116.25395965576172
round 49: local lr = 0.01, sq_norm_avg_grad = 0.7371397614479065, avg_sq_norm_grad = 116.6949462890625,                  max_norm_grad = 14.61208438873291, var_grad = 115.95780944824219

>>> Round:   50 / Acc: 81.860% / Loss: 0.6531 /Time: 5.37s
======================================================================================================

= Test = round: 50 / acc: 83.240% / loss: 0.6186 / Time: 0.99s
======================================================================================================

round 50: local lr = 0.01, sq_norm_avg_grad = 0.8124870657920837, avg_sq_norm_grad = 116.133056640625,                  max_norm_grad = 14.867570877075195, var_grad = 115.32057189941406
round 51: local lr = 0.01, sq_norm_avg_grad = 0.8629587292671204, avg_sq_norm_grad = 113.16815948486328,                  max_norm_grad = 14.567940711975098, var_grad = 112.3051986694336
round 52: local lr = 0.01, sq_norm_avg_grad = 0.80910325050354, avg_sq_norm_grad = 111.15245819091797,                  max_norm_grad = 14.451803207397461, var_grad = 110.34335327148438
round 53: local lr = 0.01, sq_norm_avg_grad = 0.7085914611816406, avg_sq_norm_grad = 110.47820281982422,                  max_norm_grad = 14.346590042114258, var_grad = 109.76960754394531
round 54: local lr = 0.01, sq_norm_avg_grad = 0.6413360238075256, avg_sq_norm_grad = 108.63626861572266,                  max_norm_grad = 14.35849380493164, var_grad = 107.99493408203125

>>> Round:   55 / Acc: 82.860% / Loss: 0.6129 /Time: 6.63s
======================================================================================================

= Test = round: 55 / acc: 84.260% / loss: 0.5795 / Time: 1.12s
======================================================================================================

round 55: local lr = 0.01, sq_norm_avg_grad = 0.5852248668670654, avg_sq_norm_grad = 107.60369110107422,                  max_norm_grad = 14.298108100891113, var_grad = 107.01846313476562
round 56: local lr = 0.01, sq_norm_avg_grad = 0.7381441593170166, avg_sq_norm_grad = 105.32877349853516,                  max_norm_grad = 14.353156089782715, var_grad = 104.59062957763672
round 57: local lr = 0.01, sq_norm_avg_grad = 0.7180529236793518, avg_sq_norm_grad = 105.65518951416016,                  max_norm_grad = 14.078100204467773, var_grad = 104.9371337890625
round 58: local lr = 0.01, sq_norm_avg_grad = 0.6170927286148071, avg_sq_norm_grad = 103.67383575439453,                  max_norm_grad = 13.874322891235352, var_grad = 103.0567398071289
round 59: local lr = 0.01, sq_norm_avg_grad = 0.8170703649520874, avg_sq_norm_grad = 101.19036102294922,                  max_norm_grad = 13.933260917663574, var_grad = 100.373291015625

>>> Round:   60 / Acc: 83.576% / Loss: 0.5794 /Time: 4.99s
======================================================================================================

= Test = round: 60 / acc: 84.880% / loss: 0.5467 / Time: 1.05s
======================================================================================================

round 60: local lr = 0.01, sq_norm_avg_grad = 0.5467708706855774, avg_sq_norm_grad = 101.01919555664062,                  max_norm_grad = 13.545269966125488, var_grad = 100.47242736816406
round 61: local lr = 0.01, sq_norm_avg_grad = 0.5716499090194702, avg_sq_norm_grad = 99.78360748291016,                  max_norm_grad = 13.750638961791992, var_grad = 99.21195983886719
round 62: local lr = 0.01, sq_norm_avg_grad = 0.7036706805229187, avg_sq_norm_grad = 99.0054702758789,                  max_norm_grad = 14.078107833862305, var_grad = 98.30179595947266
round 63: local lr = 0.01, sq_norm_avg_grad = 0.462741494178772, avg_sq_norm_grad = 95.14893341064453,                  max_norm_grad = 13.121675491333008, var_grad = 94.68619537353516
round 64: local lr = 0.01, sq_norm_avg_grad = 0.5276050567626953, avg_sq_norm_grad = 93.7276382446289,                  max_norm_grad = 13.118857383728027, var_grad = 93.20003509521484

>>> Round:   65 / Acc: 84.120% / Loss: 0.5542 /Time: 5.60s
======================================================================================================

= Test = round: 65 / acc: 85.320% / loss: 0.5217 / Time: 0.83s
======================================================================================================

round 65: local lr = 0.01, sq_norm_avg_grad = 0.6018089056015015, avg_sq_norm_grad = 93.23616790771484,                  max_norm_grad = 13.289198875427246, var_grad = 92.63436126708984
round 66: local lr = 0.01, sq_norm_avg_grad = 0.48349958658218384, avg_sq_norm_grad = 91.92548370361328,                  max_norm_grad = 13.042052268981934, var_grad = 91.44198608398438
round 67: local lr = 0.01, sq_norm_avg_grad = 0.6079514622688293, avg_sq_norm_grad = 91.22461700439453,                  max_norm_grad = 13.060230255126953, var_grad = 90.61666870117188
round 68: local lr = 0.01, sq_norm_avg_grad = 0.5044557452201843, avg_sq_norm_grad = 88.50150299072266,                  max_norm_grad = 12.927512168884277, var_grad = 87.9970474243164
round 69: local lr = 0.01, sq_norm_avg_grad = 0.5169230699539185, avg_sq_norm_grad = 88.53267669677734,                  max_norm_grad = 12.843347549438477, var_grad = 88.01575469970703

>>> Round:   70 / Acc: 84.924% / Loss: 0.5279 /Time: 4.97s
======================================================================================================

= Test = round: 70 / acc: 86.080% / loss: 0.4954 / Time: 0.92s
======================================================================================================

round 70: local lr = 0.01, sq_norm_avg_grad = 0.34269654750823975, avg_sq_norm_grad = 87.64435577392578,                  max_norm_grad = 12.343722343444824, var_grad = 87.3016586303711
round 71: local lr = 0.01, sq_norm_avg_grad = 0.5867615938186646, avg_sq_norm_grad = 86.39351654052734,                  max_norm_grad = 13.165514945983887, var_grad = 85.80675506591797
round 72: local lr = 0.01, sq_norm_avg_grad = 0.7188018560409546, avg_sq_norm_grad = 86.13275146484375,                  max_norm_grad = 13.282309532165527, var_grad = 85.41394805908203
round 73: local lr = 0.01, sq_norm_avg_grad = 0.4120695888996124, avg_sq_norm_grad = 83.54786682128906,                  max_norm_grad = 12.739032745361328, var_grad = 83.13579559326172
round 74: local lr = 0.01, sq_norm_avg_grad = 0.3863130211830139, avg_sq_norm_grad = 82.94783782958984,                  max_norm_grad = 12.557926177978516, var_grad = 82.5615234375

>>> Round:   75 / Acc: 85.428% / Loss: 0.5073 /Time: 4.62s
======================================================================================================

= Test = round: 75 / acc: 86.760% / loss: 0.4753 / Time: 0.91s
======================================================================================================

round 75: local lr = 0.01, sq_norm_avg_grad = 0.3701430857181549, avg_sq_norm_grad = 82.66480255126953,                  max_norm_grad = 12.290518760681152, var_grad = 82.29466247558594
round 76: local lr = 0.01, sq_norm_avg_grad = 0.32186171412467957, avg_sq_norm_grad = 81.48368835449219,                  max_norm_grad = 12.22742748260498, var_grad = 81.16182708740234
round 77: local lr = 0.01, sq_norm_avg_grad = 0.4006102383136749, avg_sq_norm_grad = 80.9613037109375,                  max_norm_grad = 12.373435020446777, var_grad = 80.5606918334961
round 78: local lr = 0.01, sq_norm_avg_grad = 0.4433343708515167, avg_sq_norm_grad = 80.24261474609375,                  max_norm_grad = 12.335590362548828, var_grad = 79.79927825927734
round 79: local lr = 0.01, sq_norm_avg_grad = 0.4443574547767639, avg_sq_norm_grad = 79.173828125,                  max_norm_grad = 12.410680770874023, var_grad = 78.7294692993164

>>> Round:   80 / Acc: 85.906% / Loss: 0.4886 /Time: 5.23s
======================================================================================================

= Test = round: 80 / acc: 87.170% / loss: 0.4573 / Time: 1.03s
======================================================================================================

round 80: local lr = 0.01, sq_norm_avg_grad = 0.3762776851654053, avg_sq_norm_grad = 78.84548950195312,                  max_norm_grad = 11.948831558227539, var_grad = 78.4692153930664
round 81: local lr = 0.01, sq_norm_avg_grad = 0.4707832634449005, avg_sq_norm_grad = 78.3917465209961,                  max_norm_grad = 12.210038185119629, var_grad = 77.92095947265625
round 82: local lr = 0.01, sq_norm_avg_grad = 0.39488038420677185, avg_sq_norm_grad = 77.81246185302734,                  max_norm_grad = 12.009324073791504, var_grad = 77.4175796508789
round 83: local lr = 0.01, sq_norm_avg_grad = 0.3524841368198395, avg_sq_norm_grad = 76.41417694091797,                  max_norm_grad = 11.892427444458008, var_grad = 76.06169128417969
round 84: local lr = 0.01, sq_norm_avg_grad = 0.5204556584358215, avg_sq_norm_grad = 76.04075622558594,                  max_norm_grad = 12.066935539245605, var_grad = 75.52030181884766

>>> Round:   85 / Acc: 86.327% / Loss: 0.4713 /Time: 5.48s
======================================================================================================

= Test = round: 85 / acc: 87.470% / loss: 0.4402 / Time: 1.37s
======================================================================================================

round 85: local lr = 0.01, sq_norm_avg_grad = 0.31075626611709595, avg_sq_norm_grad = 74.70854187011719,                  max_norm_grad = 11.763092041015625, var_grad = 74.39778900146484
round 86: local lr = 0.01, sq_norm_avg_grad = 0.3144277334213257, avg_sq_norm_grad = 74.13896179199219,                  max_norm_grad = 11.790706634521484, var_grad = 73.82453155517578
round 87: local lr = 0.01, sq_norm_avg_grad = 0.31469103693962097, avg_sq_norm_grad = 73.17082214355469,                  max_norm_grad = 11.865263938903809, var_grad = 72.85613250732422
round 88: local lr = 0.01, sq_norm_avg_grad = 0.3591856062412262, avg_sq_norm_grad = 71.75908660888672,                  max_norm_grad = 11.531473159790039, var_grad = 71.39990234375
round 89: local lr = 0.01, sq_norm_avg_grad = 0.3132956624031067, avg_sq_norm_grad = 72.00514221191406,                  max_norm_grad = 11.537470817565918, var_grad = 71.69184875488281

>>> Round:   90 / Acc: 86.756% / Loss: 0.4554 /Time: 5.12s
======================================================================================================

= Test = round: 90 / acc: 87.820% / loss: 0.4242 / Time: 1.08s
======================================================================================================

round 90: local lr = 0.01, sq_norm_avg_grad = 0.309246689081192, avg_sq_norm_grad = 72.10623168945312,                  max_norm_grad = 11.42774772644043, var_grad = 71.79698181152344
round 91: local lr = 0.01, sq_norm_avg_grad = 0.291878879070282, avg_sq_norm_grad = 71.8224868774414,                  max_norm_grad = 11.338712692260742, var_grad = 71.53060913085938
round 92: local lr = 0.01, sq_norm_avg_grad = 0.30505165457725525, avg_sq_norm_grad = 70.86626434326172,                  max_norm_grad = 11.58736515045166, var_grad = 70.56121063232422
round 93: local lr = 0.01, sq_norm_avg_grad = 0.333021879196167, avg_sq_norm_grad = 70.52359771728516,                  max_norm_grad = 11.545171737670898, var_grad = 70.1905746459961
round 94: local lr = 0.01, sq_norm_avg_grad = 0.24393129348754883, avg_sq_norm_grad = 68.61504364013672,                  max_norm_grad = 11.17889404296875, var_grad = 68.37110900878906

>>> Round:   95 / Acc: 87.142% / Loss: 0.4429 /Time: 5.10s
======================================================================================================

= Test = round: 95 / acc: 88.230% / loss: 0.4119 / Time: 1.01s
======================================================================================================

round 95: local lr = 0.01, sq_norm_avg_grad = 0.31265896558761597, avg_sq_norm_grad = 67.9273910522461,                  max_norm_grad = 11.129985809326172, var_grad = 67.61473083496094
round 96: local lr = 0.01, sq_norm_avg_grad = 0.37535086274147034, avg_sq_norm_grad = 67.88980102539062,                  max_norm_grad = 11.549983024597168, var_grad = 67.51445007324219
round 97: local lr = 0.01, sq_norm_avg_grad = 0.24547134339809418, avg_sq_norm_grad = 66.73270416259766,                  max_norm_grad = 11.300946235656738, var_grad = 66.48723602294922
round 98: local lr = 0.01, sq_norm_avg_grad = 0.234794482588768, avg_sq_norm_grad = 66.7394790649414,                  max_norm_grad = 11.09126091003418, var_grad = 66.50468444824219
round 99: local lr = 0.01, sq_norm_avg_grad = 0.19250130653381348, avg_sq_norm_grad = 65.88668823242188,                  max_norm_grad = 10.823002815246582, var_grad = 65.69418334960938

>>> Round:  100 / Acc: 87.504% / Loss: 0.4285 /Time: 5.73s
======================================================================================================

= Test = round: 100 / acc: 88.600% / loss: 0.3980 / Time: 1.23s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.3373, Train_acc: 0.5853, Test_loss: 1.3127, Test_acc: 0.5939
Epoch: 006, Train_loss: 1.1214, Train_acc: 0.6551, Test_loss: 1.1023, Test_acc: 0.6593
Epoch: 011, Train_loss: 1.0935, Train_acc: 0.6666, Test_loss: 1.0744, Test_acc: 0.6713
Epoch: 016, Train_loss: 1.0950, Train_acc: 0.6672, Test_loss: 1.0783, Test_acc: 0.6689
Epoch: 021, Train_loss: 1.0830, Train_acc: 0.6693, Test_loss: 1.0630, Test_acc: 0.6729
Epoch: 026, Train_loss: 1.0925, Train_acc: 0.6720, Test_loss: 1.0718, Test_acc: 0.6793
Epoch: 031, Train_loss: 1.0720, Train_acc: 0.6783, Test_loss: 1.0543, Test_acc: 0.6806
Epoch: 036, Train_loss: 1.0714, Train_acc: 0.6795, Test_loss: 1.0580, Test_acc: 0.6810
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231001233602_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231001233602_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.0642814392542523, 0.6807681225741936, 1.0514449003921218, 0.6794800577713588]
model_source_only: [2.5680765375052728, 0.4017898001728784, 2.600016067033608, 0.3994000666592601]

************************************************************************************************************************

uid: 20231002002805
FL pretrained model will be saved at ./models/lenet_mnist_20231002002805.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 14.788% / Loss: 2.3056 /Time: 5.12s
======================================================================================================

= Test = round: 0 / acc: 14.820% / loss: 2.3074 / Time: 0.89s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.057499490678310394, avg_sq_norm_grad = 2.7803781032562256,                  max_norm_grad = 1.9720778465270996, var_grad = 2.7228786945343018
round 2: local lr = 0.01, sq_norm_avg_grad = 0.07774694263935089, avg_sq_norm_grad = 4.129530906677246,                  max_norm_grad = 2.437798023223877, var_grad = 4.051784038543701
round 3: local lr = 0.01, sq_norm_avg_grad = 0.12576070427894592, avg_sq_norm_grad = 7.18594217300415,                  max_norm_grad = 3.2457363605499268, var_grad = 7.060181617736816
round 4: local lr = 0.01, sq_norm_avg_grad = 0.27581465244293213, avg_sq_norm_grad = 15.801531791687012,                  max_norm_grad = 4.820827007293701, var_grad = 15.525716781616211

>>> Round:    5 / Acc: 19.705% / Loss: 2.2803 /Time: 5.87s
======================================================================================================

= Test = round: 5 / acc: 20.030% / loss: 2.2820 / Time: 1.20s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.9394583106040955, avg_sq_norm_grad = 40.5402946472168,                  max_norm_grad = 7.787567615509033, var_grad = 39.60083770751953
round 6: local lr = 0.01, sq_norm_avg_grad = 2.2892227172851562, avg_sq_norm_grad = 91.05256652832031,                  max_norm_grad = 11.608877182006836, var_grad = 88.76334381103516
round 7: local lr = 0.01, sq_norm_avg_grad = 2.836760997772217, avg_sq_norm_grad = 153.82965087890625,                  max_norm_grad = 14.901935577392578, var_grad = 150.99288940429688
round 8: local lr = 0.01, sq_norm_avg_grad = 2.1643803119659424, avg_sq_norm_grad = 188.70741271972656,                  max_norm_grad = 16.360515594482422, var_grad = 186.54302978515625
round 9: local lr = 0.01, sq_norm_avg_grad = 1.7264776229858398, avg_sq_norm_grad = 193.4638671875,                  max_norm_grad = 16.370071411132812, var_grad = 191.73739624023438

>>> Round:   10 / Acc: 51.891% / Loss: 2.0487 /Time: 4.34s
======================================================================================================

= Test = round: 10 / acc: 53.560% / loss: 2.0390 / Time: 0.85s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 1.6226296424865723, avg_sq_norm_grad = 196.6049346923828,                  max_norm_grad = 16.34964942932129, var_grad = 194.9822998046875
round 11: local lr = 0.01, sq_norm_avg_grad = 1.8355792760849, avg_sq_norm_grad = 200.20726013183594,                  max_norm_grad = 16.497549057006836, var_grad = 198.37167358398438
round 12: local lr = 0.01, sq_norm_avg_grad = 1.9604305028915405, avg_sq_norm_grad = 196.93492126464844,                  max_norm_grad = 16.233301162719727, var_grad = 194.9744873046875
round 13: local lr = 0.01, sq_norm_avg_grad = 2.013474225997925, avg_sq_norm_grad = 198.72572326660156,                  max_norm_grad = 16.438568115234375, var_grad = 196.71224975585938
round 14: local lr = 0.01, sq_norm_avg_grad = 2.1983189582824707, avg_sq_norm_grad = 198.00439453125,                  max_norm_grad = 16.58167266845703, var_grad = 195.8060760498047

>>> Round:   15 / Acc: 63.825% / Loss: 1.7647 /Time: 4.54s
======================================================================================================

= Test = round: 15 / acc: 66.130% / loss: 1.7411 / Time: 0.90s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 2.3962106704711914, avg_sq_norm_grad = 199.5498809814453,                  max_norm_grad = 16.595348358154297, var_grad = 197.15367126464844
round 16: local lr = 0.01, sq_norm_avg_grad = 2.5080416202545166, avg_sq_norm_grad = 197.99365234375,                  max_norm_grad = 16.507761001586914, var_grad = 195.48561096191406
round 17: local lr = 0.01, sq_norm_avg_grad = 2.6645383834838867, avg_sq_norm_grad = 196.1342010498047,                  max_norm_grad = 16.43014907836914, var_grad = 193.46966552734375
round 18: local lr = 0.01, sq_norm_avg_grad = 2.4703829288482666, avg_sq_norm_grad = 197.67605590820312,                  max_norm_grad = 16.250072479248047, var_grad = 195.20567321777344
round 19: local lr = 0.01, sq_norm_avg_grad = 2.4438064098358154, avg_sq_norm_grad = 195.4457550048828,                  max_norm_grad = 16.26521873474121, var_grad = 193.001953125

>>> Round:   20 / Acc: 69.768% / Loss: 1.4453 /Time: 4.94s
======================================================================================================

= Test = round: 20 / acc: 72.030% / loss: 1.4100 / Time: 0.90s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 2.7076199054718018, avg_sq_norm_grad = 191.88218688964844,                  max_norm_grad = 16.380373001098633, var_grad = 189.174560546875
round 21: local lr = 0.01, sq_norm_avg_grad = 2.1071624755859375, avg_sq_norm_grad = 189.63189697265625,                  max_norm_grad = 16.03510284423828, var_grad = 187.5247344970703
round 22: local lr = 0.01, sq_norm_avg_grad = 2.412888288497925, avg_sq_norm_grad = 185.98655700683594,                  max_norm_grad = 16.338394165039062, var_grad = 183.57366943359375
round 23: local lr = 0.01, sq_norm_avg_grad = 2.1838862895965576, avg_sq_norm_grad = 180.78797912597656,                  max_norm_grad = 16.154691696166992, var_grad = 178.60409545898438
round 24: local lr = 0.01, sq_norm_avg_grad = 2.217738389968872, avg_sq_norm_grad = 182.1202850341797,                  max_norm_grad = 16.474695205688477, var_grad = 179.9025421142578

>>> Round:   25 / Acc: 73.738% / Loss: 1.1751 /Time: 4.72s
======================================================================================================

= Test = round: 25 / acc: 75.890% / loss: 1.1347 / Time: 0.92s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 2.228888988494873, avg_sq_norm_grad = 180.00775146484375,                  max_norm_grad = 16.436548233032227, var_grad = 177.77886962890625
round 26: local lr = 0.01, sq_norm_avg_grad = 2.295159101486206, avg_sq_norm_grad = 174.07986450195312,                  max_norm_grad = 16.452009201049805, var_grad = 171.78469848632812
round 27: local lr = 0.01, sq_norm_avg_grad = 1.9354074001312256, avg_sq_norm_grad = 168.48092651367188,                  max_norm_grad = 16.572343826293945, var_grad = 166.54551696777344
round 28: local lr = 0.01, sq_norm_avg_grad = 1.757949709892273, avg_sq_norm_grad = 166.11480712890625,                  max_norm_grad = 16.44960594177246, var_grad = 164.3568572998047
round 29: local lr = 0.01, sq_norm_avg_grad = 1.6224905252456665, avg_sq_norm_grad = 165.0658416748047,                  max_norm_grad = 16.293766021728516, var_grad = 163.44334411621094

>>> Round:   30 / Acc: 76.024% / Loss: 0.9886 /Time: 4.58s
======================================================================================================

= Test = round: 30 / acc: 78.170% / loss: 0.9472 / Time: 0.90s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 1.7491960525512695, avg_sq_norm_grad = 161.59730529785156,                  max_norm_grad = 16.08634376525879, var_grad = 159.84811401367188
round 31: local lr = 0.01, sq_norm_avg_grad = 1.8831285238265991, avg_sq_norm_grad = 161.6635284423828,                  max_norm_grad = 16.596940994262695, var_grad = 159.7803955078125
round 32: local lr = 0.01, sq_norm_avg_grad = 1.5998754501342773, avg_sq_norm_grad = 152.5487518310547,                  max_norm_grad = 16.19643211364746, var_grad = 150.94888305664062
round 33: local lr = 0.01, sq_norm_avg_grad = 1.6006001234054565, avg_sq_norm_grad = 151.15826416015625,                  max_norm_grad = 16.413715362548828, var_grad = 149.5576629638672
round 34: local lr = 0.01, sq_norm_avg_grad = 1.3973182439804077, avg_sq_norm_grad = 149.44956970214844,                  max_norm_grad = 16.23907470703125, var_grad = 148.05224609375

>>> Round:   35 / Acc: 78.096% / Loss: 0.8608 /Time: 4.44s
======================================================================================================

= Test = round: 35 / acc: 80.540% / loss: 0.8210 / Time: 0.90s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 1.2597872018814087, avg_sq_norm_grad = 145.6674041748047,                  max_norm_grad = 15.698841094970703, var_grad = 144.40762329101562
round 36: local lr = 0.01, sq_norm_avg_grad = 1.3420668840408325, avg_sq_norm_grad = 145.60057067871094,                  max_norm_grad = 15.89759349822998, var_grad = 144.2584991455078
round 37: local lr = 0.01, sq_norm_avg_grad = 1.2014824151992798, avg_sq_norm_grad = 140.33523559570312,                  max_norm_grad = 16.071306228637695, var_grad = 139.13375854492188
round 38: local lr = 0.01, sq_norm_avg_grad = 1.2444220781326294, avg_sq_norm_grad = 139.30003356933594,                  max_norm_grad = 16.060304641723633, var_grad = 138.0556182861328
round 39: local lr = 0.01, sq_norm_avg_grad = 1.2722043991088867, avg_sq_norm_grad = 136.77723693847656,                  max_norm_grad = 15.44070816040039, var_grad = 135.50503540039062

>>> Round:   40 / Acc: 79.664% / Loss: 0.7700 /Time: 4.70s
======================================================================================================

= Test = round: 40 / acc: 81.620% / loss: 0.7322 / Time: 0.94s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 1.0804363489151, avg_sq_norm_grad = 135.6866455078125,                  max_norm_grad = 15.250201225280762, var_grad = 134.60621643066406
round 41: local lr = 0.01, sq_norm_avg_grad = 0.907689094543457, avg_sq_norm_grad = 131.30831909179688,                  max_norm_grad = 15.24677848815918, var_grad = 130.400634765625
round 42: local lr = 0.01, sq_norm_avg_grad = 1.0663539171218872, avg_sq_norm_grad = 132.0591583251953,                  max_norm_grad = 15.223766326904297, var_grad = 130.9927978515625
round 43: local lr = 0.01, sq_norm_avg_grad = 1.1412913799285889, avg_sq_norm_grad = 127.43087768554688,                  max_norm_grad = 15.485569953918457, var_grad = 126.28958892822266
round 44: local lr = 0.01, sq_norm_avg_grad = 0.8128311634063721, avg_sq_norm_grad = 127.21736907958984,                  max_norm_grad = 14.97519588470459, var_grad = 126.404541015625

>>> Round:   45 / Acc: 80.867% / Loss: 0.7061 /Time: 4.75s
======================================================================================================

= Test = round: 45 / acc: 82.710% / loss: 0.6706 / Time: 0.89s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 0.843168318271637, avg_sq_norm_grad = 121.88389587402344,                  max_norm_grad = 14.776646614074707, var_grad = 121.04072570800781
round 46: local lr = 0.01, sq_norm_avg_grad = 0.8383747935295105, avg_sq_norm_grad = 121.10077667236328,                  max_norm_grad = 14.996501922607422, var_grad = 120.26240539550781
round 47: local lr = 0.01, sq_norm_avg_grad = 0.8130695819854736, avg_sq_norm_grad = 121.3182601928711,                  max_norm_grad = 14.85986328125, var_grad = 120.50518798828125
round 48: local lr = 0.01, sq_norm_avg_grad = 0.7399843335151672, avg_sq_norm_grad = 119.82247924804688,                  max_norm_grad = 14.629079818725586, var_grad = 119.0824966430664
round 49: local lr = 0.01, sq_norm_avg_grad = 0.7615130543708801, avg_sq_norm_grad = 116.98007202148438,                  max_norm_grad = 14.977579116821289, var_grad = 116.21855926513672

>>> Round:   50 / Acc: 81.625% / Loss: 0.6578 /Time: 4.58s
======================================================================================================

= Test = round: 50 / acc: 83.120% / loss: 0.6233 / Time: 0.87s
======================================================================================================

round 50: local lr = 0.01, sq_norm_avg_grad = 1.2236907482147217, avg_sq_norm_grad = 114.76107025146484,                  max_norm_grad = 15.434697151184082, var_grad = 113.5373764038086
round 51: local lr = 0.01, sq_norm_avg_grad = 0.9548711180686951, avg_sq_norm_grad = 112.40015411376953,                  max_norm_grad = 14.761273384094238, var_grad = 111.44528198242188
round 52: local lr = 0.01, sq_norm_avg_grad = 0.6572800874710083, avg_sq_norm_grad = 111.76580047607422,                  max_norm_grad = 14.575098991394043, var_grad = 111.1085205078125
round 53: local lr = 0.01, sq_norm_avg_grad = 0.7718419432640076, avg_sq_norm_grad = 110.61884307861328,                  max_norm_grad = 14.458992004394531, var_grad = 109.84700012207031
round 54: local lr = 0.01, sq_norm_avg_grad = 0.8145908713340759, avg_sq_norm_grad = 107.99286651611328,                  max_norm_grad = 14.363137245178223, var_grad = 107.17827606201172

>>> Round:   55 / Acc: 82.797% / Loss: 0.6146 /Time: 4.86s
======================================================================================================

= Test = round: 55 / acc: 84.200% / loss: 0.5812 / Time: 0.96s
======================================================================================================

round 55: local lr = 0.01, sq_norm_avg_grad = 0.7121512293815613, avg_sq_norm_grad = 106.28648376464844,                  max_norm_grad = 14.234992027282715, var_grad = 105.57433319091797
round 56: local lr = 0.01, sq_norm_avg_grad = 0.7050237059593201, avg_sq_norm_grad = 103.8635025024414,                  max_norm_grad = 14.185620307922363, var_grad = 103.15847778320312
round 57: local lr = 0.01, sq_norm_avg_grad = 0.6739112138748169, avg_sq_norm_grad = 103.58935546875,                  max_norm_grad = 14.098946571350098, var_grad = 102.91544342041016
round 58: local lr = 0.01, sq_norm_avg_grad = 0.596832811832428, avg_sq_norm_grad = 102.51569366455078,                  max_norm_grad = 13.967771530151367, var_grad = 101.91886138916016
round 59: local lr = 0.01, sq_norm_avg_grad = 0.5182406902313232, avg_sq_norm_grad = 100.74781036376953,                  max_norm_grad = 13.791640281677246, var_grad = 100.22956848144531

>>> Round:   60 / Acc: 83.458% / Loss: 0.5824 /Time: 4.75s
======================================================================================================

= Test = round: 60 / acc: 84.820% / loss: 0.5493 / Time: 0.91s
======================================================================================================

round 60: local lr = 0.01, sq_norm_avg_grad = 0.7196228504180908, avg_sq_norm_grad = 99.63519287109375,                  max_norm_grad = 14.195429801940918, var_grad = 98.91557312011719
round 61: local lr = 0.01, sq_norm_avg_grad = 0.5734220743179321, avg_sq_norm_grad = 99.20951843261719,                  max_norm_grad = 13.821887016296387, var_grad = 98.63609313964844
round 62: local lr = 0.01, sq_norm_avg_grad = 0.45174694061279297, avg_sq_norm_grad = 97.61094665527344,                  max_norm_grad = 13.4855375289917, var_grad = 97.1592025756836
round 63: local lr = 0.01, sq_norm_avg_grad = 0.4725239872932434, avg_sq_norm_grad = 96.98703002929688,                  max_norm_grad = 13.227334022521973, var_grad = 96.5145034790039
round 64: local lr = 0.01, sq_norm_avg_grad = 0.5182201266288757, avg_sq_norm_grad = 93.9545669555664,                  max_norm_grad = 13.13800048828125, var_grad = 93.43634796142578

>>> Round:   65 / Acc: 84.199% / Loss: 0.5540 /Time: 4.72s
======================================================================================================

= Test = round: 65 / acc: 85.370% / loss: 0.5216 / Time: 0.88s
======================================================================================================

round 65: local lr = 0.01, sq_norm_avg_grad = 0.5649661421775818, avg_sq_norm_grad = 93.50736236572266,                  max_norm_grad = 13.10008430480957, var_grad = 92.94239807128906
round 66: local lr = 0.01, sq_norm_avg_grad = 0.49254974722862244, avg_sq_norm_grad = 92.32223510742188,                  max_norm_grad = 13.085592269897461, var_grad = 91.8296890258789
round 67: local lr = 0.01, sq_norm_avg_grad = 0.4513832628726959, avg_sq_norm_grad = 89.86666870117188,                  max_norm_grad = 13.098800659179688, var_grad = 89.415283203125
round 68: local lr = 0.01, sq_norm_avg_grad = 0.5551974177360535, avg_sq_norm_grad = 90.44409942626953,                  max_norm_grad = 13.325529098510742, var_grad = 89.88890075683594
round 69: local lr = 0.01, sq_norm_avg_grad = 0.4053990840911865, avg_sq_norm_grad = 89.07711029052734,                  max_norm_grad = 12.976397514343262, var_grad = 88.67171478271484

>>> Round:   70 / Acc: 84.860% / Loss: 0.5288 /Time: 4.58s
======================================================================================================

= Test = round: 70 / acc: 86.120% / loss: 0.4967 / Time: 0.93s
======================================================================================================

round 70: local lr = 0.01, sq_norm_avg_grad = 0.41802558302879333, avg_sq_norm_grad = 88.40556335449219,                  max_norm_grad = 12.806479454040527, var_grad = 87.98754119873047
round 71: local lr = 0.01, sq_norm_avg_grad = 0.43643930554389954, avg_sq_norm_grad = 87.3926773071289,                  max_norm_grad = 12.724065780639648, var_grad = 86.95623779296875
round 72: local lr = 0.01, sq_norm_avg_grad = 0.42736685276031494, avg_sq_norm_grad = 86.83064270019531,                  max_norm_grad = 12.692915916442871, var_grad = 86.40327453613281
round 73: local lr = 0.01, sq_norm_avg_grad = 0.6040030121803284, avg_sq_norm_grad = 85.85964965820312,                  max_norm_grad = 12.951391220092773, var_grad = 85.25564575195312
round 74: local lr = 0.01, sq_norm_avg_grad = 0.39420056343078613, avg_sq_norm_grad = 84.33489990234375,                  max_norm_grad = 12.781129837036133, var_grad = 83.9406967163086

>>> Round:   75 / Acc: 85.389% / Loss: 0.5090 /Time: 4.34s
======================================================================================================

= Test = round: 75 / acc: 86.570% / loss: 0.4772 / Time: 0.83s
======================================================================================================

round 75: local lr = 0.01, sq_norm_avg_grad = 0.4653509259223938, avg_sq_norm_grad = 83.01696014404297,                  max_norm_grad = 12.785969734191895, var_grad = 82.5516128540039
round 76: local lr = 0.01, sq_norm_avg_grad = 0.4330529570579529, avg_sq_norm_grad = 82.10820007324219,                  max_norm_grad = 12.438494682312012, var_grad = 81.6751480102539
round 77: local lr = 0.01, sq_norm_avg_grad = 0.38753587007522583, avg_sq_norm_grad = 81.4177017211914,                  max_norm_grad = 12.064773559570312, var_grad = 81.03016662597656
round 78: local lr = 0.01, sq_norm_avg_grad = 0.40769949555397034, avg_sq_norm_grad = 80.22054290771484,                  max_norm_grad = 12.245387077331543, var_grad = 79.8128433227539
round 79: local lr = 0.01, sq_norm_avg_grad = 0.3399523198604584, avg_sq_norm_grad = 79.28328704833984,                  max_norm_grad = 12.01905345916748, var_grad = 78.9433364868164

>>> Round:   80 / Acc: 85.860% / Loss: 0.4893 /Time: 4.64s
======================================================================================================

= Test = round: 80 / acc: 87.080% / loss: 0.4577 / Time: 0.93s
======================================================================================================

round 80: local lr = 0.01, sq_norm_avg_grad = 0.3829460144042969, avg_sq_norm_grad = 79.14434051513672,                  max_norm_grad = 11.94600772857666, var_grad = 78.76139831542969
round 81: local lr = 0.01, sq_norm_avg_grad = 0.2904280722141266, avg_sq_norm_grad = 77.83950805664062,                  max_norm_grad = 12.086503028869629, var_grad = 77.54907989501953
round 82: local lr = 0.01, sq_norm_avg_grad = 0.37741246819496155, avg_sq_norm_grad = 75.74449920654297,                  max_norm_grad = 12.033214569091797, var_grad = 75.3670883178711
round 83: local lr = 0.01, sq_norm_avg_grad = 0.39346906542778015, avg_sq_norm_grad = 75.26443481445312,                  max_norm_grad = 12.014615058898926, var_grad = 74.87096405029297
round 84: local lr = 0.01, sq_norm_avg_grad = 0.3225959837436676, avg_sq_norm_grad = 74.87562561035156,                  max_norm_grad = 11.702836990356445, var_grad = 74.55303192138672

>>> Round:   85 / Acc: 86.317% / Loss: 0.4726 /Time: 4.29s
======================================================================================================

= Test = round: 85 / acc: 87.410% / loss: 0.4416 / Time: 0.83s
======================================================================================================

round 85: local lr = 0.01, sq_norm_avg_grad = 0.289665549993515, avg_sq_norm_grad = 74.14152526855469,                  max_norm_grad = 11.848647117614746, var_grad = 73.85186004638672
round 86: local lr = 0.01, sq_norm_avg_grad = 0.3698679506778717, avg_sq_norm_grad = 74.1611328125,                  max_norm_grad = 11.969249725341797, var_grad = 73.79126739501953
round 87: local lr = 0.01, sq_norm_avg_grad = 0.4754559099674225, avg_sq_norm_grad = 74.07063293457031,                  max_norm_grad = 12.105670928955078, var_grad = 73.59517669677734
round 88: local lr = 0.01, sq_norm_avg_grad = 0.3861384093761444, avg_sq_norm_grad = 73.35511779785156,                  max_norm_grad = 11.710367202758789, var_grad = 72.96897888183594
round 89: local lr = 0.01, sq_norm_avg_grad = 0.30168795585632324, avg_sq_norm_grad = 72.31170654296875,                  max_norm_grad = 11.737385749816895, var_grad = 72.01001739501953

>>> Round:   90 / Acc: 86.696% / Loss: 0.4558 /Time: 4.45s
======================================================================================================

= Test = round: 90 / acc: 87.830% / loss: 0.4253 / Time: 0.84s
======================================================================================================

round 90: local lr = 0.01, sq_norm_avg_grad = 0.28000712394714355, avg_sq_norm_grad = 72.17717742919922,                  max_norm_grad = 11.710628509521484, var_grad = 71.89717102050781
round 91: local lr = 0.01, sq_norm_avg_grad = 0.34598103165626526, avg_sq_norm_grad = 70.87744140625,                  max_norm_grad = 11.8694486618042, var_grad = 70.53146362304688
round 92: local lr = 0.01, sq_norm_avg_grad = 0.2951103448867798, avg_sq_norm_grad = 70.87930297851562,                  max_norm_grad = 11.556777000427246, var_grad = 70.58419036865234
round 93: local lr = 0.01, sq_norm_avg_grad = 0.3198854625225067, avg_sq_norm_grad = 69.88856506347656,                  max_norm_grad = 11.597877502441406, var_grad = 69.56867980957031
round 94: local lr = 0.01, sq_norm_avg_grad = 0.2745826840400696, avg_sq_norm_grad = 69.69208526611328,                  max_norm_grad = 11.395928382873535, var_grad = 69.4175033569336

>>> Round:   95 / Acc: 87.142% / Loss: 0.4413 /Time: 4.28s
======================================================================================================

= Test = round: 95 / acc: 88.230% / loss: 0.4108 / Time: 0.86s
======================================================================================================

round 95: local lr = 0.01, sq_norm_avg_grad = 0.23404917120933533, avg_sq_norm_grad = 68.71939086914062,                  max_norm_grad = 11.415387153625488, var_grad = 68.48534393310547
round 96: local lr = 0.01, sq_norm_avg_grad = 0.30320459604263306, avg_sq_norm_grad = 68.86636352539062,                  max_norm_grad = 11.189607620239258, var_grad = 68.56315612792969
round 97: local lr = 0.01, sq_norm_avg_grad = 0.234222874045372, avg_sq_norm_grad = 67.58686065673828,                  max_norm_grad = 11.279486656188965, var_grad = 67.3526382446289
round 98: local lr = 0.01, sq_norm_avg_grad = 0.2599489390850067, avg_sq_norm_grad = 67.46861267089844,                  max_norm_grad = 11.376530647277832, var_grad = 67.20866394042969
round 99: local lr = 0.01, sq_norm_avg_grad = 0.31259724497795105, avg_sq_norm_grad = 66.47066497802734,                  max_norm_grad = 11.428772926330566, var_grad = 66.15806579589844

>>> Round:  100 / Acc: 87.463% / Loss: 0.4287 /Time: 4.43s
======================================================================================================

= Test = round: 100 / acc: 88.500% / loss: 0.3982 / Time: 0.87s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.3295, Train_acc: 0.5919, Test_loss: 1.3058, Test_acc: 0.6000
Epoch: 006, Train_loss: 1.1117, Train_acc: 0.6632, Test_loss: 1.0915, Test_acc: 0.6718
Epoch: 011, Train_loss: 1.0866, Train_acc: 0.6696, Test_loss: 1.0690, Test_acc: 0.6737
Epoch: 016, Train_loss: 1.0801, Train_acc: 0.6760, Test_loss: 1.0654, Test_acc: 0.6801
Epoch: 021, Train_loss: 1.0778, Train_acc: 0.6712, Test_loss: 1.0680, Test_acc: 0.6751
Epoch: 026, Train_loss: 1.0748, Train_acc: 0.6778, Test_loss: 1.0608, Test_acc: 0.6798
Epoch: 031, Train_loss: 1.0749, Train_acc: 0.6807, Test_loss: 1.0603, Test_acc: 0.6856
Epoch: 036, Train_loss: 1.0873, Train_acc: 0.6717, Test_loss: 1.0683, Test_acc: 0.6754
Epoch: 041, Train_loss: 1.0629, Train_acc: 0.6797, Test_loss: 1.0484, Test_acc: 0.6848
Epoch: 046, Train_loss: 1.0713, Train_acc: 0.6694, Test_loss: 1.0570, Test_acc: 0.6727
Epoch: 051, Train_loss: 1.0630, Train_acc: 0.6819, Test_loss: 1.0469, Test_acc: 0.6859
Epoch: 056, Train_loss: 1.0789, Train_acc: 0.6716, Test_loss: 1.0627, Test_acc: 0.6755
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002002805_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002002805_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.0613904403703915, 0.6837850205928713, 1.047962439860149, 0.6842573047439173]
model_source_only: [2.561900242173335, 0.40234911272690294, 2.5940849641444563, 0.4003999555604933]

************************************************************************************************************************

uid: 20231002011516
FL pretrained model will be saved at ./models/lenet_mnist_20231002011516.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 14.788% / Loss: 2.3056 /Time: 6.18s
======================================================================================================

= Test = round: 0 / acc: 14.820% / loss: 2.3074 / Time: 0.88s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.057453714311122894, avg_sq_norm_grad = 2.7787935733795166,                  max_norm_grad = 1.9708852767944336, var_grad = 2.7213399410247803
round 2: local lr = 0.01, sq_norm_avg_grad = 0.07796521484851837, avg_sq_norm_grad = 4.1323699951171875,                  max_norm_grad = 2.4400227069854736, var_grad = 4.0544047355651855
round 3: local lr = 0.01, sq_norm_avg_grad = 0.12525634467601776, avg_sq_norm_grad = 7.16752290725708,                  max_norm_grad = 3.2419726848602295, var_grad = 7.042266368865967
round 4: local lr = 0.01, sq_norm_avg_grad = 0.2722775936126709, avg_sq_norm_grad = 15.753890037536621,                  max_norm_grad = 4.815916538238525, var_grad = 15.481612205505371

>>> Round:    5 / Acc: 19.875% / Loss: 2.2802 /Time: 4.61s
======================================================================================================

= Test = round: 5 / acc: 20.220% / loss: 2.2819 / Time: 0.86s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.9316791892051697, avg_sq_norm_grad = 40.428794860839844,                  max_norm_grad = 7.769341945648193, var_grad = 39.49711608886719
round 6: local lr = 0.01, sq_norm_avg_grad = 2.2358856201171875, avg_sq_norm_grad = 90.90470123291016,                  max_norm_grad = 11.589221954345703, var_grad = 88.66881561279297
round 7: local lr = 0.01, sq_norm_avg_grad = 2.968621015548706, avg_sq_norm_grad = 155.79934692382812,                  max_norm_grad = 15.043896675109863, var_grad = 152.83071899414062
round 8: local lr = 0.01, sq_norm_avg_grad = 2.2261979579925537, avg_sq_norm_grad = 189.39801025390625,                  max_norm_grad = 16.37859535217285, var_grad = 187.17181396484375
round 9: local lr = 0.01, sq_norm_avg_grad = 1.7082220315933228, avg_sq_norm_grad = 194.8260498046875,                  max_norm_grad = 16.39120864868164, var_grad = 193.11782836914062

>>> Round:   10 / Acc: 49.179% / Loss: 2.0492 /Time: 4.99s
======================================================================================================

= Test = round: 10 / acc: 50.980% / loss: 2.0386 / Time: 0.97s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 1.7138396501541138, avg_sq_norm_grad = 197.21620178222656,                  max_norm_grad = 16.425682067871094, var_grad = 195.5023651123047
round 11: local lr = 0.01, sq_norm_avg_grad = 2.032707691192627, avg_sq_norm_grad = 196.77633666992188,                  max_norm_grad = 16.547008514404297, var_grad = 194.74362182617188
round 12: local lr = 0.01, sq_norm_avg_grad = 1.9311370849609375, avg_sq_norm_grad = 196.55667114257812,                  max_norm_grad = 16.471010208129883, var_grad = 194.6255340576172
round 13: local lr = 0.01, sq_norm_avg_grad = 2.2532620429992676, avg_sq_norm_grad = 196.7625732421875,                  max_norm_grad = 16.493762969970703, var_grad = 194.50930786132812
round 14: local lr = 0.01, sq_norm_avg_grad = 2.2662031650543213, avg_sq_norm_grad = 201.06781005859375,                  max_norm_grad = 16.58119773864746, var_grad = 198.80160522460938

>>> Round:   15 / Acc: 63.631% / Loss: 1.7664 /Time: 5.41s
======================================================================================================

= Test = round: 15 / acc: 65.500% / loss: 1.7435 / Time: 0.86s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 2.4612441062927246, avg_sq_norm_grad = 200.4201202392578,                  max_norm_grad = 16.649738311767578, var_grad = 197.95887756347656
round 16: local lr = 0.01, sq_norm_avg_grad = 2.4537203311920166, avg_sq_norm_grad = 195.64797973632812,                  max_norm_grad = 16.471086502075195, var_grad = 193.1942596435547
round 17: local lr = 0.01, sq_norm_avg_grad = 2.4408559799194336, avg_sq_norm_grad = 195.1798858642578,                  max_norm_grad = 16.32417106628418, var_grad = 192.73902893066406
round 18: local lr = 0.01, sq_norm_avg_grad = 2.449551582336426, avg_sq_norm_grad = 194.37991333007812,                  max_norm_grad = 16.193735122680664, var_grad = 191.93035888671875
round 19: local lr = 0.01, sq_norm_avg_grad = 2.4948830604553223, avg_sq_norm_grad = 191.34112548828125,                  max_norm_grad = 15.9273042678833, var_grad = 188.8462371826172

>>> Round:   20 / Acc: 70.240% / Loss: 1.4427 /Time: 5.75s
======================================================================================================

= Test = round: 20 / acc: 72.780% / loss: 1.4075 / Time: 0.85s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 2.678103446960449, avg_sq_norm_grad = 193.62301635742188,                  max_norm_grad = 16.267295837402344, var_grad = 190.94491577148438
round 21: local lr = 0.01, sq_norm_avg_grad = 2.454561471939087, avg_sq_norm_grad = 188.994873046875,                  max_norm_grad = 16.14954376220703, var_grad = 186.54031372070312
round 22: local lr = 0.01, sq_norm_avg_grad = 2.4835877418518066, avg_sq_norm_grad = 185.1541748046875,                  max_norm_grad = 16.107894897460938, var_grad = 182.67059326171875
round 23: local lr = 0.01, sq_norm_avg_grad = 2.3186686038970947, avg_sq_norm_grad = 182.81539916992188,                  max_norm_grad = 16.213672637939453, var_grad = 180.49673461914062
round 24: local lr = 0.01, sq_norm_avg_grad = 2.128908395767212, avg_sq_norm_grad = 182.02044677734375,                  max_norm_grad = 16.461589813232422, var_grad = 179.89154052734375

>>> Round:   25 / Acc: 73.308% / Loss: 1.1771 /Time: 4.63s
======================================================================================================

= Test = round: 25 / acc: 75.480% / loss: 1.1354 / Time: 0.88s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 2.1171884536743164, avg_sq_norm_grad = 173.00588989257812,                  max_norm_grad = 16.187074661254883, var_grad = 170.88870239257812
round 26: local lr = 0.01, sq_norm_avg_grad = 2.0778465270996094, avg_sq_norm_grad = 171.61851501464844,                  max_norm_grad = 16.335708618164062, var_grad = 169.54066467285156
round 27: local lr = 0.01, sq_norm_avg_grad = 1.9887615442276, avg_sq_norm_grad = 170.59725952148438,                  max_norm_grad = 16.546701431274414, var_grad = 168.60850524902344
round 28: local lr = 0.01, sq_norm_avg_grad = 1.7161507606506348, avg_sq_norm_grad = 167.15151977539062,                  max_norm_grad = 16.172861099243164, var_grad = 165.43536376953125
round 29: local lr = 0.01, sq_norm_avg_grad = 1.591403603553772, avg_sq_norm_grad = 166.41343688964844,                  max_norm_grad = 16.23625373840332, var_grad = 164.82203674316406

>>> Round:   30 / Acc: 76.028% / Loss: 0.9879 /Time: 4.99s
======================================================================================================

= Test = round: 30 / acc: 78.320% / loss: 0.9460 / Time: 1.03s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 1.6039485931396484, avg_sq_norm_grad = 158.9800567626953,                  max_norm_grad = 15.968759536743164, var_grad = 157.37611389160156
round 31: local lr = 0.01, sq_norm_avg_grad = 1.5228742361068726, avg_sq_norm_grad = 156.22308349609375,                  max_norm_grad = 16.15078353881836, var_grad = 154.70021057128906
round 32: local lr = 0.01, sq_norm_avg_grad = 1.505689263343811, avg_sq_norm_grad = 153.47775268554688,                  max_norm_grad = 16.19719123840332, var_grad = 151.97206115722656
round 33: local lr = 0.01, sq_norm_avg_grad = 1.4883849620819092, avg_sq_norm_grad = 152.20616149902344,                  max_norm_grad = 16.102407455444336, var_grad = 150.7177734375
round 34: local lr = 0.01, sq_norm_avg_grad = 1.4974709749221802, avg_sq_norm_grad = 149.83457946777344,                  max_norm_grad = 16.100183486938477, var_grad = 148.3371124267578

>>> Round:   35 / Acc: 78.065% / Loss: 0.8605 /Time: 4.38s
======================================================================================================

= Test = round: 35 / acc: 80.450% / loss: 0.8215 / Time: 0.81s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 1.293093204498291, avg_sq_norm_grad = 146.1634979248047,                  max_norm_grad = 15.8712797164917, var_grad = 144.8704071044922
round 36: local lr = 0.01, sq_norm_avg_grad = 1.2800577878952026, avg_sq_norm_grad = 144.83944702148438,                  max_norm_grad = 16.154632568359375, var_grad = 143.55938720703125
round 37: local lr = 0.01, sq_norm_avg_grad = 1.1326206922531128, avg_sq_norm_grad = 143.81033325195312,                  max_norm_grad = 15.854751586914062, var_grad = 142.67771911621094
round 38: local lr = 0.01, sq_norm_avg_grad = 1.5379382371902466, avg_sq_norm_grad = 136.9606475830078,                  max_norm_grad = 15.979010581970215, var_grad = 135.42271423339844
round 39: local lr = 0.01, sq_norm_avg_grad = 1.1662211418151855, avg_sq_norm_grad = 135.32542419433594,                  max_norm_grad = 15.667732238769531, var_grad = 134.15921020507812

>>> Round:   40 / Acc: 79.744% / Loss: 0.7708 /Time: 4.07s
======================================================================================================

= Test = round: 40 / acc: 81.600% / loss: 0.7335 / Time: 0.80s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 1.0071426630020142, avg_sq_norm_grad = 133.88409423828125,                  max_norm_grad = 15.626206398010254, var_grad = 132.876953125
round 41: local lr = 0.01, sq_norm_avg_grad = 0.8100802302360535, avg_sq_norm_grad = 131.84559631347656,                  max_norm_grad = 15.331082344055176, var_grad = 131.0355224609375
round 42: local lr = 0.01, sq_norm_avg_grad = 1.0598516464233398, avg_sq_norm_grad = 130.73638916015625,                  max_norm_grad = 15.82607364654541, var_grad = 129.67654418945312
round 43: local lr = 0.01, sq_norm_avg_grad = 1.1340278387069702, avg_sq_norm_grad = 127.90998077392578,                  max_norm_grad = 15.921091079711914, var_grad = 126.77595520019531
round 44: local lr = 0.01, sq_norm_avg_grad = 0.980786144733429, avg_sq_norm_grad = 127.4900131225586,                  max_norm_grad = 15.236205101013184, var_grad = 126.50922393798828

>>> Round:   45 / Acc: 80.969% / Loss: 0.7022 /Time: 4.04s
======================================================================================================

= Test = round: 45 / acc: 82.700% / loss: 0.6666 / Time: 0.77s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 0.7014464735984802, avg_sq_norm_grad = 125.22318267822266,                  max_norm_grad = 14.849745750427246, var_grad = 124.52173614501953
round 46: local lr = 0.01, sq_norm_avg_grad = 0.7935476899147034, avg_sq_norm_grad = 122.90196990966797,                  max_norm_grad = 14.864712715148926, var_grad = 122.1084213256836
round 47: local lr = 0.01, sq_norm_avg_grad = 0.7937647700309753, avg_sq_norm_grad = 119.97119140625,                  max_norm_grad = 14.760016441345215, var_grad = 119.17742919921875
round 48: local lr = 0.01, sq_norm_avg_grad = 0.7928494215011597, avg_sq_norm_grad = 118.29971313476562,                  max_norm_grad = 14.755488395690918, var_grad = 117.50686645507812
round 49: local lr = 0.01, sq_norm_avg_grad = 0.837343156337738, avg_sq_norm_grad = 116.03524780273438,                  max_norm_grad = 14.857908248901367, var_grad = 115.19790649414062

>>> Round:   50 / Acc: 81.913% / Loss: 0.6564 /Time: 4.25s
======================================================================================================

= Test = round: 50 / acc: 83.400% / loss: 0.6219 / Time: 0.86s
======================================================================================================

round 50: local lr = 0.01, sq_norm_avg_grad = 0.7775089740753174, avg_sq_norm_grad = 113.59230041503906,                  max_norm_grad = 14.469178199768066, var_grad = 112.81478881835938
round 51: local lr = 0.01, sq_norm_avg_grad = 0.7528201937675476, avg_sq_norm_grad = 113.26839447021484,                  max_norm_grad = 14.238945007324219, var_grad = 112.51557159423828
round 52: local lr = 0.01, sq_norm_avg_grad = 0.7917715311050415, avg_sq_norm_grad = 111.35990142822266,                  max_norm_grad = 14.604437828063965, var_grad = 110.56813049316406
round 53: local lr = 0.01, sq_norm_avg_grad = 0.6635900139808655, avg_sq_norm_grad = 111.10163879394531,                  max_norm_grad = 14.463908195495605, var_grad = 110.43804931640625
round 54: local lr = 0.01, sq_norm_avg_grad = 0.6297537088394165, avg_sq_norm_grad = 108.43611145019531,                  max_norm_grad = 14.434043884277344, var_grad = 107.80635833740234

>>> Round:   55 / Acc: 82.766% / Loss: 0.6142 /Time: 4.11s
======================================================================================================

= Test = round: 55 / acc: 84.180% / loss: 0.5805 / Time: 0.77s
======================================================================================================

round 55: local lr = 0.01, sq_norm_avg_grad = 0.6487011313438416, avg_sq_norm_grad = 106.91475677490234,                  max_norm_grad = 14.162418365478516, var_grad = 106.26605224609375
round 56: local lr = 0.01, sq_norm_avg_grad = 0.6771323084831238, avg_sq_norm_grad = 104.77312469482422,                  max_norm_grad = 14.095499992370605, var_grad = 104.09599304199219
round 57: local lr = 0.01, sq_norm_avg_grad = 0.6256722211837769, avg_sq_norm_grad = 102.74640655517578,                  max_norm_grad = 13.856195449829102, var_grad = 102.12073516845703
round 58: local lr = 0.01, sq_norm_avg_grad = 0.7273609638214111, avg_sq_norm_grad = 102.27574157714844,                  max_norm_grad = 14.018852233886719, var_grad = 101.54837799072266
round 59: local lr = 0.01, sq_norm_avg_grad = 0.8263471722602844, avg_sq_norm_grad = 101.5392837524414,                  max_norm_grad = 13.58662223815918, var_grad = 100.71293640136719

>>> Round:   60 / Acc: 83.434% / Loss: 0.5823 /Time: 4.01s
======================================================================================================

= Test = round: 60 / acc: 84.680% / loss: 0.5498 / Time: 0.80s
======================================================================================================

round 60: local lr = 0.01, sq_norm_avg_grad = 0.7250560522079468, avg_sq_norm_grad = 100.53123474121094,                  max_norm_grad = 13.65420913696289, var_grad = 99.8061752319336
round 61: local lr = 0.01, sq_norm_avg_grad = 0.5883710384368896, avg_sq_norm_grad = 97.14314270019531,                  max_norm_grad = 13.502636909484863, var_grad = 96.55477142333984
round 62: local lr = 0.01, sq_norm_avg_grad = 0.46533775329589844, avg_sq_norm_grad = 96.45247650146484,                  max_norm_grad = 13.12605094909668, var_grad = 95.98713684082031
round 63: local lr = 0.01, sq_norm_avg_grad = 0.4965381622314453, avg_sq_norm_grad = 95.88439178466797,                  max_norm_grad = 13.547717094421387, var_grad = 95.38785552978516
round 64: local lr = 0.01, sq_norm_avg_grad = 0.39468467235565186, avg_sq_norm_grad = 94.38480377197266,                  max_norm_grad = 13.019078254699707, var_grad = 93.99011993408203

>>> Round:   65 / Acc: 84.251% / Loss: 0.5535 /Time: 4.11s
======================================================================================================

= Test = round: 65 / acc: 85.480% / loss: 0.5208 / Time: 0.79s
======================================================================================================

round 65: local lr = 0.01, sq_norm_avg_grad = 0.41851940751075745, avg_sq_norm_grad = 92.7476577758789,                  max_norm_grad = 13.062488555908203, var_grad = 92.32913970947266
round 66: local lr = 0.01, sq_norm_avg_grad = 0.4500105082988739, avg_sq_norm_grad = 92.26573181152344,                  max_norm_grad = 12.995543479919434, var_grad = 91.81571960449219
round 67: local lr = 0.01, sq_norm_avg_grad = 0.4288051128387451, avg_sq_norm_grad = 91.5819320678711,                  max_norm_grad = 12.979138374328613, var_grad = 91.15312957763672
round 68: local lr = 0.01, sq_norm_avg_grad = 0.5000951290130615, avg_sq_norm_grad = 91.5638656616211,                  max_norm_grad = 12.930824279785156, var_grad = 91.06377410888672
round 69: local lr = 0.01, sq_norm_avg_grad = 0.5348263382911682, avg_sq_norm_grad = 88.83454895019531,                  max_norm_grad = 13.17721176147461, var_grad = 88.29972076416016

>>> Round:   70 / Acc: 84.836% / Loss: 0.5297 /Time: 4.03s
======================================================================================================

= Test = round: 70 / acc: 86.030% / loss: 0.4977 / Time: 0.79s
======================================================================================================

round 70: local lr = 0.01, sq_norm_avg_grad = 0.41659268736839294, avg_sq_norm_grad = 87.4823989868164,                  max_norm_grad = 12.67374038696289, var_grad = 87.06580352783203
round 71: local lr = 0.01, sq_norm_avg_grad = 0.4333936274051666, avg_sq_norm_grad = 87.91741943359375,                  max_norm_grad = 12.61870002746582, var_grad = 87.48402404785156
round 72: local lr = 0.01, sq_norm_avg_grad = 0.39713427424430847, avg_sq_norm_grad = 86.73020935058594,                  max_norm_grad = 12.601630210876465, var_grad = 86.33307647705078
round 73: local lr = 0.01, sq_norm_avg_grad = 0.42779606580734253, avg_sq_norm_grad = 85.79894256591797,                  max_norm_grad = 12.432199478149414, var_grad = 85.37114715576172
round 74: local lr = 0.01, sq_norm_avg_grad = 0.46098971366882324, avg_sq_norm_grad = 84.14677429199219,                  max_norm_grad = 12.389571189880371, var_grad = 83.68578338623047

>>> Round:   75 / Acc: 85.382% / Loss: 0.5080 /Time: 4.06s
======================================================================================================

= Test = round: 75 / acc: 86.520% / loss: 0.4758 / Time: 0.76s
======================================================================================================

round 75: local lr = 0.01, sq_norm_avg_grad = 0.3161674439907074, avg_sq_norm_grad = 82.70111083984375,                  max_norm_grad = 11.900972366333008, var_grad = 82.38494110107422
round 76: local lr = 0.01, sq_norm_avg_grad = 0.3743956685066223, avg_sq_norm_grad = 82.0687484741211,                  max_norm_grad = 12.124451637268066, var_grad = 81.69435119628906
round 77: local lr = 0.01, sq_norm_avg_grad = 0.38740772008895874, avg_sq_norm_grad = 80.92334747314453,                  max_norm_grad = 12.220239639282227, var_grad = 80.53594207763672
round 78: local lr = 0.01, sq_norm_avg_grad = 0.351252019405365, avg_sq_norm_grad = 80.6282730102539,                  max_norm_grad = 11.881439208984375, var_grad = 80.27702331542969
round 79: local lr = 0.01, sq_norm_avg_grad = 0.2642431855201721, avg_sq_norm_grad = 79.1366958618164,                  max_norm_grad = 11.942389488220215, var_grad = 78.87245178222656

>>> Round:   80 / Acc: 85.926% / Loss: 0.4895 /Time: 4.03s
======================================================================================================

= Test = round: 80 / acc: 87.050% / loss: 0.4582 / Time: 0.75s
======================================================================================================

round 80: local lr = 0.01, sq_norm_avg_grad = 0.27871087193489075, avg_sq_norm_grad = 77.70195770263672,                  max_norm_grad = 11.824613571166992, var_grad = 77.42324829101562
round 81: local lr = 0.01, sq_norm_avg_grad = 0.3295665383338928, avg_sq_norm_grad = 76.69165802001953,                  max_norm_grad = 11.863232612609863, var_grad = 76.36209106445312
round 82: local lr = 0.01, sq_norm_avg_grad = 0.28946205973625183, avg_sq_norm_grad = 76.31240844726562,                  max_norm_grad = 11.740073204040527, var_grad = 76.02294921875
round 83: local lr = 0.01, sq_norm_avg_grad = 0.32372626662254333, avg_sq_norm_grad = 76.3446044921875,                  max_norm_grad = 11.980819702148438, var_grad = 76.02088165283203
round 84: local lr = 0.01, sq_norm_avg_grad = 0.27227646112442017, avg_sq_norm_grad = 75.19338989257812,                  max_norm_grad = 11.72091007232666, var_grad = 74.92111206054688

>>> Round:   85 / Acc: 86.225% / Loss: 0.4733 /Time: 4.11s
======================================================================================================

= Test = round: 85 / acc: 87.390% / loss: 0.4425 / Time: 0.79s
======================================================================================================

round 85: local lr = 0.01, sq_norm_avg_grad = 0.4177685081958771, avg_sq_norm_grad = 74.7095718383789,                  max_norm_grad = 12.252352714538574, var_grad = 74.29180145263672
round 86: local lr = 0.01, sq_norm_avg_grad = 0.45620641112327576, avg_sq_norm_grad = 74.31578826904297,                  max_norm_grad = 12.18814468383789, var_grad = 73.85958099365234
round 87: local lr = 0.01, sq_norm_avg_grad = 0.29677799344062805, avg_sq_norm_grad = 73.74347686767578,                  max_norm_grad = 11.92840576171875, var_grad = 73.44670104980469
round 88: local lr = 0.01, sq_norm_avg_grad = 0.4569490849971771, avg_sq_norm_grad = 73.75312042236328,                  max_norm_grad = 11.945507049560547, var_grad = 73.29617309570312
round 89: local lr = 0.01, sq_norm_avg_grad = 0.4263865649700165, avg_sq_norm_grad = 73.29656219482422,                  max_norm_grad = 12.11620044708252, var_grad = 72.87017822265625

>>> Round:   90 / Acc: 86.712% / Loss: 0.4568 /Time: 3.94s
======================================================================================================

= Test = round: 90 / acc: 87.680% / loss: 0.4260 / Time: 0.75s
======================================================================================================

round 90: local lr = 0.01, sq_norm_avg_grad = 0.3985401690006256, avg_sq_norm_grad = 72.20895385742188,                  max_norm_grad = 11.815681457519531, var_grad = 71.81041717529297
round 91: local lr = 0.01, sq_norm_avg_grad = 0.25891557335853577, avg_sq_norm_grad = 71.14572143554688,                  max_norm_grad = 11.649517059326172, var_grad = 70.88680267333984
round 92: local lr = 0.01, sq_norm_avg_grad = 0.29313257336616516, avg_sq_norm_grad = 70.42401123046875,                  max_norm_grad = 11.651602745056152, var_grad = 70.1308822631836
round 93: local lr = 0.01, sq_norm_avg_grad = 0.2694975435733795, avg_sq_norm_grad = 69.78649139404297,                  max_norm_grad = 11.662142753601074, var_grad = 69.5169906616211
round 94: local lr = 0.01, sq_norm_avg_grad = 0.28914564847946167, avg_sq_norm_grad = 68.334228515625,                  max_norm_grad = 11.369433403015137, var_grad = 68.04508209228516

>>> Round:   95 / Acc: 87.114% / Loss: 0.4419 /Time: 4.06s
======================================================================================================

= Test = round: 95 / acc: 88.320% / loss: 0.4107 / Time: 0.76s
======================================================================================================

round 95: local lr = 0.01, sq_norm_avg_grad = 0.24118511378765106, avg_sq_norm_grad = 68.03254699707031,                  max_norm_grad = 11.110855102539062, var_grad = 67.7913589477539
round 96: local lr = 0.01, sq_norm_avg_grad = 0.3519308865070343, avg_sq_norm_grad = 67.42359161376953,                  max_norm_grad = 11.535314559936523, var_grad = 67.07166290283203
round 97: local lr = 0.01, sq_norm_avg_grad = 0.30762195587158203, avg_sq_norm_grad = 66.98969268798828,                  max_norm_grad = 11.240579605102539, var_grad = 66.68206787109375
round 98: local lr = 0.01, sq_norm_avg_grad = 0.29026472568511963, avg_sq_norm_grad = 66.42647552490234,                  max_norm_grad = 11.273512840270996, var_grad = 66.1362075805664
round 99: local lr = 0.01, sq_norm_avg_grad = 0.3434883952140808, avg_sq_norm_grad = 66.37356567382812,                  max_norm_grad = 11.39535140991211, var_grad = 66.03007507324219

>>> Round:  100 / Acc: 87.500% / Loss: 0.4284 /Time: 4.30s
======================================================================================================

= Test = round: 100 / acc: 88.610% / loss: 0.3980 / Time: 0.83s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.3459, Train_acc: 0.5855, Test_loss: 1.3190, Test_acc: 0.5960
Epoch: 006, Train_loss: 1.1223, Train_acc: 0.6582, Test_loss: 1.1011, Test_acc: 0.6631
Epoch: 011, Train_loss: 1.0982, Train_acc: 0.6667, Test_loss: 1.0794, Test_acc: 0.6709
Epoch: 016, Train_loss: 1.0842, Train_acc: 0.6689, Test_loss: 1.0732, Test_acc: 0.6713
Epoch: 021, Train_loss: 1.0708, Train_acc: 0.6803, Test_loss: 1.0545, Test_acc: 0.6814
Epoch: 026, Train_loss: 1.0742, Train_acc: 0.6729, Test_loss: 1.0604, Test_acc: 0.6750
Epoch: 031, Train_loss: 1.0688, Train_acc: 0.6802, Test_loss: 1.0531, Test_acc: 0.6831
Epoch: 036, Train_loss: 1.0687, Train_acc: 0.6766, Test_loss: 1.0563, Test_acc: 0.6783
Epoch: 041, Train_loss: 1.0647, Train_acc: 0.6807, Test_loss: 1.0501, Test_acc: 0.6858
Epoch: 046, Train_loss: 1.0761, Train_acc: 0.6765, Test_loss: 1.0624, Test_acc: 0.6794
Epoch: 051, Train_loss: 1.0577, Train_acc: 0.6850, Test_loss: 1.0448, Test_acc: 0.6857
Epoch: 056, Train_loss: 1.0685, Train_acc: 0.6825, Test_loss: 1.0546, Test_acc: 0.6879
Epoch: 061, Train_loss: 1.0861, Train_acc: 0.6661, Test_loss: 1.0722, Test_acc: 0.6717
Epoch: 066, Train_loss: 1.0637, Train_acc: 0.6835, Test_loss: 1.0494, Test_acc: 0.6841
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002011516_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002011516_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.0571055576950046, 0.6821070829307978, 1.0463187212136675, 0.6855904899455616]
model_source_only: [2.5666268729098385, 0.401450822867409, 2.5984399883154565, 0.3982890789912232]

************************************************************************************************************************

uid: 20231002020448
FL pretrained model will be saved at ./models/lenet_mnist_20231002020448.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 14.788% / Loss: 2.3056 /Time: 4.54s
======================================================================================================

= Test = round: 0 / acc: 14.820% / loss: 2.3074 / Time: 0.81s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.05763095244765282, avg_sq_norm_grad = 2.783989191055298,                  max_norm_grad = 1.972071886062622, var_grad = 2.72635817527771
round 2: local lr = 0.01, sq_norm_avg_grad = 0.07801486551761627, avg_sq_norm_grad = 4.138524532318115,                  max_norm_grad = 2.442903995513916, var_grad = 4.06050968170166
round 3: local lr = 0.01, sq_norm_avg_grad = 0.12587878108024597, avg_sq_norm_grad = 7.212038993835449,                  max_norm_grad = 3.256333827972412, var_grad = 7.086160182952881
round 4: local lr = 0.01, sq_norm_avg_grad = 0.2784951627254486, avg_sq_norm_grad = 15.842529296875,                  max_norm_grad = 4.834106922149658, var_grad = 15.564034461975098

>>> Round:    5 / Acc: 19.749% / Loss: 2.2810 /Time: 4.18s
======================================================================================================

= Test = round: 5 / acc: 19.890% / loss: 2.2828 / Time: 0.86s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.9445794820785522, avg_sq_norm_grad = 40.49409484863281,                  max_norm_grad = 7.785089015960693, var_grad = 39.54951477050781
round 6: local lr = 0.01, sq_norm_avg_grad = 2.2560267448425293, avg_sq_norm_grad = 90.85816192626953,                  max_norm_grad = 11.593323707580566, var_grad = 88.60213470458984
round 7: local lr = 0.01, sq_norm_avg_grad = 2.9952354431152344, avg_sq_norm_grad = 155.0341339111328,                  max_norm_grad = 15.027485847473145, var_grad = 152.0388946533203
round 8: local lr = 0.01, sq_norm_avg_grad = 2.2604167461395264, avg_sq_norm_grad = 190.0314483642578,                  max_norm_grad = 16.444303512573242, var_grad = 187.77102661132812
round 9: local lr = 0.01, sq_norm_avg_grad = 1.9063009023666382, avg_sq_norm_grad = 194.42483520507812,                  max_norm_grad = 16.471086502075195, var_grad = 192.51853942871094

>>> Round:   10 / Acc: 47.271% / Loss: 2.0515 /Time: 4.20s
======================================================================================================

= Test = round: 10 / acc: 48.770% / loss: 2.0420 / Time: 0.83s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 1.8680752515792847, avg_sq_norm_grad = 194.80531311035156,                  max_norm_grad = 16.445833206176758, var_grad = 192.93724060058594
round 11: local lr = 0.01, sq_norm_avg_grad = 1.984674096107483, avg_sq_norm_grad = 196.54263305664062,                  max_norm_grad = 16.460268020629883, var_grad = 194.55795288085938
round 12: local lr = 0.01, sq_norm_avg_grad = 1.9463754892349243, avg_sq_norm_grad = 198.37530517578125,                  max_norm_grad = 16.47482681274414, var_grad = 196.42892456054688
round 13: local lr = 0.01, sq_norm_avg_grad = 2.0952205657958984, avg_sq_norm_grad = 193.3684539794922,                  max_norm_grad = 16.445283889770508, var_grad = 191.2732391357422
round 14: local lr = 0.01, sq_norm_avg_grad = 2.266941547393799, avg_sq_norm_grad = 198.17733764648438,                  max_norm_grad = 16.52999496459961, var_grad = 195.910400390625

>>> Round:   15 / Acc: 64.253% / Loss: 1.7656 /Time: 4.20s
======================================================================================================

= Test = round: 15 / acc: 66.300% / loss: 1.7423 / Time: 0.91s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 2.427241325378418, avg_sq_norm_grad = 196.80601501464844,                  max_norm_grad = 16.520410537719727, var_grad = 194.37876892089844
round 16: local lr = 0.01, sq_norm_avg_grad = 2.2699661254882812, avg_sq_norm_grad = 197.34664916992188,                  max_norm_grad = 16.268552780151367, var_grad = 195.07669067382812
round 17: local lr = 0.01, sq_norm_avg_grad = 2.6361753940582275, avg_sq_norm_grad = 194.67257690429688,                  max_norm_grad = 16.280475616455078, var_grad = 192.03640747070312
round 18: local lr = 0.01, sq_norm_avg_grad = 2.5185585021972656, avg_sq_norm_grad = 193.3731231689453,                  max_norm_grad = 16.256282806396484, var_grad = 190.8545684814453
round 19: local lr = 0.01, sq_norm_avg_grad = 2.3800504207611084, avg_sq_norm_grad = 196.5001678466797,                  max_norm_grad = 16.327476501464844, var_grad = 194.1201171875

>>> Round:   20 / Acc: 70.031% / Loss: 1.4416 /Time: 4.03s
======================================================================================================

= Test = round: 20 / acc: 72.920% / loss: 1.4060 / Time: 0.74s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 2.385146141052246, avg_sq_norm_grad = 190.12921142578125,                  max_norm_grad = 15.925710678100586, var_grad = 187.7440643310547
round 21: local lr = 0.01, sq_norm_avg_grad = 2.553814172744751, avg_sq_norm_grad = 190.69827270507812,                  max_norm_grad = 16.38252830505371, var_grad = 188.1444549560547
round 22: local lr = 0.01, sq_norm_avg_grad = 2.432220935821533, avg_sq_norm_grad = 188.2669219970703,                  max_norm_grad = 16.232297897338867, var_grad = 185.83470153808594
round 23: local lr = 0.01, sq_norm_avg_grad = 2.0123085975646973, avg_sq_norm_grad = 184.00656127929688,                  max_norm_grad = 16.175146102905273, var_grad = 181.99424743652344
round 24: local lr = 0.01, sq_norm_avg_grad = 2.2083117961883545, avg_sq_norm_grad = 179.63629150390625,                  max_norm_grad = 16.239566802978516, var_grad = 177.427978515625

>>> Round:   25 / Acc: 73.738% / Loss: 1.1717 /Time: 4.07s
======================================================================================================

= Test = round: 25 / acc: 75.600% / loss: 1.1310 / Time: 0.78s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 2.0749311447143555, avg_sq_norm_grad = 181.08135986328125,                  max_norm_grad = 16.42996597290039, var_grad = 179.0064239501953
round 26: local lr = 0.01, sq_norm_avg_grad = 2.035437822341919, avg_sq_norm_grad = 176.01499938964844,                  max_norm_grad = 16.566009521484375, var_grad = 173.9795684814453
round 27: local lr = 0.01, sq_norm_avg_grad = 2.062056303024292, avg_sq_norm_grad = 175.01019287109375,                  max_norm_grad = 16.740144729614258, var_grad = 172.94813537597656
round 28: local lr = 0.01, sq_norm_avg_grad = 1.9116557836532593, avg_sq_norm_grad = 167.28968811035156,                  max_norm_grad = 16.315412521362305, var_grad = 165.37803649902344
round 29: local lr = 0.01, sq_norm_avg_grad = 1.7008861303329468, avg_sq_norm_grad = 165.6510772705078,                  max_norm_grad = 16.149377822875977, var_grad = 163.9501953125

>>> Round:   30 / Acc: 76.125% / Loss: 0.9906 /Time: 3.95s
======================================================================================================

= Test = round: 30 / acc: 78.330% / loss: 0.9489 / Time: 0.76s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 1.5873758792877197, avg_sq_norm_grad = 158.78414916992188,                  max_norm_grad = 16.200788497924805, var_grad = 157.19677734375
round 31: local lr = 0.01, sq_norm_avg_grad = 1.5677295923233032, avg_sq_norm_grad = 157.57888793945312,                  max_norm_grad = 16.077404022216797, var_grad = 156.0111541748047
round 32: local lr = 0.01, sq_norm_avg_grad = 1.5780754089355469, avg_sq_norm_grad = 154.75369262695312,                  max_norm_grad = 16.23179817199707, var_grad = 153.1756134033203
round 33: local lr = 0.01, sq_norm_avg_grad = 1.7850459814071655, avg_sq_norm_grad = 151.99058532714844,                  max_norm_grad = 16.483110427856445, var_grad = 150.20553588867188
round 34: local lr = 0.01, sq_norm_avg_grad = 1.6710164546966553, avg_sq_norm_grad = 152.12338256835938,                  max_norm_grad = 16.334239959716797, var_grad = 150.45236206054688

>>> Round:   35 / Acc: 78.028% / Loss: 0.8599 /Time: 4.02s
======================================================================================================

= Test = round: 35 / acc: 80.140% / loss: 0.8208 / Time: 0.75s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 1.4423425197601318, avg_sq_norm_grad = 148.00326538085938,                  max_norm_grad = 16.280160903930664, var_grad = 146.56092834472656
round 36: local lr = 0.01, sq_norm_avg_grad = 1.3946300745010376, avg_sq_norm_grad = 144.6597442626953,                  max_norm_grad = 16.250751495361328, var_grad = 143.26512145996094
round 37: local lr = 0.01, sq_norm_avg_grad = 1.1508362293243408, avg_sq_norm_grad = 141.40003967285156,                  max_norm_grad = 15.696234703063965, var_grad = 140.24920654296875
round 38: local lr = 0.01, sq_norm_avg_grad = 1.3354382514953613, avg_sq_norm_grad = 139.83465576171875,                  max_norm_grad = 16.061281204223633, var_grad = 138.4992218017578
round 39: local lr = 0.01, sq_norm_avg_grad = 1.302991271018982, avg_sq_norm_grad = 135.04788208007812,                  max_norm_grad = 15.904618263244629, var_grad = 133.74488830566406

>>> Round:   40 / Acc: 79.686% / Loss: 0.7696 /Time: 4.06s
======================================================================================================

= Test = round: 40 / acc: 81.640% / loss: 0.7323 / Time: 0.78s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 1.0422178506851196, avg_sq_norm_grad = 134.72808837890625,                  max_norm_grad = 15.568652153015137, var_grad = 133.6858673095703
round 41: local lr = 0.01, sq_norm_avg_grad = 1.031330943107605, avg_sq_norm_grad = 130.6082763671875,                  max_norm_grad = 15.741853713989258, var_grad = 129.5769500732422
round 42: local lr = 0.01, sq_norm_avg_grad = 1.1686503887176514, avg_sq_norm_grad = 129.6942901611328,                  max_norm_grad = 15.245965003967285, var_grad = 128.525634765625
round 43: local lr = 0.01, sq_norm_avg_grad = 1.2001653909683228, avg_sq_norm_grad = 128.5699005126953,                  max_norm_grad = 15.486745834350586, var_grad = 127.36973571777344
round 44: local lr = 0.01, sq_norm_avg_grad = 0.8633058667182922, avg_sq_norm_grad = 126.8010025024414,                  max_norm_grad = 14.864171028137207, var_grad = 125.93769836425781

>>> Round:   45 / Acc: 80.773% / Loss: 0.7049 /Time: 4.00s
======================================================================================================

= Test = round: 45 / acc: 82.540% / loss: 0.6697 / Time: 0.77s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 1.025713562965393, avg_sq_norm_grad = 124.5263671875,                  max_norm_grad = 15.40569019317627, var_grad = 123.50065612792969
round 46: local lr = 0.01, sq_norm_avg_grad = 1.0541259050369263, avg_sq_norm_grad = 121.2100830078125,                  max_norm_grad = 14.867227554321289, var_grad = 120.15596008300781
round 47: local lr = 0.01, sq_norm_avg_grad = 0.9155664443969727, avg_sq_norm_grad = 118.03115844726562,                  max_norm_grad = 14.876899719238281, var_grad = 117.11559295654297
round 48: local lr = 0.01, sq_norm_avg_grad = 0.9674955010414124, avg_sq_norm_grad = 116.59073638916016,                  max_norm_grad = 14.952503204345703, var_grad = 115.62323760986328
round 49: local lr = 0.01, sq_norm_avg_grad = 0.816900372505188, avg_sq_norm_grad = 115.21931457519531,                  max_norm_grad = 14.817037582397461, var_grad = 114.40241241455078

>>> Round:   50 / Acc: 81.943% / Loss: 0.6534 /Time: 4.24s
======================================================================================================

= Test = round: 50 / acc: 83.430% / loss: 0.6192 / Time: 0.80s
======================================================================================================

round 50: local lr = 0.01, sq_norm_avg_grad = 0.6914328932762146, avg_sq_norm_grad = 114.52175903320312,                  max_norm_grad = 14.574172019958496, var_grad = 113.83032989501953
round 51: local lr = 0.01, sq_norm_avg_grad = 0.6381529569625854, avg_sq_norm_grad = 112.96707153320312,                  max_norm_grad = 14.364476203918457, var_grad = 112.32891845703125
round 52: local lr = 0.01, sq_norm_avg_grad = 0.6697119474411011, avg_sq_norm_grad = 111.2828598022461,                  max_norm_grad = 14.612201690673828, var_grad = 110.61315155029297
round 53: local lr = 0.01, sq_norm_avg_grad = 0.7358707785606384, avg_sq_norm_grad = 110.33255767822266,                  max_norm_grad = 14.479260444641113, var_grad = 109.59668731689453
round 54: local lr = 0.01, sq_norm_avg_grad = 0.9682777523994446, avg_sq_norm_grad = 108.42902374267578,                  max_norm_grad = 14.700735092163086, var_grad = 107.46074676513672

>>> Round:   55 / Acc: 82.625% / Loss: 0.6167 /Time: 4.12s
======================================================================================================

= Test = round: 55 / acc: 83.860% / loss: 0.5838 / Time: 0.80s
======================================================================================================

round 55: local lr = 0.01, sq_norm_avg_grad = 0.9455724358558655, avg_sq_norm_grad = 106.71517181396484,                  max_norm_grad = 14.368797302246094, var_grad = 105.76959991455078
round 56: local lr = 0.01, sq_norm_avg_grad = 0.6687227487564087, avg_sq_norm_grad = 104.87849426269531,                  max_norm_grad = 13.983272552490234, var_grad = 104.20977020263672
round 57: local lr = 0.01, sq_norm_avg_grad = 0.7360069751739502, avg_sq_norm_grad = 104.99588775634766,                  max_norm_grad = 13.976200103759766, var_grad = 104.25988006591797
round 58: local lr = 0.01, sq_norm_avg_grad = 0.5263044834136963, avg_sq_norm_grad = 102.27497863769531,                  max_norm_grad = 13.701799392700195, var_grad = 101.74867248535156
round 59: local lr = 0.01, sq_norm_avg_grad = 0.613272488117218, avg_sq_norm_grad = 102.03153991699219,                  max_norm_grad = 13.810301780700684, var_grad = 101.41826629638672

>>> Round:   60 / Acc: 83.537% / Loss: 0.5810 /Time: 4.01s
======================================================================================================

= Test = round: 60 / acc: 84.830% / loss: 0.5478 / Time: 0.77s
======================================================================================================

round 60: local lr = 0.01, sq_norm_avg_grad = 0.6414492130279541, avg_sq_norm_grad = 100.4590835571289,                  max_norm_grad = 13.573575973510742, var_grad = 99.81763458251953
round 61: local lr = 0.01, sq_norm_avg_grad = 0.8214309215545654, avg_sq_norm_grad = 98.31320190429688,                  max_norm_grad = 13.793065071105957, var_grad = 97.49176788330078
round 62: local lr = 0.01, sq_norm_avg_grad = 0.5327903628349304, avg_sq_norm_grad = 97.02996063232422,                  max_norm_grad = 13.343809127807617, var_grad = 96.4971694946289
round 63: local lr = 0.01, sq_norm_avg_grad = 0.6528787016868591, avg_sq_norm_grad = 95.08609008789062,                  max_norm_grad = 13.616592407226562, var_grad = 94.43321228027344
round 64: local lr = 0.01, sq_norm_avg_grad = 0.6082372665405273, avg_sq_norm_grad = 95.25299835205078,                  max_norm_grad = 13.437036514282227, var_grad = 94.64476013183594

>>> Round:   65 / Acc: 84.210% / Loss: 0.5537 /Time: 4.04s
======================================================================================================

= Test = round: 65 / acc: 85.500% / loss: 0.5212 / Time: 0.76s
======================================================================================================

round 65: local lr = 0.01, sq_norm_avg_grad = 0.5633617043495178, avg_sq_norm_grad = 93.7441177368164,                  max_norm_grad = 13.314294815063477, var_grad = 93.18075561523438
round 66: local lr = 0.01, sq_norm_avg_grad = 0.36464887857437134, avg_sq_norm_grad = 92.19419860839844,                  max_norm_grad = 12.957676887512207, var_grad = 91.82955169677734
round 67: local lr = 0.01, sq_norm_avg_grad = 0.37766268849372864, avg_sq_norm_grad = 91.61941528320312,                  max_norm_grad = 12.89625072479248, var_grad = 91.24175262451172
round 68: local lr = 0.01, sq_norm_avg_grad = 0.41915959119796753, avg_sq_norm_grad = 89.5726318359375,                  max_norm_grad = 13.0684814453125, var_grad = 89.15347290039062
round 69: local lr = 0.01, sq_norm_avg_grad = 0.44665083289146423, avg_sq_norm_grad = 90.14681243896484,                  max_norm_grad = 12.925546646118164, var_grad = 89.70016479492188

>>> Round:   70 / Acc: 84.910% / Loss: 0.5282 /Time: 4.04s
======================================================================================================

= Test = round: 70 / acc: 86.230% / loss: 0.4956 / Time: 0.75s
======================================================================================================

round 70: local lr = 0.01, sq_norm_avg_grad = 0.39629054069519043, avg_sq_norm_grad = 88.7725830078125,                  max_norm_grad = 13.000000953674316, var_grad = 88.37628936767578
round 71: local lr = 0.01, sq_norm_avg_grad = 0.3496445119380951, avg_sq_norm_grad = 87.44956970214844,                  max_norm_grad = 12.802361488342285, var_grad = 87.09992218017578
round 72: local lr = 0.01, sq_norm_avg_grad = 0.37373992800712585, avg_sq_norm_grad = 86.50128936767578,                  max_norm_grad = 12.289410591125488, var_grad = 86.12754821777344
round 73: local lr = 0.01, sq_norm_avg_grad = 0.4136550724506378, avg_sq_norm_grad = 85.21404266357422,                  max_norm_grad = 12.42634391784668, var_grad = 84.80038452148438
round 74: local lr = 0.01, sq_norm_avg_grad = 0.39009663462638855, avg_sq_norm_grad = 84.11253356933594,                  max_norm_grad = 12.585539817810059, var_grad = 83.7224349975586

>>> Round:   75 / Acc: 85.445% / Loss: 0.5085 /Time: 3.96s
======================================================================================================

= Test = round: 75 / acc: 86.580% / loss: 0.4768 / Time: 0.77s
======================================================================================================

round 75: local lr = 0.01, sq_norm_avg_grad = 0.44867977499961853, avg_sq_norm_grad = 83.52704620361328,                  max_norm_grad = 12.689253807067871, var_grad = 83.078369140625
round 76: local lr = 0.01, sq_norm_avg_grad = 0.34455859661102295, avg_sq_norm_grad = 81.91573333740234,                  max_norm_grad = 12.365346908569336, var_grad = 81.57117462158203
round 77: local lr = 0.01, sq_norm_avg_grad = 0.37100496888160706, avg_sq_norm_grad = 81.69786071777344,                  max_norm_grad = 12.36207103729248, var_grad = 81.32685852050781
round 78: local lr = 0.01, sq_norm_avg_grad = 0.3223930299282074, avg_sq_norm_grad = 80.40308380126953,                  max_norm_grad = 12.101154327392578, var_grad = 80.0806884765625
round 79: local lr = 0.01, sq_norm_avg_grad = 0.4827529489994049, avg_sq_norm_grad = 80.25381469726562,                  max_norm_grad = 12.509668350219727, var_grad = 79.77106475830078

>>> Round:   80 / Acc: 85.843% / Loss: 0.4904 /Time: 3.93s
======================================================================================================

= Test = round: 80 / acc: 86.990% / loss: 0.4589 / Time: 0.77s
======================================================================================================

round 80: local lr = 0.01, sq_norm_avg_grad = 0.4815528690814972, avg_sq_norm_grad = 79.04774475097656,                  max_norm_grad = 12.243745803833008, var_grad = 78.56619262695312
round 81: local lr = 0.01, sq_norm_avg_grad = 0.40696993470191956, avg_sq_norm_grad = 78.62667846679688,                  max_norm_grad = 12.127652168273926, var_grad = 78.21971130371094
round 82: local lr = 0.01, sq_norm_avg_grad = 0.34758320450782776, avg_sq_norm_grad = 77.82454681396484,                  max_norm_grad = 12.164077758789062, var_grad = 77.47696685791016
round 83: local lr = 0.01, sq_norm_avg_grad = 0.3009200096130371, avg_sq_norm_grad = 77.11406707763672,                  max_norm_grad = 12.110503196716309, var_grad = 76.81314849853516
round 84: local lr = 0.01, sq_norm_avg_grad = 0.2612718343734741, avg_sq_norm_grad = 75.93024444580078,                  max_norm_grad = 11.979239463806152, var_grad = 75.66897583007812

>>> Round:   85 / Acc: 86.315% / Loss: 0.4719 /Time: 4.07s
======================================================================================================

= Test = round: 85 / acc: 87.410% / loss: 0.4409 / Time: 0.75s
======================================================================================================

round 85: local lr = 0.01, sq_norm_avg_grad = 0.3204996585845947, avg_sq_norm_grad = 75.35125732421875,                  max_norm_grad = 12.052733421325684, var_grad = 75.03075408935547
round 86: local lr = 0.01, sq_norm_avg_grad = 0.48381438851356506, avg_sq_norm_grad = 74.34269714355469,                  max_norm_grad = 12.140721321105957, var_grad = 73.85887908935547
round 87: local lr = 0.01, sq_norm_avg_grad = 0.3236900568008423, avg_sq_norm_grad = 73.6900405883789,                  max_norm_grad = 11.853011131286621, var_grad = 73.36634826660156
round 88: local lr = 0.01, sq_norm_avg_grad = 0.31900134682655334, avg_sq_norm_grad = 73.40516662597656,                  max_norm_grad = 11.9692964553833, var_grad = 73.08616638183594
round 89: local lr = 0.01, sq_norm_avg_grad = 0.2803211510181427, avg_sq_norm_grad = 72.714599609375,                  max_norm_grad = 11.815431594848633, var_grad = 72.43428039550781

>>> Round:   90 / Acc: 86.768% / Loss: 0.4558 /Time: 4.04s
======================================================================================================

= Test = round: 90 / acc: 87.840% / loss: 0.4250 / Time: 0.79s
======================================================================================================

round 90: local lr = 0.01, sq_norm_avg_grad = 0.310213178396225, avg_sq_norm_grad = 72.49700927734375,                  max_norm_grad = 11.713735580444336, var_grad = 72.18679809570312
round 91: local lr = 0.01, sq_norm_avg_grad = 0.20221450924873352, avg_sq_norm_grad = 70.16787719726562,                  max_norm_grad = 11.20245361328125, var_grad = 69.96566009521484
round 92: local lr = 0.01, sq_norm_avg_grad = 0.2625904977321625, avg_sq_norm_grad = 70.24504089355469,                  max_norm_grad = 11.272319793701172, var_grad = 69.98245239257812
round 93: local lr = 0.01, sq_norm_avg_grad = 0.2889929711818695, avg_sq_norm_grad = 69.81068420410156,                  max_norm_grad = 11.47778034210205, var_grad = 69.52169036865234
round 94: local lr = 0.01, sq_norm_avg_grad = 0.2825894355773926, avg_sq_norm_grad = 69.8187026977539,                  max_norm_grad = 11.535480499267578, var_grad = 69.5361099243164

>>> Round:   95 / Acc: 87.081% / Loss: 0.4412 /Time: 3.95s
======================================================================================================

= Test = round: 95 / acc: 88.130% / loss: 0.4106 / Time: 0.76s
======================================================================================================

round 95: local lr = 0.01, sq_norm_avg_grad = 0.24025112390518188, avg_sq_norm_grad = 68.84400177001953,                  max_norm_grad = 11.497187614440918, var_grad = 68.60375213623047
round 96: local lr = 0.01, sq_norm_avg_grad = 0.3536592721939087, avg_sq_norm_grad = 68.2199478149414,                  max_norm_grad = 11.361959457397461, var_grad = 67.86628723144531
round 97: local lr = 0.01, sq_norm_avg_grad = 0.3580004572868347, avg_sq_norm_grad = 67.9656753540039,                  max_norm_grad = 11.138688087463379, var_grad = 67.60767364501953
round 98: local lr = 0.01, sq_norm_avg_grad = 0.24819979071617126, avg_sq_norm_grad = 66.91333770751953,                  max_norm_grad = 11.40471076965332, var_grad = 66.6651382446289
round 99: local lr = 0.01, sq_norm_avg_grad = 0.2649330198764801, avg_sq_norm_grad = 67.23649597167969,                  max_norm_grad = 11.240368843078613, var_grad = 66.97156524658203

>>> Round:  100 / Acc: 87.470% / Loss: 0.4283 /Time: 4.08s
======================================================================================================

= Test = round: 100 / acc: 88.540% / loss: 0.3982 / Time: 0.78s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.3275, Train_acc: 0.5909, Test_loss: 1.2997, Test_acc: 0.5984
Epoch: 006, Train_loss: 1.1198, Train_acc: 0.6579, Test_loss: 1.1028, Test_acc: 0.6658
Epoch: 011, Train_loss: 1.0967, Train_acc: 0.6702, Test_loss: 1.0805, Test_acc: 0.6727
Epoch: 016, Train_loss: 1.0745, Train_acc: 0.6782, Test_loss: 1.0621, Test_acc: 0.6820
Epoch: 021, Train_loss: 1.0693, Train_acc: 0.6782, Test_loss: 1.0563, Test_acc: 0.6810
Epoch: 026, Train_loss: 1.0780, Train_acc: 0.6799, Test_loss: 1.0649, Test_acc: 0.6797
Epoch: 031, Train_loss: 1.0713, Train_acc: 0.6757, Test_loss: 1.0586, Test_acc: 0.6770
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20231002020448_model_ft.pt.
Model saved at ./models/ft_checkpoints/20231002020448_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [1.0664534797119054, 0.6814969237809528, 1.0511110067076186, 0.6853682924119542]
model_source_only: [2.5719464134922276, 0.40217962407416824, 2.6039333508526163, 0.39917786912565273]
fl_test_acc_mean 0.88538
model_source_only_test_acc_mean 0.3991334296189312
model_ft_test_acc_mean 0.6845683812909678
