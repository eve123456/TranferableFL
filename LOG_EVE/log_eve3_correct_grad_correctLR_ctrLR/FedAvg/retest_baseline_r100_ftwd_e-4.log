nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 100
	        num_round : 200
	           opt_lr : False
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.0
	  reg_J_norm_coef : 0.0
	          reg_max : False
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928194134
FL pretrained model will be saved at ./models/lenet_mnist_20230928194134.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 

nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 100
	           opt_lr : False
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.0
	  reg_J_norm_coef : 0.0
	          reg_max : False
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230928194205
FL pretrained model will be saved at ./models/lenet_mnist_20230928194205.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 7.555% / Loss: 2.3110 /Time: 4.39s
======================================================================================================

= Test = round: 0 / acc: 7.220% / loss: 2.3106 / Time: 0.90s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.01, sq_norm_avg_grad = 0.031813718378543854, avg_sq_norm_grad = 1.5293020009994507,                  max_norm_grad = 1.4434781074523926, var_grad = 1.497488260269165
round 2: local lr = 0.01, sq_norm_avg_grad = 0.036940883845090866, avg_sq_norm_grad = 1.7603068351745605,                  max_norm_grad = 1.5495107173919678, var_grad = 1.7233659029006958
round 3: local lr = 0.01, sq_norm_avg_grad = 0.04491482675075531, avg_sq_norm_grad = 2.118818521499634,                  max_norm_grad = 1.6994376182556152, var_grad = 2.073903799057007
round 4: local lr = 0.01, sq_norm_avg_grad = 0.057994384318590164, avg_sq_norm_grad = 2.730940580368042,                  max_norm_grad = 1.9239814281463623, var_grad = 2.6729462146759033

>>> Round:    5 / Acc: 13.817% / Loss: 2.2928 /Time: 4.41s
======================================================================================================

= Test = round: 5 / acc: 13.610% / loss: 2.2904 / Time: 0.87s
======================================================================================================

round 5: local lr = 0.01, sq_norm_avg_grad = 0.07524330914020538, avg_sq_norm_grad = 3.876897096633911,                  max_norm_grad = 2.2692527770996094, var_grad = 3.8016538619995117
round 6: local lr = 0.01, sq_norm_avg_grad = 0.10254811495542526, avg_sq_norm_grad = 6.204899787902832,                  max_norm_grad = 2.8780734539031982, var_grad = 6.102351665496826
round 7: local lr = 0.01, sq_norm_avg_grad = 0.1845586895942688, avg_sq_norm_grad = 11.83691120147705,                  max_norm_grad = 3.997922897338867, var_grad = 11.652352333068848
round 8: local lr = 0.01, sq_norm_avg_grad = 0.5114285945892334, avg_sq_norm_grad = 27.363603591918945,                  max_norm_grad = 6.139095306396484, var_grad = 26.852174758911133
round 9: local lr = 0.01, sq_norm_avg_grad = 1.4888756275177002, avg_sq_norm_grad = 65.02835083007812,                  max_norm_grad = 9.731857299804688, var_grad = 63.53947448730469

>>> Round:   10 / Acc: 18.050% / Loss: 2.2087 /Time: 4.41s
======================================================================================================

= Test = round: 10 / acc: 19.330% / loss: 2.1970 / Time: 0.86s
======================================================================================================

round 10: local lr = 0.01, sq_norm_avg_grad = 2.5089468955993652, avg_sq_norm_grad = 124.3580551147461,                  max_norm_grad = 13.680970191955566, var_grad = 121.84910583496094
round 11: local lr = 0.01, sq_norm_avg_grad = 2.5374646186828613, avg_sq_norm_grad = 177.69903564453125,                  max_norm_grad = 16.268564224243164, var_grad = 175.1615753173828
round 12: local lr = 0.01, sq_norm_avg_grad = 2.0264904499053955, avg_sq_norm_grad = 191.59304809570312,                  max_norm_grad = 16.777294158935547, var_grad = 189.56655883789062
round 13: local lr = 0.01, sq_norm_avg_grad = 1.9160455465316772, avg_sq_norm_grad = 193.53089904785156,                  max_norm_grad = 16.816078186035156, var_grad = 191.61485290527344
round 14: local lr = 0.01, sq_norm_avg_grad = 1.6982585191726685, avg_sq_norm_grad = 193.24073791503906,                  max_norm_grad = 16.66997528076172, var_grad = 191.54248046875

>>> Round:   15 / Acc: 54.932% / Loss: 1.9507 /Time: 4.45s
======================================================================================================

= Test = round: 15 / acc: 57.510% / loss: 1.9359 / Time: 0.91s
======================================================================================================

round 15: local lr = 0.01, sq_norm_avg_grad = 1.5409940481185913, avg_sq_norm_grad = 193.415771484375,                  max_norm_grad = 16.47882843017578, var_grad = 191.87477111816406
round 16: local lr = 0.01, sq_norm_avg_grad = 1.7060872316360474, avg_sq_norm_grad = 193.5401153564453,                  max_norm_grad = 16.44920539855957, var_grad = 191.8340301513672
round 17: local lr = 0.01, sq_norm_avg_grad = 1.943916916847229, avg_sq_norm_grad = 191.196533203125,                  max_norm_grad = 16.313304901123047, var_grad = 189.2526092529297
round 18: local lr = 0.01, sq_norm_avg_grad = 2.261859893798828, avg_sq_norm_grad = 192.86753845214844,                  max_norm_grad = 16.500858306884766, var_grad = 190.60568237304688
round 19: local lr = 0.01, sq_norm_avg_grad = 2.232548713684082, avg_sq_norm_grad = 191.7625732421875,                  max_norm_grad = 16.708938598632812, var_grad = 189.530029296875

>>> Round:   20 / Acc: 61.546% / Loss: 1.6677 /Time: 4.36s
======================================================================================================

= Test = round: 20 / acc: 64.470% / loss: 1.6414 / Time: 0.84s
======================================================================================================

round 20: local lr = 0.01, sq_norm_avg_grad = 2.3391335010528564, avg_sq_norm_grad = 195.53732299804688,                  max_norm_grad = 16.854652404785156, var_grad = 193.1981964111328
round 21: local lr = 0.01, sq_norm_avg_grad = 2.003891706466675, avg_sq_norm_grad = 191.64002990722656,                  max_norm_grad = 16.73310089111328, var_grad = 189.63613891601562
round 22: local lr = 0.01, sq_norm_avg_grad = 2.2390806674957275, avg_sq_norm_grad = 193.81314086914062,                  max_norm_grad = 17.100799560546875, var_grad = 191.57406616210938
round 23: local lr = 0.01, sq_norm_avg_grad = 1.9766879081726074, avg_sq_norm_grad = 190.35452270507812,                  max_norm_grad = 16.86257553100586, var_grad = 188.37783813476562
round 24: local lr = 0.01, sq_norm_avg_grad = 2.0287394523620605, avg_sq_norm_grad = 188.00741577148438,                  max_norm_grad = 16.850540161132812, var_grad = 185.9786834716797

>>> Round:   25 / Acc: 67.229% / Loss: 1.3797 /Time: 4.79s
======================================================================================================

= Test = round: 25 / acc: 70.110% / loss: 1.3439 / Time: 0.91s
======================================================================================================

round 25: local lr = 0.01, sq_norm_avg_grad = 2.156170129776001, avg_sq_norm_grad = 183.35989379882812,                  max_norm_grad = 16.977264404296875, var_grad = 181.20372009277344
round 26: local lr = 0.01, sq_norm_avg_grad = 2.032526969909668, avg_sq_norm_grad = 182.86187744140625,                  max_norm_grad = 16.923532485961914, var_grad = 180.829345703125
round 27: local lr = 0.01, sq_norm_avg_grad = 1.9643151760101318, avg_sq_norm_grad = 179.7946014404297,                  max_norm_grad = 16.804119110107422, var_grad = 177.83029174804688
round 28: local lr = 0.01, sq_norm_avg_grad = 1.9262012243270874, avg_sq_norm_grad = 175.32647705078125,                  max_norm_grad = 16.76655387878418, var_grad = 173.4002685546875
round 29: local lr = 0.01, sq_norm_avg_grad = 2.0479984283447266, avg_sq_norm_grad = 175.70407104492188,                  max_norm_grad = 16.930456161499023, var_grad = 173.65606689453125

>>> Round:   30 / Acc: 71.496% / Loss: 1.1465 /Time: 4.55s
======================================================================================================

= Test = round: 30 / acc: 73.960% / loss: 1.1075 / Time: 0.85s
======================================================================================================

round 30: local lr = 0.01, sq_norm_avg_grad = 1.986930251121521, avg_sq_norm_grad = 173.89707946777344,                  max_norm_grad = 16.73472023010254, var_grad = 171.91015625
round 31: local lr = 0.01, sq_norm_avg_grad = 2.035522937774658, avg_sq_norm_grad = 172.72593688964844,                  max_norm_grad = 16.794872283935547, var_grad = 170.69041442871094
round 32: local lr = 0.01, sq_norm_avg_grad = 2.273817777633667, avg_sq_norm_grad = 170.4275360107422,                  max_norm_grad = 16.69668197631836, var_grad = 168.15371704101562
round 33: local lr = 0.01, sq_norm_avg_grad = 1.9131038188934326, avg_sq_norm_grad = 167.82659912109375,                  max_norm_grad = 16.529407501220703, var_grad = 165.9134979248047
round 34: local lr = 0.01, sq_norm_avg_grad = 1.774026870727539, avg_sq_norm_grad = 163.22921752929688,                  max_norm_grad = 16.265918731689453, var_grad = 161.45518493652344

>>> Round:   35 / Acc: 74.611% / Loss: 0.9813 /Time: 5.05s
======================================================================================================

= Test = round: 35 / acc: 76.710% / loss: 0.9443 / Time: 0.82s
======================================================================================================

round 35: local lr = 0.01, sq_norm_avg_grad = 1.6840418577194214, avg_sq_norm_grad = 162.1127166748047,                  max_norm_grad = 16.316600799560547, var_grad = 160.42868041992188
round 36: local lr = 0.01, sq_norm_avg_grad = 1.8533233404159546, avg_sq_norm_grad = 158.4903106689453,                  max_norm_grad = 16.298664093017578, var_grad = 156.63699340820312
round 37: local lr = 0.01, sq_norm_avg_grad = 1.6439719200134277, avg_sq_norm_grad = 156.47105407714844,                  max_norm_grad = 15.972298622131348, var_grad = 154.82708740234375
round 38: local lr = 0.01, sq_norm_avg_grad = 1.3900424242019653, avg_sq_norm_grad = 156.7591094970703,                  max_norm_grad = 16.064828872680664, var_grad = 155.3690643310547
round 39: local lr = 0.01, sq_norm_avg_grad = 1.514107584953308, avg_sq_norm_grad = 152.768798828125,                  max_norm_grad = 15.846543312072754, var_grad = 151.2546844482422

>>> Round:   40 / Acc: 76.437% / Loss: 0.8692 /Time: 5.14s
======================================================================================================

= Test = round: 40 / acc: 78.300% / loss: 0.8331 / Time: 1.26s
======================================================================================================

round 40: local lr = 0.01, sq_norm_avg_grad = 1.7952297925949097, avg_sq_norm_grad = 149.8747100830078,                  max_norm_grad = 16.013080596923828, var_grad = 148.07948303222656
round 41: local lr = 0.01, sq_norm_avg_grad = 1.2558485269546509, avg_sq_norm_grad = 146.47691345214844,                  max_norm_grad = 15.413225173950195, var_grad = 145.2210693359375
round 42: local lr = 0.01, sq_norm_avg_grad = 1.4407867193222046, avg_sq_norm_grad = 144.83779907226562,                  max_norm_grad = 15.403614044189453, var_grad = 143.3970184326172
round 43: local lr = 0.01, sq_norm_avg_grad = 1.567919135093689, avg_sq_norm_grad = 144.13070678710938,                  max_norm_grad = 15.83447265625, var_grad = 142.5627899169922
round 44: local lr = 0.01, sq_norm_avg_grad = 1.47198486328125, avg_sq_norm_grad = 141.65884399414062,                  max_norm_grad = 15.615283012390137, var_grad = 140.18685913085938

>>> Round:   45 / Acc: 78.253% / Loss: 0.7842 /Time: 4.69s
======================================================================================================

= Test = round: 45 / acc: 79.650% / loss: 0.7495 / Time: 0.92s
======================================================================================================

round 45: local lr = 0.01, sq_norm_avg_grad = 1.427122950553894, avg_sq_norm_grad = 139.69482421875,                  max_norm_grad = 15.235102653503418, var_grad = 138.2677001953125
round 46: local lr = 0.01, sq_norm_avg_grad = 1.4213441610336304, avg_sq_norm_grad = 137.74679565429688,                  max_norm_grad = 15.372598648071289, var_grad = 136.32545471191406
round 47: local lr = 0.01, sq_norm_avg_grad = 1.226880431175232, avg_sq_norm_grad = 136.01278686523438,                  max_norm_grad = 15.099507331848145, var_grad = 134.78590393066406
round 48: local lr = 0.01, sq_norm_avg_grad = 1.4086254835128784, avg_sq_norm_grad = 130.8807830810547,                  max_norm_grad = 14.910552978515625, var_grad = 129.47215270996094
round 49: local lr = 0.01, sq_norm_avg_grad = 1.240708827972412, avg_sq_norm_grad = 127.37177276611328,                  max_norm_grad = 14.40960693359375, var_grad = 126.13106536865234

>>> Round:   50 / Acc: 79.688% / Loss: 0.7221 /Time: 4.89s
======================================================================================================

= Test = round: 50 / acc: 81.050% / loss: 0.6876 / Time: 0.90s
======================================================================================================

round 50: local lr = 0.01, sq_norm_avg_grad = 0.8144344687461853, avg_sq_norm_grad = 125.55986022949219,                  max_norm_grad = 13.927140235900879, var_grad = 124.74542236328125
round 51: local lr = 0.01, sq_norm_avg_grad = 0.9117510914802551, avg_sq_norm_grad = 125.17802429199219,                  max_norm_grad = 14.040446281433105, var_grad = 124.26627349853516
round 52: local lr = 0.01, sq_norm_avg_grad = 1.0135918855667114, avg_sq_norm_grad = 123.19458770751953,                  max_norm_grad = 14.242921829223633, var_grad = 122.18099212646484
round 53: local lr = 0.01, sq_norm_avg_grad = 0.9076644778251648, avg_sq_norm_grad = 121.46817779541016,                  max_norm_grad = 14.221307754516602, var_grad = 120.56051635742188
round 54: local lr = 0.01, sq_norm_avg_grad = 0.94008469581604, avg_sq_norm_grad = 119.8949966430664,                  max_norm_grad = 14.230549812316895, var_grad = 118.95491027832031

>>> Round:   55 / Acc: 80.655% / Loss: 0.6715 /Time: 4.75s
======================================================================================================

= Test = round: 55 / acc: 82.070% / loss: 0.6378 / Time: 0.94s
======================================================================================================

round 55: local lr = 0.01, sq_norm_avg_grad = 0.7554875016212463, avg_sq_norm_grad = 118.79537963867188,                  max_norm_grad = 13.9132719039917, var_grad = 118.0398941040039
round 56: local lr = 0.01, sq_norm_avg_grad = 0.8568551540374756, avg_sq_norm_grad = 119.04164123535156,                  max_norm_grad = 14.262117385864258, var_grad = 118.18478393554688
round 57: local lr = 0.01, sq_norm_avg_grad = 0.9646944403648376, avg_sq_norm_grad = 117.20403289794922,                  max_norm_grad = 14.227054595947266, var_grad = 116.23934173583984
round 58: local lr = 0.01, sq_norm_avg_grad = 0.831222653388977, avg_sq_norm_grad = 113.24617004394531,                  max_norm_grad = 13.727514266967773, var_grad = 112.41494750976562
round 59: local lr = 0.01, sq_norm_avg_grad = 0.9107075929641724, avg_sq_norm_grad = 112.49616241455078,                  max_norm_grad = 13.859126091003418, var_grad = 111.58545684814453

>>> Round:   60 / Acc: 81.585% / Loss: 0.6354 /Time: 4.80s
======================================================================================================

= Test = round: 60 / acc: 82.950% / loss: 0.6027 / Time: 1.01s
======================================================================================================

round 60: local lr = 0.01, sq_norm_avg_grad = 0.8101644515991211, avg_sq_norm_grad = 109.48207092285156,                  max_norm_grad = 13.340804100036621, var_grad = 108.67190551757812
round 61: local lr = 0.01, sq_norm_avg_grad = 0.5999104976654053, avg_sq_norm_grad = 109.67396545410156,                  max_norm_grad = 13.247514724731445, var_grad = 109.07405853271484
round 62: local lr = 0.01, sq_norm_avg_grad = 0.5900510549545288, avg_sq_norm_grad = 109.05079650878906,                  max_norm_grad = 13.279772758483887, var_grad = 108.46074676513672
round 63: local lr = 0.01, sq_norm_avg_grad = 0.6092113256454468, avg_sq_norm_grad = 107.54591369628906,                  max_norm_grad = 13.29029369354248, var_grad = 106.93669891357422
round 64: local lr = 0.01, sq_norm_avg_grad = 0.6302570700645447, avg_sq_norm_grad = 106.10112762451172,                  max_norm_grad = 13.221784591674805, var_grad = 105.47087097167969

>>> Round:   65 / Acc: 82.483% / Loss: 0.6005 /Time: 5.11s
======================================================================================================

= Test = round: 65 / acc: 83.760% / loss: 0.5680 / Time: 0.90s
======================================================================================================

round 65: local lr = 0.01, sq_norm_avg_grad = 0.5825722813606262, avg_sq_norm_grad = 104.01947784423828,                  max_norm_grad = 13.134943962097168, var_grad = 103.43690490722656
round 66: local lr = 0.01, sq_norm_avg_grad = 0.7150778770446777, avg_sq_norm_grad = 103.25686645507812,                  max_norm_grad = 13.389514923095703, var_grad = 102.54178619384766
round 67: local lr = 0.01, sq_norm_avg_grad = 0.5259528756141663, avg_sq_norm_grad = 100.67941284179688,                  max_norm_grad = 12.714491844177246, var_grad = 100.15345764160156
round 68: local lr = 0.01, sq_norm_avg_grad = 0.8043811917304993, avg_sq_norm_grad = 101.49478149414062,                  max_norm_grad = 13.2807035446167, var_grad = 100.69039916992188
round 69: local lr = 0.01, sq_norm_avg_grad = 0.45054230093955994, avg_sq_norm_grad = 99.25968170166016,                  max_norm_grad = 12.806838035583496, var_grad = 98.80914306640625

>>> Round:   70 / Acc: 83.100% / Loss: 0.5734 /Time: 4.88s
======================================================================================================

= Test = round: 70 / acc: 84.260% / loss: 0.5421 / Time: 1.05s
======================================================================================================

round 70: local lr = 0.01, sq_norm_avg_grad = 0.6400877833366394, avg_sq_norm_grad = 98.986083984375,                  max_norm_grad = 12.811563491821289, var_grad = 98.34599304199219
round 71: local lr = 0.01, sq_norm_avg_grad = 0.5327573418617249, avg_sq_norm_grad = 97.29488372802734,                  max_norm_grad = 12.762256622314453, var_grad = 96.76212310791016
round 72: local lr = 0.01, sq_norm_avg_grad = 0.4754617512226105, avg_sq_norm_grad = 97.1333999633789,                  max_norm_grad = 12.8214750289917, var_grad = 96.6579360961914
round 73: local lr = 0.01, sq_norm_avg_grad = 0.5877543687820435, avg_sq_norm_grad = 94.8768081665039,                  max_norm_grad = 12.413721084594727, var_grad = 94.28905487060547
round 74: local lr = 0.01, sq_norm_avg_grad = 0.41314613819122314, avg_sq_norm_grad = 94.73902893066406,                  max_norm_grad = 12.455530166625977, var_grad = 94.32588195800781

>>> Round:   75 / Acc: 83.865% / Loss: 0.5490 /Time: 4.89s
======================================================================================================

= Test = round: 75 / acc: 85.030% / loss: 0.5179 / Time: 1.11s
======================================================================================================

round 75: local lr = 0.01, sq_norm_avg_grad = 0.45326846837997437, avg_sq_norm_grad = 93.54911804199219,                  max_norm_grad = 12.562021255493164, var_grad = 93.0958480834961
round 76: local lr = 0.01, sq_norm_avg_grad = 0.4569593071937561, avg_sq_norm_grad = 93.62714385986328,                  max_norm_grad = 12.570869445800781, var_grad = 93.17018127441406
round 77: local lr = 0.01, sq_norm_avg_grad = 0.4118291735649109, avg_sq_norm_grad = 92.42642974853516,                  max_norm_grad = 12.23404598236084, var_grad = 92.01460266113281
round 78: local lr = 0.01, sq_norm_avg_grad = 0.4085758328437805, avg_sq_norm_grad = 91.96147918701172,                  max_norm_grad = 12.473203659057617, var_grad = 91.55290222167969
round 79: local lr = 0.01, sq_norm_avg_grad = 0.427788645029068, avg_sq_norm_grad = 89.92655944824219,                  max_norm_grad = 12.794817924499512, var_grad = 89.49877166748047

>>> Round:   80 / Acc: 84.408% / Loss: 0.5280 /Time: 5.23s
======================================================================================================

= Test = round: 80 / acc: 85.390% / loss: 0.4972 / Time: 1.04s
======================================================================================================

round 80: local lr = 0.01, sq_norm_avg_grad = 0.3322896361351013, avg_sq_norm_grad = 88.61531829833984,                  max_norm_grad = 12.436285018920898, var_grad = 88.28302764892578
round 81: local lr = 0.01, sq_norm_avg_grad = 0.32239195704460144, avg_sq_norm_grad = 87.95734405517578,                  max_norm_grad = 12.184412002563477, var_grad = 87.63494873046875
round 82: local lr = 0.01, sq_norm_avg_grad = 0.3600999712944031, avg_sq_norm_grad = 86.80731964111328,                  max_norm_grad = 12.021746635437012, var_grad = 86.44721984863281
round 83: local lr = 0.01, sq_norm_avg_grad = 0.33655688166618347, avg_sq_norm_grad = 86.02455139160156,                  max_norm_grad = 12.146038055419922, var_grad = 85.68799591064453
round 84: local lr = 0.01, sq_norm_avg_grad = 0.2785279452800751, avg_sq_norm_grad = 86.01372528076172,                  max_norm_grad = 12.050567626953125, var_grad = 85.73519897460938

>>> Round:   85 / Acc: 84.946% / Loss: 0.5097 /Time: 5.07s
======================================================================================================

= Test = round: 85 / acc: 85.860% / loss: 0.4795 / Time: 0.95s
======================================================================================================

round 85: local lr = 0.01, sq_norm_avg_grad = 0.4306448996067047, avg_sq_norm_grad = 85.78136444091797,                  max_norm_grad = 12.234673500061035, var_grad = 85.35072326660156
round 86: local lr = 0.01, sq_norm_avg_grad = 0.3338443338871002, avg_sq_norm_grad = 85.41658020019531,                  max_norm_grad = 12.131114959716797, var_grad = 85.08273315429688
round 87: local lr = 0.01, sq_norm_avg_grad = 0.39323189854621887, avg_sq_norm_grad = 84.72896575927734,                  max_norm_grad = 12.132283210754395, var_grad = 84.33573150634766
round 88: local lr = 0.01, sq_norm_avg_grad = 0.3400508165359497, avg_sq_norm_grad = 83.70171356201172,                  max_norm_grad = 11.98731803894043, var_grad = 83.36166381835938
round 89: local lr = 0.01, sq_norm_avg_grad = 0.34948670864105225, avg_sq_norm_grad = 83.32855224609375,                  max_norm_grad = 12.074341773986816, var_grad = 82.97906494140625

>>> Round:   90 / Acc: 85.417% / Loss: 0.4928 /Time: 5.29s
======================================================================================================

= Test = round: 90 / acc: 86.320% / loss: 0.4624 / Time: 0.91s
======================================================================================================

round 90: local lr = 0.01, sq_norm_avg_grad = 0.32225707173347473, avg_sq_norm_grad = 82.13890075683594,                  max_norm_grad = 11.926459312438965, var_grad = 81.81664276123047
round 91: local lr = 0.01, sq_norm_avg_grad = 0.35587558150291443, avg_sq_norm_grad = 81.05955505371094,                  max_norm_grad = 11.681388854980469, var_grad = 80.70368194580078
round 92: local lr = 0.01, sq_norm_avg_grad = 0.3803681433200836, avg_sq_norm_grad = 81.365966796875,                  max_norm_grad = 11.94343090057373, var_grad = 80.985595703125
round 93: local lr = 0.01, sq_norm_avg_grad = 0.30313554406166077, avg_sq_norm_grad = 80.5997543334961,                  max_norm_grad = 11.82711124420166, var_grad = 80.29661560058594
round 94: local lr = 0.01, sq_norm_avg_grad = 0.3124019205570221, avg_sq_norm_grad = 79.29894256591797,                  max_norm_grad = 11.78341007232666, var_grad = 78.98654174804688

>>> Round:   95 / Acc: 85.845% / Loss: 0.4787 /Time: 4.79s
======================================================================================================

= Test = round: 95 / acc: 86.620% / loss: 0.4487 / Time: 0.91s
======================================================================================================

round 95: local lr = 0.01, sq_norm_avg_grad = 0.3418116271495819, avg_sq_norm_grad = 78.46942901611328,                  max_norm_grad = 11.670754432678223, var_grad = 78.12761688232422
round 96: local lr = 0.01, sq_norm_avg_grad = 0.2854229509830475, avg_sq_norm_grad = 77.66193389892578,                  max_norm_grad = 11.716387748718262, var_grad = 77.37651062011719
round 97: local lr = 0.01, sq_norm_avg_grad = 0.4756752550601959, avg_sq_norm_grad = 78.53611755371094,                  max_norm_grad = 11.983016014099121, var_grad = 78.06044006347656
round 98: local lr = 0.01, sq_norm_avg_grad = 0.3435943126678467, avg_sq_norm_grad = 76.64321899414062,                  max_norm_grad = 11.590276718139648, var_grad = 76.29962158203125
round 99: local lr = 0.01, sq_norm_avg_grad = 0.2972343862056732, avg_sq_norm_grad = 75.037353515625,                  max_norm_grad = 11.408888816833496, var_grad = 74.74011993408203

>>> Round:  100 / Acc: 86.196% / Loss: 0.4652 /Time: 4.81s
======================================================================================================

= Test = round: 100 / acc: 87.110% / loss: 0.4346 / Time: 0.85s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.2946, Train_acc: 0.5982, Test_loss: 1.2723, Test_acc: 0.6087
Epoch: 006, Train_loss: 1.0044, Train_acc: 0.6868, Test_loss: 1.0056, Test_acc: 0.6864
Epoch: 011, Train_loss: 0.9243, Train_acc: 0.7091, Test_loss: 0.9386, Test_acc: 0.7081
Epoch: 016, Train_loss: 0.8804, Train_acc: 0.7201, Test_loss: 0.9044, Test_acc: 0.7169
Epoch: 021, Train_loss: 0.8627, Train_acc: 0.7277, Test_loss: 0.8936, Test_acc: 0.7203
Epoch: 026, Train_loss: 0.8435, Train_acc: 0.7306, Test_loss: 0.8806, Test_acc: 0.7241
Epoch: 031, Train_loss: 0.8277, Train_acc: 0.7372, Test_loss: 0.8768, Test_acc: 0.7276
Epoch: 036, Train_loss: 0.7996, Train_acc: 0.7473, Test_loss: 0.8501, Test_acc: 0.7326
Epoch: 041, Train_loss: 0.7905, Train_acc: 0.7503, Test_loss: 0.8467, Test_acc: 0.7317
Epoch: 046, Train_loss: 0.7802, Train_acc: 0.7517, Test_loss: 0.8391, Test_acc: 0.7380
Epoch: 051, Train_loss: 0.7872, Train_acc: 0.7471, Test_loss: 0.8560, Test_acc: 0.7296
Epoch: 056, Train_loss: 0.7714, Train_acc: 0.7544, Test_loss: 0.8500, Test_acc: 0.7339
Epoch: 061, Train_loss: 0.7590, Train_acc: 0.7581, Test_loss: 0.8317, Test_acc: 0.7394
Epoch: 066, Train_loss: 0.7490, Train_acc: 0.7619, Test_loss: 0.8274, Test_acc: 0.7416
Epoch: 071, Train_loss: 0.7460, Train_acc: 0.7610, Test_loss: 0.8279, Test_acc: 0.7423
Epoch: 076, Train_loss: 0.7462, Train_acc: 0.7607, Test_loss: 0.8253, Test_acc: 0.7446
Epoch: 081, Train_loss: 0.7486, Train_acc: 0.7613, Test_loss: 0.8350, Test_acc: 0.7363
Epoch: 086, Train_loss: 0.7515, Train_acc: 0.7551, Test_loss: 0.8390, Test_acc: 0.7344
Epoch: 091, Train_loss: 0.7401, Train_acc: 0.7616, Test_loss: 0.8345, Test_acc: 0.7387
Epoch: 096, Train_loss: 0.7419, Train_acc: 0.7603, Test_loss: 0.8415, Test_acc: 0.7385
Epoch: 101, Train_loss: 0.7245, Train_acc: 0.7672, Test_loss: 0.8208, Test_acc: 0.7471
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20230928194205_model_ft.pt.
Model saved at ./models/ft_checkpoints/20230928194205_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [0.7202521973012983, 0.7702581312181149, 0.8130354373047927, 0.7455838240195534]
model_source_only: [2.4962594435418297, 0.40326435145167033, 2.5208210347559885, 0.40206643706254863]
fl_test_acc_mean 0.8706
model_source_only_test_acc_mean 0.40206643706254863
model_ft_test_acc_mean 0.7455838240195534
