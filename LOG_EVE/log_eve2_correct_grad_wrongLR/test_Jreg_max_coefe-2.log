nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	          reg_max : True
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230927234912
FL pretrained model will be saved at ./models/lenet_mnist_20230927234912.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 11.480% / Loss: 2.3077 /Time: 4.46s
======================================================================================================

= Test = round: 0 / acc: 11.920% / loss: 2.3058 / Time: 0.93s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.010505985468626022, norm_avg_grad = 0.17190571129322052, avg_norm_grad = 1.236629843711853,                  max_norm_grad = 1.39724862575531
round 2: local lr = 0.010389734990894794, norm_avg_grad = 0.180292010307312, avg_norm_grad = 1.311469554901123,                  max_norm_grad = 1.4877721071243286
round 3: local lr = 0.010293531231582165, norm_avg_grad = 0.19151553511619568, avg_norm_grad = 1.4061311483383179,                  max_norm_grad = 1.6008243560791016
round 4: local lr = 0.010206184349954128, norm_avg_grad = 0.20630453526973724, avg_norm_grad = 1.5276771783828735,                  max_norm_grad = 1.7335580587387085

>>> Round:    5 / Acc: 14.596% / Loss: 2.2923 /Time: 4.18s
======================================================================================================

= Test = round: 5 / acc: 15.220% / loss: 2.2904 / Time: 0.81s
======================================================================================================

round 5: local lr = 0.010139680467545986, norm_avg_grad = 0.22597664594650269, avg_norm_grad = 1.6843235492706299,                  max_norm_grad = 1.923812747001648
round 6: local lr = 0.010127009823918343, norm_avg_grad = 0.252797394990921, avg_norm_grad = 1.8865902423858643,                  max_norm_grad = 2.1945037841796875
round 7: local lr = 0.010191922076046467, norm_avg_grad = 0.29082080721855164, avg_norm_grad = 2.1565303802490234,                  max_norm_grad = 2.5561249256134033
round 8: local lr = 0.010210053063929081, norm_avg_grad = 0.34383121132850647, avg_norm_grad = 2.5450923442840576,                  max_norm_grad = 3.047880172729492
round 9: local lr = 0.01056820061057806, norm_avg_grad = 0.4379063546657562, avg_norm_grad = 3.131600856781006,                  max_norm_grad = 3.7439019680023193

>>> Round:   10 / Acc: 13.015% / Loss: 2.2645 /Time: 4.29s
======================================================================================================

= Test = round: 10 / acc: 13.290% / loss: 2.2628 / Time: 0.81s
======================================================================================================

round 10: local lr = 0.012012581340968609, norm_avg_grad = 0.6668975949287415, avg_norm_grad = 4.195744514465332,                  max_norm_grad = 5.0579423904418945
round 11: local lr = 0.01565808616578579, norm_avg_grad = 1.3693939447402954, avg_norm_grad = 6.609611511230469,                  max_norm_grad = 8.115418434143066
round 12: local lr = 0.019908364862203598, norm_avg_grad = 1.900489330291748, avg_norm_grad = 7.214663028717041,                  max_norm_grad = 9.056695938110352
round 13: local lr = 0.028563251718878746, norm_avg_grad = 2.6149961948394775, avg_norm_grad = 6.919100761413574,                  max_norm_grad = 9.029016494750977
round 14: local lr = 0.03845074400305748, norm_avg_grad = 2.8471505641937256, avg_norm_grad = 5.596182823181152,                  max_norm_grad = 7.512866497039795

>>> Round:   15 / Acc: 15.974% / Loss: 2.1874 /Time: 4.23s
======================================================================================================

= Test = round: 15 / acc: 17.270% / loss: 2.1783 / Time: 0.80s
======================================================================================================

round 15: local lr = 0.016128938645124435, norm_avg_grad = 0.5338731408119202, avg_norm_grad = 2.501603603363037,                  max_norm_grad = 3.011612892150879
round 16: local lr = 0.018841229379177094, norm_avg_grad = 1.1536802053451538, avg_norm_grad = 4.6276702880859375,                  max_norm_grad = 5.406221389770508
round 17: local lr = 0.020036647096276283, norm_avg_grad = 1.59364652633667, avg_norm_grad = 6.011088848114014,                  max_norm_grad = 7.125653266906738
round 18: local lr = 0.03196006268262863, norm_avg_grad = 2.6890053749084473, avg_norm_grad = 6.358727931976318,                  max_norm_grad = 7.949405193328857
round 19: local lr = 0.022832095623016357, norm_avg_grad = 1.0012333393096924, avg_norm_grad = 3.3141775131225586,                  max_norm_grad = 3.920987606048584

>>> Round:   20 / Acc: 31.673% / Loss: 1.8179 /Time: 4.22s
======================================================================================================

= Test = round: 20 / acc: 33.730% / loss: 1.7896 / Time: 0.81s
======================================================================================================

round 20: local lr = 0.02073044702410698, norm_avg_grad = 1.5838356018066406, avg_norm_grad = 5.774144172668457,                  max_norm_grad = 6.645544528961182
round 21: local lr = 0.030740559101104736, norm_avg_grad = 2.6289329528808594, avg_norm_grad = 6.463294506072998,                  max_norm_grad = 7.858423233032227
round 22: local lr = 0.027497673407197, norm_avg_grad = 1.4471626281738281, avg_norm_grad = 3.9774763584136963,                  max_norm_grad = 4.845882892608643
round 23: local lr = 0.024951865896582603, norm_avg_grad = 1.4321324825286865, avg_norm_grad = 4.3377685546875,                  max_norm_grad = 5.174185276031494
round 24: local lr = 0.02559180185198784, norm_avg_grad = 1.5509895086288452, avg_norm_grad = 4.580303192138672,                  max_norm_grad = 5.508578300476074

>>> Round:   25 / Acc: 56.507% / Loss: 1.4625 /Time: 4.54s
======================================================================================================

= Test = round: 25 / acc: 57.140% / loss: 1.4296 / Time: 0.86s
======================================================================================================

round 25: local lr = 0.025678200647234917, norm_avg_grad = 1.816701054573059, avg_norm_grad = 5.34693717956543,                  max_norm_grad = 6.268649578094482
round 26: local lr = 0.02792035974562168, norm_avg_grad = 1.937633991241455, avg_norm_grad = 5.24489688873291,                  max_norm_grad = 6.297718524932861
round 27: local lr = 0.029142433777451515, norm_avg_grad = 1.92052161693573, avg_norm_grad = 4.980576038360596,                  max_norm_grad = 6.0721235275268555
round 28: local lr = 0.029017899185419083, norm_avg_grad = 1.9827250242233276, avg_norm_grad = 5.163958549499512,                  max_norm_grad = 6.475597858428955
round 29: local lr = 0.030467800796031952, norm_avg_grad = 2.2999207973480225, avg_norm_grad = 5.70503044128418,                  max_norm_grad = 7.6866350173950195

>>> Round:   30 / Acc: 58.607% / Loss: 1.2883 /Time: 4.60s
======================================================================================================

= Test = round: 30 / acc: 59.370% / loss: 1.2580 / Time: 0.83s
======================================================================================================

round 30: local lr = 0.03651992976665497, norm_avg_grad = 2.6612884998321533, avg_norm_grad = 5.507420539855957,                  max_norm_grad = 8.269271850585938
round 31: local lr = 0.035570818930864334, norm_avg_grad = 2.3153231143951416, avg_norm_grad = 4.919307231903076,                  max_norm_grad = 7.322105884552002
round 32: local lr = 0.03476978465914726, norm_avg_grad = 2.39506196975708, avg_norm_grad = 5.20596170425415,                  max_norm_grad = 7.931852340698242
round 33: local lr = 0.039017993956804276, norm_avg_grad = 3.096737861633301, avg_norm_grad = 5.998265743255615,                  max_norm_grad = 10.03154468536377
round 34: local lr = 0.03524177893996239, norm_avg_grad = 1.927392840385437, avg_norm_grad = 4.1333160400390625,                  max_norm_grad = 5.4747724533081055

>>> Round:   35 / Acc: 73.463% / Loss: 0.9003 /Time: 4.55s
======================================================================================================

= Test = round: 35 / acc: 74.350% / loss: 0.8651 / Time: 0.84s
======================================================================================================

round 35: local lr = 0.029936663806438446, norm_avg_grad = 1.853494644165039, avg_norm_grad = 4.679227352142334,                  max_norm_grad = 6.729465007781982
round 36: local lr = 0.027976932004094124, norm_avg_grad = 1.84135103225708, avg_norm_grad = 4.974194049835205,                  max_norm_grad = 7.585773944854736
round 37: local lr = 0.03239411860704422, norm_avg_grad = 2.2664647102355957, avg_norm_grad = 5.287726879119873,                  max_norm_grad = 8.683159828186035
round 38: local lr = 0.03876343369483948, norm_avg_grad = 2.739403486251831, avg_norm_grad = 5.340968132019043,                  max_norm_grad = 8.843268394470215
round 39: local lr = 0.035460758954286575, norm_avg_grad = 2.0861988067626953, avg_norm_grad = 4.446249961853027,                  max_norm_grad = 6.494584560394287

>>> Round:   40 / Acc: 75.875% / Loss: 0.7552 /Time: 4.47s
======================================================================================================

= Test = round: 40 / acc: 76.570% / loss: 0.7197 / Time: 0.90s
======================================================================================================

round 40: local lr = 0.03256845474243164, norm_avg_grad = 2.027561902999878, avg_norm_grad = 4.705038547515869,                  max_norm_grad = 7.366236209869385
round 41: local lr = 0.0330212339758873, norm_avg_grad = 2.124800443649292, avg_norm_grad = 4.863076210021973,                  max_norm_grad = 7.909675121307373
round 42: local lr = 0.03709706664085388, norm_avg_grad = 2.500483274459839, avg_norm_grad = 5.094136714935303,                  max_norm_grad = 8.56270980834961
round 43: local lr = 0.035835158079862595, norm_avg_grad = 2.2151949405670166, avg_norm_grad = 4.671849250793457,                  max_norm_grad = 7.330620288848877
round 44: local lr = 0.03300197795033455, norm_avg_grad = 1.9052832126617432, avg_norm_grad = 4.363206386566162,                  max_norm_grad = 6.564534664154053

>>> Round:   45 / Acc: 82.609% / Loss: 0.6058 /Time: 4.51s
======================================================================================================

= Test = round: 45 / acc: 83.670% / loss: 0.5724 / Time: 0.88s
======================================================================================================

round 45: local lr = 0.031228257343173027, norm_avg_grad = 1.834516167640686, avg_norm_grad = 4.439765453338623,                  max_norm_grad = 6.8358564376831055
round 46: local lr = 0.03386986255645752, norm_avg_grad = 2.088993787765503, avg_norm_grad = 4.661330699920654,                  max_norm_grad = 7.564592361450195
round 47: local lr = 0.03841513395309448, norm_avg_grad = 2.524709939956665, avg_norm_grad = 4.967013835906982,                  max_norm_grad = 8.211122512817383
round 48: local lr = 0.0396980345249176, norm_avg_grad = 2.478943109512329, avg_norm_grad = 4.719367027282715,                  max_norm_grad = 7.500672340393066
round 49: local lr = 0.03805208578705788, norm_avg_grad = 2.3162143230438232, avg_norm_grad = 4.6003031730651855,                  max_norm_grad = 7.018570899963379

>>> Round:   50 / Acc: 81.194% / Loss: 0.6268 /Time: 4.37s
======================================================================================================

= Test = round: 50 / acc: 82.360% / loss: 0.5940 / Time: 0.93s
======================================================================================================

round 50: local lr = 0.037617359310388565, norm_avg_grad = 2.2343204021453857, avg_norm_grad = 4.488935470581055,                  max_norm_grad = 6.794196128845215
round 51: local lr = 0.032785262912511826, norm_avg_grad = 1.755239725112915, avg_norm_grad = 4.046168804168701,                  max_norm_grad = 5.832064151763916
round 52: local lr = 0.03029973991215229, norm_avg_grad = 1.5738599300384521, avg_norm_grad = 3.925666332244873,                  max_norm_grad = 5.642413139343262
round 53: local lr = 0.030497441068291664, norm_avg_grad = 1.6132793426513672, avg_norm_grad = 3.9979043006896973,                  max_norm_grad = 5.904865264892578
round 54: local lr = 0.0313139483332634, norm_avg_grad = 1.6322633028030396, avg_norm_grad = 3.9394772052764893,                  max_norm_grad = 5.827817916870117

>>> Round:   55 / Acc: 87.548% / Loss: 0.4555 /Time: 4.45s
======================================================================================================

= Test = round: 55 / acc: 88.330% / loss: 0.4255 / Time: 0.86s
======================================================================================================

round 55: local lr = 0.030005700886249542, norm_avg_grad = 1.5173978805541992, avg_norm_grad = 3.821923017501831,                  max_norm_grad = 5.536458492279053
round 56: local lr = 0.030822936445474625, norm_avg_grad = 1.5684828758239746, avg_norm_grad = 3.8458468914031982,                  max_norm_grad = 5.74372673034668
round 57: local lr = 0.03399773687124252, norm_avg_grad = 1.8154960870742798, avg_norm_grad = 4.035817623138428,                  max_norm_grad = 6.0918779373168945
round 58: local lr = 0.03627074137330055, norm_avg_grad = 1.985628604888916, avg_norm_grad = 4.1374030113220215,                  max_norm_grad = 6.363205909729004
round 59: local lr = 0.03655748814344406, norm_avg_grad = 1.918736457824707, avg_norm_grad = 3.9666621685028076,                  max_norm_grad = 5.616256237030029

>>> Round:   60 / Acc: 87.389% / Loss: 0.4564 /Time: 4.54s
======================================================================================================

= Test = round: 60 / acc: 88.170% / loss: 0.4279 / Time: 0.90s
======================================================================================================

round 60: local lr = 0.03334677964448929, norm_avg_grad = 1.6357197761535645, avg_norm_grad = 3.7071588039398193,                  max_norm_grad = 5.488717555999756
round 61: local lr = 0.031167039647698402, norm_avg_grad = 1.475346326828003, avg_norm_grad = 3.5775411128997803,                  max_norm_grad = 5.391232967376709
round 62: local lr = 0.03332776576280594, norm_avg_grad = 1.6401067972183228, avg_norm_grad = 3.7192225456237793,                  max_norm_grad = 5.958595275878906
round 63: local lr = 0.035365283489227295, norm_avg_grad = 1.7713382244110107, avg_norm_grad = 3.7853894233703613,                  max_norm_grad = 6.044317722320557
round 64: local lr = 0.03397925943136215, norm_avg_grad = 1.6318341493606567, avg_norm_grad = 3.6295127868652344,                  max_norm_grad = 5.713340759277344

>>> Round:   65 / Acc: 88.308% / Loss: 0.4070 /Time: 4.51s
======================================================================================================

= Test = round: 65 / acc: 89.160% / loss: 0.3794 / Time: 0.86s
======================================================================================================

round 65: local lr = 0.034059058874845505, norm_avg_grad = 1.6298866271972656, avg_norm_grad = 3.616687536239624,                  max_norm_grad = 5.795284748077393
round 66: local lr = 0.0354875884950161, norm_avg_grad = 1.7318893671035767, avg_norm_grad = 3.688330888748169,                  max_norm_grad = 6.023678779602051
round 67: local lr = 0.03660636395215988, norm_avg_grad = 1.8082115650177002, avg_norm_grad = 3.7331795692443848,                  max_norm_grad = 5.831974506378174
round 68: local lr = 0.03641251474618912, norm_avg_grad = 1.7388678789138794, avg_norm_grad = 3.609126567840576,                  max_norm_grad = 5.510557651519775
round 69: local lr = 0.03398139774799347, norm_avg_grad = 1.5314786434173584, avg_norm_grad = 3.4060885906219482,                  max_norm_grad = 5.1628737449646

>>> Round:   70 / Acc: 89.590% / Loss: 0.3753 /Time: 4.56s
======================================================================================================

= Test = round: 70 / acc: 90.390% / loss: 0.3482 / Time: 0.88s
======================================================================================================

round 70: local lr = 0.033007871359586716, norm_avg_grad = 1.4507911205291748, avg_norm_grad = 3.321800708770752,                  max_norm_grad = 4.946558475494385
round 71: local lr = 0.03356663137674332, norm_avg_grad = 1.4804891347885132, avg_norm_grad = 3.333371162414551,                  max_norm_grad = 4.87706995010376
round 72: local lr = 0.0307871475815773, norm_avg_grad = 1.2767443656921387, avg_norm_grad = 3.1341567039489746,                  max_norm_grad = 4.486230850219727
round 73: local lr = 0.028549565002322197, norm_avg_grad = 1.1401057243347168, avg_norm_grad = 3.01808762550354,                  max_norm_grad = 4.273780345916748
round 74: local lr = 0.026751134544610977, norm_avg_grad = 1.0428435802459717, avg_norm_grad = 2.946206569671631,                  max_norm_grad = 4.2674641609191895

>>> Round:   75 / Acc: 91.157% / Loss: 0.3137 /Time: 5.70s
======================================================================================================

= Test = round: 75 / acc: 91.840% / loss: 0.2886 / Time: 1.03s
======================================================================================================

round 75: local lr = 0.027944520115852356, norm_avg_grad = 1.1036781072616577, avg_norm_grad = 2.984915018081665,                  max_norm_grad = 4.473111629486084
round 76: local lr = 0.030078565701842308, norm_avg_grad = 1.209397315979004, avg_norm_grad = 3.038771867752075,                  max_norm_grad = 4.7535271644592285
round 77: local lr = 0.03194178268313408, norm_avg_grad = 1.304362177848816, avg_norm_grad = 3.0862083435058594,                  max_norm_grad = 4.827044486999512
round 78: local lr = 0.03500384837388992, norm_avg_grad = 1.512343168258667, avg_norm_grad = 3.26528263092041,                  max_norm_grad = 5.619579315185547
round 79: local lr = 0.03753841668367386, norm_avg_grad = 1.6913257837295532, avg_norm_grad = 3.4051599502563477,                  max_norm_grad = 5.782571315765381

>>> Round:   80 / Acc: 88.965% / Loss: 0.3692 /Time: 5.21s
======================================================================================================

= Test = round: 80 / acc: 89.890% / loss: 0.3436 / Time: 1.00s
======================================================================================================

round 80: local lr = 0.03912898525595665, norm_avg_grad = 1.8167850971221924, avg_norm_grad = 3.509063243865967,                  max_norm_grad = 5.968510627746582
round 81: local lr = 0.03752487152814865, norm_avg_grad = 1.6282048225402832, avg_norm_grad = 3.279261350631714,                  max_norm_grad = 5.182604789733887
round 82: local lr = 0.03332730382680893, norm_avg_grad = 1.3080482482910156, avg_norm_grad = 2.966264009475708,                  max_norm_grad = 4.555014133453369
round 83: local lr = 0.03026929497718811, norm_avg_grad = 1.1230549812316895, avg_norm_grad = 2.804044723510742,                  max_norm_grad = 4.122802257537842
round 84: local lr = 0.027780478820204735, norm_avg_grad = 0.9903019666671753, avg_norm_grad = 2.6941025257110596,                  max_norm_grad = 3.9587087631225586

>>> Round:   85 / Acc: 92.310% / Loss: 0.2803 /Time: 5.41s
======================================================================================================

= Test = round: 85 / acc: 92.800% / loss: 0.2579 / Time: 0.95s
======================================================================================================

round 85: local lr = 0.02630539983510971, norm_avg_grad = 0.9235628247261047, avg_norm_grad = 2.653430700302124,                  max_norm_grad = 3.9568657875061035
round 86: local lr = 0.02692113257944584, norm_avg_grad = 0.9499008059501648, avg_norm_grad = 2.6666815280914307,                  max_norm_grad = 4.016440391540527
round 87: local lr = 0.025831397622823715, norm_avg_grad = 0.9009266495704651, avg_norm_grad = 2.635892868041992,                  max_norm_grad = 4.038243770599365
round 88: local lr = 0.02792545035481453, norm_avg_grad = 0.9895418286323547, avg_norm_grad = 2.6780593395233154,                  max_norm_grad = 4.22650146484375
round 89: local lr = 0.02916710451245308, norm_avg_grad = 1.050946593284607, avg_norm_grad = 2.7231624126434326,                  max_norm_grad = 4.443602561950684

>>> Round:   90 / Acc: 90.996% / Loss: 0.3054 /Time: 5.44s
======================================================================================================

= Test = round: 90 / acc: 91.690% / loss: 0.2820 / Time: 0.97s
======================================================================================================

round 90: local lr = 0.036330386996269226, norm_avg_grad = 1.4549968242645264, avg_norm_grad = 3.026761770248413,                  max_norm_grad = 4.882197856903076
round 91: local lr = 0.035871244966983795, norm_avg_grad = 1.409790277481079, avg_norm_grad = 2.9702587127685547,                  max_norm_grad = 4.873091220855713
round 92: local lr = 0.03574078530073166, norm_avg_grad = 1.3843899965286255, avg_norm_grad = 2.9273900985717773,                  max_norm_grad = 4.77419900894165
round 93: local lr = 0.035057004541158676, norm_avg_grad = 1.3174232244491577, avg_norm_grad = 2.8401200771331787,                  max_norm_grad = 4.61091947555542
round 94: local lr = 0.033016402274370193, norm_avg_grad = 1.175552487373352, avg_norm_grad = 2.690905809402466,                  max_norm_grad = 4.284440994262695

>>> Round:   95 / Acc: 92.972% / Loss: 0.2687 /Time: 5.37s
======================================================================================================

= Test = round: 95 / acc: 93.470% / loss: 0.2467 / Time: 1.03s
======================================================================================================

round 95: local lr = 0.02770848572254181, norm_avg_grad = 0.9019705057144165, avg_norm_grad = 2.4601736068725586,                  max_norm_grad = 3.824479341506958
round 96: local lr = 0.0261525958776474, norm_avg_grad = 0.8363911509513855, avg_norm_grad = 2.417023181915283,                  max_norm_grad = 3.7531144618988037
round 97: local lr = 0.023984510451555252, norm_avg_grad = 0.7497421503067017, avg_norm_grad = 2.3624751567840576,                  max_norm_grad = 3.6554794311523438
round 98: local lr = 0.023603368550539017, norm_avg_grad = 0.7374550104141235, avg_norm_grad = 2.361281394958496,                  max_norm_grad = 3.709789991378784
round 99: local lr = 0.024441838264465332, norm_avg_grad = 0.7679078578948975, avg_norm_grad = 2.374441385269165,                  max_norm_grad = 3.767025947570801

>>> Round:  100 / Acc: 93.240% / Loss: 0.2356 /Time: 4.74s
======================================================================================================

= Test = round: 100 / acc: 93.630% / loss: 0.2149 / Time: 0.95s
======================================================================================================

round 100: local lr = 0.02586425468325615, norm_avg_grad = 0.8215146064758301, avg_norm_grad = 2.400499105453491,                  max_norm_grad = 3.8058032989501953
round 101: local lr = 0.027437999844551086, norm_avg_grad = 0.887241005897522, avg_norm_grad = 2.443854570388794,                  max_norm_grad = 3.995265483856201
round 102: local lr = 0.029089484363794327, norm_avg_grad = 0.9622557163238525, avg_norm_grad = 2.5000040531158447,                  max_norm_grad = 4.267399311065674
round 103: local lr = 0.032932065427303314, norm_avg_grad = 1.1466498374938965, avg_norm_grad = 2.631467819213867,                  max_norm_grad = 4.550025939941406
round 104: local lr = 0.033651504665613174, norm_avg_grad = 1.1606194972991943, avg_norm_grad = 2.6065831184387207,                  max_norm_grad = 4.294924259185791

>>> Round:  105 / Acc: 92.614% / Loss: 0.2573 /Time: 4.51s
======================================================================================================

= Test = round: 105 / acc: 93.080% / loss: 0.2367 / Time: 0.84s
======================================================================================================

round 105: local lr = 0.033285610377788544, norm_avg_grad = 1.1337443590164185, avg_norm_grad = 2.5742149353027344,                  max_norm_grad = 4.181356430053711
round 106: local lr = 0.03233785927295685, norm_avg_grad = 1.0687459707260132, avg_norm_grad = 2.4977526664733887,                  max_norm_grad = 3.9119608402252197
round 107: local lr = 0.032649435102939606, norm_avg_grad = 1.079991340637207, avg_norm_grad = 2.4999470710754395,                  max_norm_grad = 3.9642460346221924
round 108: local lr = 0.028986208140850067, norm_avg_grad = 0.8990880250930786, avg_norm_grad = 2.344212770462036,                  max_norm_grad = 3.660184383392334
Training early stopped. Model saved at ./models/lenet_mnist_20230927234912.pt.

>>> Round:  200 / Acc: 93.399% / Loss: 0.2357 /Time: 4.72s
======================================================================================================

= Test = round: 200 / acc: 93.920% / loss: 0.2157 / Time: 0.96s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.0217, Train_acc: 0.6905, Test_loss: 1.0012, Test_acc: 0.6964
Epoch: 006, Train_loss: 0.8377, Train_acc: 0.7404, Test_loss: 0.8401, Test_acc: 0.7394
Epoch: 011, Train_loss: 0.7870, Train_acc: 0.7532, Test_loss: 0.8048, Test_acc: 0.7468
Epoch: 016, Train_loss: 0.7454, Train_acc: 0.7627, Test_loss: 0.7786, Test_acc: 0.7509
Epoch: 021, Train_loss: 0.7224, Train_acc: 0.7669, Test_loss: 0.7729, Test_acc: 0.7526
Epoch: 026, Train_loss: 0.6979, Train_acc: 0.7747, Test_loss: 0.7569, Test_acc: 0.7586
Epoch: 031, Train_loss: 0.6818, Train_acc: 0.7800, Test_loss: 0.7554, Test_acc: 0.7571
Epoch: 036, Train_loss: 0.6753, Train_acc: 0.7810, Test_loss: 0.7562, Test_acc: 0.7570
Epoch: 041, Train_loss: 0.6523, Train_acc: 0.7896, Test_loss: 0.7493, Test_acc: 0.7570
Epoch: 046, Train_loss: 0.6447, Train_acc: 0.7909, Test_loss: 0.7467, Test_acc: 0.7596
Epoch: 051, Train_loss: 0.6401, Train_acc: 0.7914, Test_loss: 0.7499, Test_acc: 0.7608
Epoch: 056, Train_loss: 0.6323, Train_acc: 0.7934, Test_loss: 0.7522, Test_acc: 0.7595
Epoch: 061, Train_loss: 0.6299, Train_acc: 0.7947, Test_loss: 0.7558, Test_acc: 0.7615
Epoch: 066, Train_loss: 0.6217, Train_acc: 0.7966, Test_loss: 0.7562, Test_acc: 0.7569
Epoch: 071, Train_loss: 0.6233, Train_acc: 0.7959, Test_loss: 0.7579, Test_acc: 0.7606
Epoch: 076, Train_loss: 0.6086, Train_acc: 0.8011, Test_loss: 0.7514, Test_acc: 0.7604
Epoch: 081, Train_loss: 0.6070, Train_acc: 0.8012, Test_loss: 0.7568, Test_acc: 0.7609
Epoch: 086, Train_loss: 0.6020, Train_acc: 0.8035, Test_loss: 0.7564, Test_acc: 0.7599
Epoch: 091, Train_loss: 0.5977, Train_acc: 0.8035, Test_loss: 0.7572, Test_acc: 0.7588
Epoch: 096, Train_loss: 0.5975, Train_acc: 0.8039, Test_loss: 0.7631, Test_acc: 0.7599
Epoch: 101, Train_loss: 0.5931, Train_acc: 0.8056, Test_loss: 0.7599, Test_acc: 0.7579
Epoch: 106, Train_loss: 0.5875, Train_acc: 0.8080, Test_loss: 0.7570, Test_acc: 0.7605
Epoch: 111, Train_loss: 0.5857, Train_acc: 0.8083, Test_loss: 0.7581, Test_acc: 0.7592
Epoch: 116, Train_loss: 0.5867, Train_acc: 0.8076, Test_loss: 0.7706, Test_acc: 0.7602
Epoch: 121, Train_loss: 0.5820, Train_acc: 0.8090, Test_loss: 0.7642, Test_acc: 0.7644
Epoch: 126, Train_loss: 0.5783, Train_acc: 0.8113, Test_loss: 0.7637, Test_acc: 0.7608
Epoch: 131, Train_loss: 0.5798, Train_acc: 0.8092, Test_loss: 0.7684, Test_acc: 0.7628
Epoch: 136, Train_loss: 0.5725, Train_acc: 0.8119, Test_loss: 0.7629, Test_acc: 0.7606
Epoch: 141, Train_loss: 0.5714, Train_acc: 0.8118, Test_loss: 0.7651, Test_acc: 0.7601
Epoch: 146, Train_loss: 0.5709, Train_acc: 0.8142, Test_loss: 0.7651, Test_acc: 0.7592
Epoch: 151, Train_loss: 0.5703, Train_acc: 0.8122, Test_loss: 0.7689, Test_acc: 0.7582
Epoch: 156, Train_loss: 0.5665, Train_acc: 0.8126, Test_loss: 0.7657, Test_acc: 0.7641
Epoch: 161, Train_loss: 0.5658, Train_acc: 0.8143, Test_loss: 0.7653, Test_acc: 0.7600
Epoch: 166, Train_loss: 0.5616, Train_acc: 0.8161, Test_loss: 0.7641, Test_acc: 0.7592
Epoch: 171, Train_loss: 0.5648, Train_acc: 0.8139, Test_loss: 0.7697, Test_acc: 0.7609
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20230927234912_model_ft.pt.
Model saved at ./models/ft_checkpoints/20230927234912_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [0.5616328908605872, 0.8161217606481246, 0.7640759652686799, 0.7592489723364071]
model_source_only: [1.6424519923037952, 0.4841273876713954, 1.6449098744507353, 0.48194645039440065]
fl_test_acc_mean 0.9366
model_source_only_test_acc_mean 0.48194645039440065
model_ft_test_acc_mean 0.7592489723364071
