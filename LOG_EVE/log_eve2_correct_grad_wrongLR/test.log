nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230927170630
FL pretrained model will be saved at ./models/lenet_mnist_20230927170630.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 12.651% / Loss: 2.3018 /Time: 4.07s
======================================================================================================

= Test = round: 0 / acc: 11.760% / loss: 2.3045 / Time: 0.76s
======================================================================================================

Traceback (most recent call last):
  File "main_mnist_mnist_m.py", line 394, in <module>
    main()
  File "main_mnist_mnist_m.py", line 324, in main
    fl_test_acc[repeat_i] = trainer.train()
  File "/data/shared/eve/TranferableFL/src/trainers/fedavgtl.py", line 82, in train
    solns, stats = self.local_train(round_i, selected_clients)
  File "/data/shared/eve/TranferableFL/src/trainers/base.py", line 139, in local_train
    print(f'round {round_i}: local lr = {torch.round(self.optimizer.get_current_lr(), 4)}, norm_avg_grad = {torch.round(norm_avg_grad_at_global_weight_last_round,4)}, avg_norm_grad = {avg_grad_norm_at_global_weight_last_round, 4}')
TypeError: round() received an invalid combination of arguments - got (float, int), but expected one of:
 * (Tensor input, *, Tensor out)
 * (Tensor input, *, int decimals, Tensor out)

nohup: ignoring input
working!
check why cannot sync
Using device: cuda:0
>>> Arguments:
	             algo : fedavgtl
	            alpha : 13.2316427
	       batch_size : 64
	clients_per_round : 100
	             clip : False
	          dataset : mnist_all_data_1_equal_niid
	           device : cuda:0
	              dis : 
	   early_stopping : 10
	       eval_every : 5
	    ft_batch_size : 128
	       ft_dataset : mnist-m
	        ft_epochs : 200
	            ft_lr : 0.001
	            ft_wd : 0.0001
	              gpu : True
	      input_shape : (1, 28, 28)
	           last_k : 1
	               lr : 0.01
	            model : lenet
	           n_init : 1
	        noaverage : False
	             noft : False
	          noprint : False
	        num_class : 10
	        num_epoch : 1
	        num_round : 200
	           opt_lr : True
	            reg_J : False
	       reg_J_coef : 0.0
	   reg_J_ind_coef : 0.01
	  reg_J_norm_coef : 0.0
	           repeat : 1
	     repeat_epoch : 10
	             seed : 0
	               wd : 0.0
>>> Read data from:
     ./data/mnist/data/all_data_1.pkl
>>> The estimate of constant alpha is 13.2316427.
>>> Read data from:
     ./data/mnist/data/train/all_data_1_equal_niid.pkl
     ./data/mnist/data/test/all_data_1_equal_niid.pkl

************************************************************************************************************************

uid: 20230927172317
FL pretrained model will be saved at ./models/lenet_mnist_20230927172317.pt
>>> Use gpu on device cuda:0
>>> Model statistic per layer
LeNet_MNIST(
  286.12 KMac, 100.000% MACs, 
  (conv1): Conv2d(89.856 KMac, 31.405% MACs, 1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(154.624 KMac, 54.042% MACs, 6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(30.72 KMac, 10.737% MACs, in_features=256, out_features=120, bias=True)
  (fc2): Linear(10.08 KMac, 3.523% MACs, in_features=120, out_features=84, bias=True)
  (fc3): Linear(840.0 Mac, 0.294% MACs, in_features=84, out_features=10, bias=True)
)
>>> Activate a worker for training
>>> Initialize 100 clients in total
>>> Weigh updates by simple average
>>> Select 100 clients per round 


>>> Round:    0 / Acc: 7.275% / Loss: 2.3047 /Time: 4.13s
======================================================================================================

= Test = round: 0 / acc: 6.430% / loss: 2.3053 / Time: 0.79s
======================================================================================================

round 0: local lr = 0.01
round 1: local lr = 0.009137600660324097, norm_avg_grad = 0.15118692815303802, avg_norm_grad = 1.250455617904663
round 2: local lr = 0.009064610116183758, norm_avg_grad = 0.15674713253974915, avg_norm_grad = 1.3068828582763672
round 3: local lr = 0.008974078111350536, norm_avg_grad = 0.16297762095928192, avg_norm_grad = 1.3725379705429077
round 4: local lr = 0.008891470730304718, norm_avg_grad = 0.17045915126800537, avg_norm_grad = 1.44888174533844

>>> Round:    5 / Acc: 10.006% / Loss: 2.2936 /Time: 3.91s
======================================================================================================

= Test = round: 5 / acc: 8.930% / loss: 2.2948 / Time: 0.76s
======================================================================================================

round 5: local lr = 0.008834711275994778, norm_avg_grad = 0.17970891296863556, avg_norm_grad = 1.537317156791687
round 6: local lr = 0.008780988864600658, norm_avg_grad = 0.19087128341197968, avg_norm_grad = 1.6427950859069824
round 7: local lr = 0.008741669356822968, norm_avg_grad = 0.20489031076431274, avg_norm_grad = 1.7713862657546997
round 8: local lr = 0.00874235574156046, norm_avg_grad = 0.22339676320552826, avg_norm_grad = 1.9312326908111572
round 9: local lr = 0.00865262746810913, norm_avg_grad = 0.2443697303533554, avg_norm_grad = 2.134448289871216

>>> Round:   10 / Acc: 12.661% / Loss: 2.2738 /Time: 3.99s
======================================================================================================

= Test = round: 10 / acc: 12.270% / loss: 2.2759 / Time: 0.76s
======================================================================================================

round 10: local lr = 0.008462382480502129, norm_avg_grad = 0.26759421825408936, avg_norm_grad = 2.389848232269287
round 11: local lr = 0.008229058235883713, norm_avg_grad = 0.294725239276886, avg_norm_grad = 2.706782817840576
round 12: local lr = 0.007979183457791805, norm_avg_grad = 0.3272729516029358, avg_norm_grad = 3.099830150604248
round 13: local lr = 0.007751931436359882, norm_avg_grad = 0.3683081865310669, avg_norm_grad = 3.5907704830169678
round 14: local lr = 0.007550863083451986, norm_avg_grad = 0.4237149953842163, avg_norm_grad = 4.240952968597412

>>> Round:   15 / Acc: 10.101% / Loss: 2.2344 /Time: 3.93s
======================================================================================================

= Test = round: 15 / acc: 9.810% / loss: 2.2398 / Time: 0.74s
======================================================================================================

round 15: local lr = 0.007582620717585087, norm_avg_grad = 0.5142049789428711, avg_norm_grad = 5.125109672546387
round 16: local lr = 0.008020143024623394, norm_avg_grad = 0.6803140640258789, avg_norm_grad = 6.4108195304870605
round 17: local lr = 0.008144870400428772, norm_avg_grad = 0.8637165427207947, avg_norm_grad = 8.01444149017334
round 18: local lr = 0.007500016130506992, norm_avg_grad = 0.8536585569381714, avg_norm_grad = 8.602173805236816
round 19: local lr = 0.006890866439789534, norm_avg_grad = 0.8185612559318542, avg_norm_grad = 8.977668762207031

>>> Round:   20 / Acc: 34.146% / Loss: 2.1391 /Time: 3.94s
======================================================================================================

= Test = round: 20 / acc: 33.260% / loss: 2.1407 / Time: 0.74s
======================================================================================================

round 20: local lr = 0.007133428938686848, norm_avg_grad = 0.8562697172164917, avg_norm_grad = 9.071904182434082
round 21: local lr = 0.0075703030452132225, norm_avg_grad = 0.9061776995658875, avg_norm_grad = 9.046619415283203
round 22: local lr = 0.008428217843174934, norm_avg_grad = 0.9906884431838989, avg_norm_grad = 8.883570671081543
round 23: local lr = 0.008625431917607784, norm_avg_grad = 0.9504510760307312, avg_norm_grad = 8.327893257141113
round 24: local lr = 0.008761866018176079, norm_avg_grad = 0.9658200740814209, avg_norm_grad = 8.33078384399414

>>> Round:   25 / Acc: 60.917% / Loss: 2.0393 /Time: 3.85s
======================================================================================================

= Test = round: 25 / acc: 61.890% / loss: 2.0362 / Time: 0.73s
======================================================================================================

round 25: local lr = 0.008836466819047928, norm_avg_grad = 0.974205732345581, avg_norm_grad = 8.332172393798828
round 26: local lr = 0.009173889644443989, norm_avg_grad = 1.007523536682129, avg_norm_grad = 8.300187110900879
round 27: local lr = 0.009405274875462055, norm_avg_grad = 1.0042673349380493, avg_norm_grad = 8.06982421875
round 28: local lr = 0.010161842219531536, norm_avg_grad = 1.0947844982147217, avg_norm_grad = 8.1422119140625
round 29: local lr = 0.010425705462694168, norm_avg_grad = 1.086951494216919, avg_norm_grad = 7.879360198974609

>>> Round:   30 / Acc: 69.175% / Loss: 1.8673 /Time: 3.86s
======================================================================================================

= Test = round: 30 / acc: 70.450% / loss: 1.8598 / Time: 0.73s
======================================================================================================

round 30: local lr = 0.011095398105680943, norm_avg_grad = 1.1574345827102661, avg_norm_grad = 7.883875370025635
round 31: local lr = 0.01177194993942976, norm_avg_grad = 1.2000044584274292, avg_norm_grad = 7.704078197479248
round 32: local lr = 0.012775524519383907, norm_avg_grad = 1.2423245906829834, avg_norm_grad = 7.349241733551025
round 33: local lr = 0.01372043788433075, norm_avg_grad = 1.264299750328064, avg_norm_grad = 6.964152812957764
round 34: local lr = 0.014821968972682953, norm_avg_grad = 1.319169521331787, avg_norm_grad = 6.726372718811035

>>> Round:   35 / Acc: 71.387% / Loss: 1.6294 /Time: 3.89s
======================================================================================================

= Test = round: 35 / acc: 73.240% / loss: 1.6137 / Time: 0.77s
======================================================================================================

round 35: local lr = 0.015944477170705795, norm_avg_grad = 1.4056600332260132, avg_norm_grad = 6.662791728973389
round 36: local lr = 0.017008231952786446, norm_avg_grad = 1.4485254287719727, avg_norm_grad = 6.436550140380859
round 37: local lr = 0.018899360671639442, norm_avg_grad = 1.5308712720870972, avg_norm_grad = 6.121780872344971
round 38: local lr = 0.021998699754476547, norm_avg_grad = 1.6415343284606934, avg_norm_grad = 5.639481544494629
round 39: local lr = 0.028458930552005768, norm_avg_grad = 1.8082629442214966, avg_norm_grad = 4.8020782470703125

>>> Round:   40 / Acc: 63.197% / Loss: 1.6770 /Time: 3.99s
======================================================================================================

= Test = round: 40 / acc: 65.100% / loss: 1.6590 / Time: 0.75s
======================================================================================================

round 40: local lr = 0.036562271416187286, norm_avg_grad = 1.825568675994873, avg_norm_grad = 3.7735595703125
round 41: local lr = 0.04250252619385719, norm_avg_grad = 1.6907410621643066, avg_norm_grad = 3.0064122676849365
round 42: local lr = 0.043799739331007004, norm_avg_grad = 1.6948951482772827, avg_norm_grad = 2.924539804458618
round 43: local lr = 0.04200977459549904, norm_avg_grad = 1.8516838550567627, avg_norm_grad = 3.3312151432037354
round 44: local lr = 0.036252737045288086, norm_avg_grad = 1.8849021196365356, avg_norm_grad = 3.929471969604492

>>> Round:   45 / Acc: 72.142% / Loss: 1.1542 /Time: 3.93s
======================================================================================================

= Test = round: 45 / acc: 73.590% / loss: 1.1275 / Time: 0.75s
======================================================================================================

round 45: local lr = 0.028386307880282402, norm_avg_grad = 1.7020232677459717, avg_norm_grad = 4.531508445739746
round 46: local lr = 0.022099720314145088, norm_avg_grad = 1.483393907546997, avg_norm_grad = 5.072895526885986
round 47: local lr = 0.017571289092302322, norm_avg_grad = 1.2838560342788696, avg_norm_grad = 5.522031784057617
round 48: local lr = 0.015339505858719349, norm_avg_grad = 1.1638944149017334, avg_norm_grad = 5.734405994415283
round 49: local lr = 0.013513153418898582, norm_avg_grad = 1.0630667209625244, avg_norm_grad = 5.945522785186768

>>> Round:   50 / Acc: 80.303% / Loss: 0.7263 /Time: 3.88s
======================================================================================================

= Test = round: 50 / acc: 81.970% / loss: 0.6931 / Time: 0.74s
======================================================================================================

round 50: local lr = 0.011967924423515797, norm_avg_grad = 0.9618782997131348, avg_norm_grad = 6.0741777420043945
round 51: local lr = 0.010925715789198875, norm_avg_grad = 0.8939175009727478, avg_norm_grad = 6.183491230010986
round 52: local lr = 0.01064755953848362, norm_avg_grad = 0.8708144426345825, avg_norm_grad = 6.1810431480407715
round 53: local lr = 0.010418032296001911, norm_avg_grad = 0.8512054681777954, avg_norm_grad = 6.174970626831055
round 54: local lr = 0.010028563439846039, norm_avg_grad = 0.8140435218811035, avg_norm_grad = 6.134725093841553

>>> Round:   55 / Acc: 81.998% / Loss: 0.6472 /Time: 3.96s
======================================================================================================

= Test = round: 55 / acc: 83.570% / loss: 0.6137 / Time: 0.77s
======================================================================================================

round 55: local lr = 0.01003397349268198, norm_avg_grad = 0.8120021224021912, avg_norm_grad = 6.11604118347168
round 56: local lr = 0.009622924961149693, norm_avg_grad = 0.7788233160972595, avg_norm_grad = 6.11671257019043
round 57: local lr = 0.009180993773043156, norm_avg_grad = 0.7452319860458374, avg_norm_grad = 6.13462495803833
round 58: local lr = 0.009237203747034073, norm_avg_grad = 0.7444742321968079, avg_norm_grad = 6.091094970703125
round 59: local lr = 0.00964718870818615, norm_avg_grad = 0.7673196792602539, avg_norm_grad = 6.011208534240723

>>> Round:   60 / Acc: 83.098% / Loss: 0.6031 /Time: 3.89s
======================================================================================================

= Test = round: 60 / acc: 84.630% / loss: 0.5700 / Time: 0.76s
======================================================================================================

round 60: local lr = 0.009574973955750465, norm_avg_grad = 0.7553054094314575, avg_norm_grad = 5.961714744567871
round 61: local lr = 0.009640524163842201, norm_avg_grad = 0.7535507678985596, avg_norm_grad = 5.90742301940918
round 62: local lr = 0.009508133865892887, norm_avg_grad = 0.73710697889328, avg_norm_grad = 5.858972549438477
round 63: local lr = 0.009081969037652016, norm_avg_grad = 0.7055506110191345, avg_norm_grad = 5.871301174163818
round 64: local lr = 0.008999577723443508, norm_avg_grad = 0.692188560962677, avg_norm_grad = 5.812841892242432

>>> Round:   65 / Acc: 84.081% / Loss: 0.5666 /Time: 3.87s
======================================================================================================

= Test = round: 65 / acc: 85.850% / loss: 0.5333 / Time: 0.73s
======================================================================================================

round 65: local lr = 0.00900291558355093, norm_avg_grad = 0.6902422904968262, avg_norm_grad = 5.794348239898682
round 66: local lr = 0.00893641822040081, norm_avg_grad = 0.68349689245224, avg_norm_grad = 5.780418395996094
round 67: local lr = 0.008613649755716324, norm_avg_grad = 0.6549057960510254, avg_norm_grad = 5.746161937713623
round 68: local lr = 0.009190624579787254, norm_avg_grad = 0.691325306892395, avg_norm_grad = 5.684910774230957
round 69: local lr = 0.008755220100283623, norm_avg_grad = 0.6541140079498291, avg_norm_grad = 5.646412372589111

>>> Round:   70 / Acc: 84.887% / Loss: 0.5387 /Time: 3.85s
======================================================================================================

= Test = round: 70 / acc: 86.510% / loss: 0.5062 / Time: 0.73s
======================================================================================================

round 70: local lr = 0.008687862195074558, norm_avg_grad = 0.6428632140159607, avg_norm_grad = 5.592318058013916
round 71: local lr = 0.008448811247944832, norm_avg_grad = 0.6233773827552795, avg_norm_grad = 5.576241970062256
round 72: local lr = 0.00828482024371624, norm_avg_grad = 0.6108419895172119, avg_norm_grad = 5.572268009185791
round 73: local lr = 0.008713463321328163, norm_avg_grad = 0.6384429335594177, avg_norm_grad = 5.537547588348389
round 74: local lr = 0.009210148826241493, norm_avg_grad = 0.6667383313179016, avg_norm_grad = 5.471104145050049

>>> Round:   75 / Acc: 85.450% / Loss: 0.5136 /Time: 3.82s
======================================================================================================

= Test = round: 75 / acc: 87.040% / loss: 0.4809 / Time: 0.73s
======================================================================================================

round 75: local lr = 0.009354655630886555, norm_avg_grad = 0.6730484366416931, avg_norm_grad = 5.437568187713623
round 76: local lr = 0.009181617759168148, norm_avg_grad = 0.6542889475822449, avg_norm_grad = 5.385631084442139
round 77: local lr = 0.008667614310979843, norm_avg_grad = 0.6141937971115112, avg_norm_grad = 5.355401515960693
round 78: local lr = 0.00848961528390646, norm_avg_grad = 0.6032496690750122, avg_norm_grad = 5.370259761810303
round 79: local lr = 0.008289746008813381, norm_avg_grad = 0.5861722230911255, avg_norm_grad = 5.344046592712402

>>> Round:   80 / Acc: 86.229% / Loss: 0.4886 /Time: 3.86s
======================================================================================================

= Test = round: 80 / acc: 87.620% / loss: 0.4567 / Time: 0.73s
======================================================================================================

round 80: local lr = 0.008571207523345947, norm_avg_grad = 0.6002480983734131, avg_norm_grad = 5.292672157287598
round 81: local lr = 0.008352470584213734, norm_avg_grad = 0.5832772254943848, avg_norm_grad = 5.277719497680664
round 82: local lr = 0.008364477194845676, norm_avg_grad = 0.5782678127288818, avg_norm_grad = 5.224881649017334
round 83: local lr = 0.00824069045484066, norm_avg_grad = 0.5687209963798523, avg_norm_grad = 5.215811252593994
round 84: local lr = 0.007885917089879513, norm_avg_grad = 0.5440548062324524, avg_norm_grad = 5.214067459106445

>>> Round:   85 / Acc: 86.880% / Loss: 0.4648 /Time: 3.84s
======================================================================================================

= Test = round: 85 / acc: 88.130% / loss: 0.4335 / Time: 0.73s
======================================================================================================

round 85: local lr = 0.007885852828621864, norm_avg_grad = 0.5424492955207825, avg_norm_grad = 5.198722839355469
round 86: local lr = 0.0079738674685359, norm_avg_grad = 0.5447954535484314, avg_norm_grad = 5.163577079772949
round 87: local lr = 0.008210188709199429, norm_avg_grad = 0.5570217370986938, avg_norm_grad = 5.1274943351745605
round 88: local lr = 0.008081031031906605, norm_avg_grad = 0.5437495708465576, avg_norm_grad = 5.085320949554443
round 89: local lr = 0.008546043187379837, norm_avg_grad = 0.5705065131187439, avg_norm_grad = 5.045238971710205

>>> Round:   90 / Acc: 87.240% / Loss: 0.4520 /Time: 3.95s
======================================================================================================

= Test = round: 90 / acc: 88.570% / loss: 0.4208 / Time: 0.75s
======================================================================================================

round 90: local lr = 0.009233309887349606, norm_avg_grad = 0.6118234395980835, avg_norm_grad = 5.007891654968262
round 91: local lr = 0.009327526204288006, norm_avg_grad = 0.6113743782043457, avg_norm_grad = 4.95366907119751
round 92: local lr = 0.008900610730051994, norm_avg_grad = 0.579754114151001, avg_norm_grad = 4.922778129577637
round 93: local lr = 0.008476881310343742, norm_avg_grad = 0.5491940975189209, avg_norm_grad = 4.896389484405518
round 94: local lr = 0.008148973807692528, norm_avg_grad = 0.5254600048065186, avg_norm_grad = 4.873297691345215

>>> Round:   95 / Acc: 87.738% / Loss: 0.4337 /Time: 3.90s
======================================================================================================

= Test = round: 95 / acc: 88.980% / loss: 0.4035 / Time: 0.74s
======================================================================================================

round 95: local lr = 0.008578554727137089, norm_avg_grad = 0.5522234439849854, avg_norm_grad = 4.865045547485352
round 96: local lr = 0.008111574687063694, norm_avg_grad = 0.521109938621521, avg_norm_grad = 4.855236530303955
round 97: local lr = 0.00827434379607439, norm_avg_grad = 0.529306173324585, avg_norm_grad = 4.83458948135376
round 98: local lr = 0.007921990007162094, norm_avg_grad = 0.5050221085548401, avg_norm_grad = 4.817950248718262
round 99: local lr = 0.008054064586758614, norm_avg_grad = 0.5105708241462708, avg_norm_grad = 4.791009902954102

>>> Round:  100 / Acc: 88.188% / Loss: 0.4159 /Time: 3.95s
======================================================================================================

= Test = round: 100 / acc: 89.350% / loss: 0.3855 / Time: 0.76s
======================================================================================================

round 100: local lr = 0.008223394863307476, norm_avg_grad = 0.5176897048950195, avg_norm_grad = 4.757782459259033
round 101: local lr = 0.007937501184642315, norm_avg_grad = 0.49853697419166565, avg_norm_grad = 4.7467875480651855
round 102: local lr = 0.007684904616326094, norm_avg_grad = 0.4814509153366089, avg_norm_grad = 4.734779357910156
round 103: local lr = 0.007623499259352684, norm_avg_grad = 0.4762392044067383, avg_norm_grad = 4.721250057220459
round 104: local lr = 0.007767578586935997, norm_avg_grad = 0.48242107033729553, avg_norm_grad = 4.693824291229248

>>> Round:  105 / Acc: 88.592% / Loss: 0.3998 /Time: 3.98s
======================================================================================================

= Test = round: 105 / acc: 89.920% / loss: 0.3703 / Time: 0.75s
======================================================================================================

round 105: local lr = 0.007710828445851803, norm_avg_grad = 0.4772511422634125, avg_norm_grad = 4.677697658538818
round 106: local lr = 0.0078016361221671104, norm_avg_grad = 0.48000064492225647, avg_norm_grad = 4.649886608123779
round 107: local lr = 0.008264416828751564, norm_avg_grad = 0.5044441223144531, avg_norm_grad = 4.613038063049316
round 108: local lr = 0.008044300600886345, norm_avg_grad = 0.48851197957992554, avg_norm_grad = 4.5895819664001465
round 109: local lr = 0.008168104104697704, norm_avg_grad = 0.4908810257911682, avg_norm_grad = 4.541937828063965

>>> Round:  110 / Acc: 89.037% / Loss: 0.3882 /Time: 3.94s
======================================================================================================

= Test = round: 110 / acc: 90.280% / loss: 0.3593 / Time: 0.75s
======================================================================================================

round 110: local lr = 0.007689718157052994, norm_avg_grad = 0.46092304587364197, avg_norm_grad = 4.530062675476074
round 111: local lr = 0.0076061515137553215, norm_avg_grad = 0.45441490411758423, avg_norm_grad = 4.515166759490967
round 112: local lr = 0.007503845728933811, norm_avg_grad = 0.4469636380672455, avg_norm_grad = 4.501678943634033
round 113: local lr = 0.007430506870150566, norm_avg_grad = 0.4414546489715576, avg_norm_grad = 4.490077972412109
round 114: local lr = 0.007340618409216404, norm_avg_grad = 0.43631914258003235, avg_norm_grad = 4.492187023162842

>>> Round:  115 / Acc: 89.415% / Loss: 0.3729 /Time: 3.95s
======================================================================================================

= Test = round: 115 / acc: 90.500% / loss: 0.3446 / Time: 0.76s
======================================================================================================

round 115: local lr = 0.007100063841789961, norm_avg_grad = 0.4213069975376129, avg_norm_grad = 4.484589099884033
round 116: local lr = 0.007318223360925913, norm_avg_grad = 0.432450532913208, avg_norm_grad = 4.465982437133789
round 117: local lr = 0.007335316389799118, norm_avg_grad = 0.4323989748954773, avg_norm_grad = 4.455044269561768
round 118: local lr = 0.007362171541899443, norm_avg_grad = 0.43161237239837646, avg_norm_grad = 4.430718421936035
round 119: local lr = 0.007642998825758696, norm_avg_grad = 0.445935994386673, avg_norm_grad = 4.409556865692139

>>> Round:  120 / Acc: 89.697% / Loss: 0.3629 /Time: 3.91s
======================================================================================================

= Test = round: 120 / acc: 90.800% / loss: 0.3348 / Time: 0.75s
======================================================================================================

round 120: local lr = 0.007756209932267666, norm_avg_grad = 0.4491923451423645, avg_norm_grad = 4.37692403793335
round 121: local lr = 0.007735890801995993, norm_avg_grad = 0.4462737739086151, avg_norm_grad = 4.359907150268555
round 122: local lr = 0.007845822721719742, norm_avg_grad = 0.45119568705558777, avg_norm_grad = 4.346229553222656
round 123: local lr = 0.0072635468095541, norm_avg_grad = 0.4153773784637451, avg_norm_grad = 4.321955680847168
round 124: local lr = 0.007232251577079296, norm_avg_grad = 0.4123409688472748, avg_norm_grad = 4.308927536010742

>>> Round:  125 / Acc: 90.004% / Loss: 0.3520 /Time: 3.90s
======================================================================================================

= Test = round: 125 / acc: 91.030% / loss: 0.3248 / Time: 0.74s
======================================================================================================

round 125: local lr = 0.007077299058437347, norm_avg_grad = 0.4016205370426178, avg_norm_grad = 4.288787841796875
round 126: local lr = 0.007149950601160526, norm_avg_grad = 0.4055974781513214, avg_norm_grad = 4.287246227264404
round 127: local lr = 0.007333594374358654, norm_avg_grad = 0.411990761756897, avg_norm_grad = 4.2457733154296875
round 128: local lr = 0.007274487987160683, norm_avg_grad = 0.40953606367111206, avg_norm_grad = 4.254768371582031
round 129: local lr = 0.007226102519780397, norm_avg_grad = 0.404558926820755, avg_norm_grad = 4.231203079223633

>>> Round:  130 / Acc: 90.304% / Loss: 0.3409 /Time: 3.88s
======================================================================================================

= Test = round: 130 / acc: 91.250% / loss: 0.3141 / Time: 0.75s
======================================================================================================

round 130: local lr = 0.007046951446682215, norm_avg_grad = 0.39388924837112427, avg_norm_grad = 4.224341869354248
round 131: local lr = 0.007174036465585232, norm_avg_grad = 0.39926716685295105, avg_norm_grad = 4.206164360046387
round 132: local lr = 0.007191324606537819, norm_avg_grad = 0.40005606412887573, avg_norm_grad = 4.204343318939209
round 133: local lr = 0.00704254861921072, norm_avg_grad = 0.3890819847583771, avg_norm_grad = 4.175394058227539
round 134: local lr = 0.0074125719256699085, norm_avg_grad = 0.4068206548690796, avg_norm_grad = 4.147823810577393

>>> Round:  135 / Acc: 90.603% / Loss: 0.3339 /Time: 3.97s
======================================================================================================

= Test = round: 135 / acc: 91.420% / loss: 0.3073 / Time: 0.77s
======================================================================================================

round 135: local lr = 0.007447701878845692, norm_avg_grad = 0.4055623412132263, avg_norm_grad = 4.115489959716797
round 136: local lr = 0.007147790864109993, norm_avg_grad = 0.3893207609653473, avg_norm_grad = 4.116441249847412
round 137: local lr = 0.006790520157665014, norm_avg_grad = 0.36832886934280396, avg_norm_grad = 4.099387168884277
round 138: local lr = 0.00703195296227932, norm_avg_grad = 0.38016536831855774, avg_norm_grad = 4.085853576660156
round 139: local lr = 0.007250521332025528, norm_avg_grad = 0.3901861011981964, avg_norm_grad = 4.067136764526367

>>> Round:  140 / Acc: 90.876% / Loss: 0.3249 /Time: 3.96s
======================================================================================================

= Test = round: 140 / acc: 91.570% / loss: 0.2990 / Time: 0.76s
======================================================================================================

round 140: local lr = 0.007192261517047882, norm_avg_grad = 0.3845624029636383, avg_norm_grad = 4.040987968444824
round 141: local lr = 0.007217929698526859, norm_avg_grad = 0.38431110978126526, avg_norm_grad = 4.023986339569092
round 142: local lr = 0.007912526838481426, norm_avg_grad = 0.42067673802375793, avg_norm_grad = 4.0180888175964355
round 143: local lr = 0.007409585174173117, norm_avg_grad = 0.3924320638179779, avg_norm_grad = 4.002734661102295
round 144: local lr = 0.007267368957400322, norm_avg_grad = 0.38203057646751404, avg_norm_grad = 3.972895383834839

>>> Round:  145 / Acc: 91.105% / Loss: 0.3163 /Time: 4.00s
======================================================================================================

= Test = round: 145 / acc: 91.740% / loss: 0.2909 / Time: 0.76s
======================================================================================================

round 145: local lr = 0.007138391491025686, norm_avg_grad = 0.37537845969200134, avg_norm_grad = 3.974250316619873
round 146: local lr = 0.0070374771021306515, norm_avg_grad = 0.3700708746910095, avg_norm_grad = 3.974240303039551
round 147: local lr = 0.0071350024081766605, norm_avg_grad = 0.3750307857990265, avg_norm_grad = 3.9724552631378174
round 148: local lr = 0.006991526111960411, norm_avg_grad = 0.36651673913002014, avg_norm_grad = 3.9619412422180176
round 149: local lr = 0.007583712227642536, norm_avg_grad = 0.39511579275131226, avg_norm_grad = 3.9375741481781006

>>> Round:  150 / Acc: 91.310% / Loss: 0.3077 /Time: 3.87s
======================================================================================================

= Test = round: 150 / acc: 91.920% / loss: 0.2827 / Time: 0.74s
======================================================================================================

round 150: local lr = 0.007310350425541401, norm_avg_grad = 0.37945955991744995, avg_norm_grad = 3.9229564666748047
round 151: local lr = 0.00814446434378624, norm_avg_grad = 0.42020857334136963, avg_norm_grad = 3.899317502975464
round 152: local lr = 0.007580711971968412, norm_avg_grad = 0.3899070918560028, avg_norm_grad = 3.887204170227051
round 153: local lr = 0.00706481421366334, norm_avg_grad = 0.3634728491306305, avg_norm_grad = 3.8882789611816406
round 154: local lr = 0.006896244361996651, norm_avg_grad = 0.35473859310150146, avg_norm_grad = 3.887603759765625

>>> Round:  155 / Acc: 91.509% / Loss: 0.2990 /Time: 5.51s
======================================================================================================

= Test = round: 155 / acc: 92.180% / loss: 0.2744 / Time: 1.03s
======================================================================================================

round 155: local lr = 0.006870790384709835, norm_avg_grad = 0.3520651161670685, avg_norm_grad = 3.872598886489868
round 156: local lr = 0.007184225134551525, norm_avg_grad = 0.36788371205329895, avg_norm_grad = 3.8700523376464844
round 157: local lr = 0.006836814805865288, norm_avg_grad = 0.34863004088401794, avg_norm_grad = 3.8538713455200195
round 158: local lr = 0.007128557655960321, norm_avg_grad = 0.3613426089286804, avg_norm_grad = 3.830925703048706
round 159: local lr = 0.0065514626912772655, norm_avg_grad = 0.3315059542655945, avg_norm_grad = 3.824188470840454

>>> Round:  160 / Acc: 91.657% / Loss: 0.2918 /Time: 3.90s
======================================================================================================

= Test = round: 160 / acc: 92.320% / loss: 0.2676 / Time: 0.75s
======================================================================================================

round 160: local lr = 0.006756703834980726, norm_avg_grad = 0.34104499220848083, avg_norm_grad = 3.814723014831543
round 161: local lr = 0.006609081756323576, norm_avg_grad = 0.3323647975921631, avg_norm_grad = 3.8006694316864014
round 162: local lr = 0.006680802907794714, norm_avg_grad = 0.3352953791618347, avg_norm_grad = 3.793020009994507
round 163: local lr = 0.0065875728614628315, norm_avg_grad = 0.3301317095756531, avg_norm_grad = 3.7874596118927
round 164: local lr = 0.006437511648982763, norm_avg_grad = 0.32207778096199036, avg_norm_grad = 3.781193733215332

>>> Round:  165 / Acc: 91.852% / Loss: 0.2837 /Time: 5.39s
======================================================================================================

= Test = round: 165 / acc: 92.460% / loss: 0.2603 / Time: 1.03s
======================================================================================================

round 165: local lr = 0.005995923653244972, norm_avg_grad = 0.30006006360054016, avg_norm_grad = 3.782146453857422
round 166: local lr = 0.006264430470764637, norm_avg_grad = 0.3127453923225403, avg_norm_grad = 3.773076057434082
round 167: local lr = 0.0061992499977350235, norm_avg_grad = 0.3092750012874603, avg_norm_grad = 3.7704386711120605
round 168: local lr = 0.006105212494730949, norm_avg_grad = 0.30389681458473206, avg_norm_grad = 3.761937379837036
round 169: local lr = 0.006544668693095446, norm_avg_grad = 0.32508739829063416, avg_norm_grad = 3.754038095474243

>>> Round:  170 / Acc: 91.978% / Loss: 0.2782 /Time: 5.42s
======================================================================================================

= Test = round: 170 / acc: 92.650% / loss: 0.2550 / Time: 1.02s
======================================================================================================

round 170: local lr = 0.006832804996520281, norm_avg_grad = 0.3371492326259613, avg_norm_grad = 3.7291455268859863
round 171: local lr = 0.006588342599570751, norm_avg_grad = 0.32452908158302307, avg_norm_grad = 3.722748041152954
round 172: local lr = 0.006671442184597254, norm_avg_grad = 0.32681939005851746, avg_norm_grad = 3.7023229598999023
round 173: local lr = 0.006949912291020155, norm_avg_grad = 0.33961722254753113, avg_norm_grad = 3.6931469440460205
round 174: local lr = 0.006228100508451462, norm_avg_grad = 0.3035528063774109, avg_norm_grad = 3.683535575866699

>>> Round:  175 / Acc: 92.133% / Loss: 0.2718 /Time: 3.85s
======================================================================================================

= Test = round: 175 / acc: 92.770% / loss: 0.2489 / Time: 0.74s
======================================================================================================

round 175: local lr = 0.00621409434825182, norm_avg_grad = 0.30279141664505005, avg_norm_grad = 3.6825778484344482
round 176: local lr = 0.005754446145147085, norm_avg_grad = 0.28118348121643066, avg_norm_grad = 3.6929421424865723
round 177: local lr = 0.0057514738291502, norm_avg_grad = 0.2813110649585724, avg_norm_grad = 3.6965267658233643
round 178: local lr = 0.005721521098166704, norm_avg_grad = 0.2803114950656891, avg_norm_grad = 3.7026753425598145
round 179: local lr = 0.005749841220676899, norm_avg_grad = 0.2809024453163147, avg_norm_grad = 3.6922056674957275

>>> Round:  180 / Acc: 92.249% / Loss: 0.2642 /Time: 3.91s
======================================================================================================

= Test = round: 180 / acc: 92.910% / loss: 0.2417 / Time: 0.73s
======================================================================================================

round 180: local lr = 0.005662745796144009, norm_avg_grad = 0.2774006724357605, avg_norm_grad = 3.7022576332092285
round 181: local lr = 0.006274379324167967, norm_avg_grad = 0.3064360022544861, avg_norm_grad = 3.6910948753356934
round 182: local lr = 0.0061370753683149815, norm_avg_grad = 0.29788902401924133, avg_norm_grad = 3.668421745300293
round 183: local lr = 0.006126631982624531, norm_avg_grad = 0.29617512226104736, avg_norm_grad = 3.6535327434539795
round 184: local lr = 0.005403461400419474, norm_avg_grad = 0.2617987394332886, avg_norm_grad = 3.661691188812256

>>> Round:  185 / Acc: 92.408% / Loss: 0.2590 /Time: 3.86s
======================================================================================================

= Test = round: 185 / acc: 93.040% / loss: 0.2367 / Time: 0.74s
======================================================================================================

round 185: local lr = 0.0052977921441197395, norm_avg_grad = 0.2572323679924011, avg_norm_grad = 3.6695845127105713
round 186: local lr = 0.0052283587865531445, norm_avg_grad = 0.2538704574108124, avg_norm_grad = 3.669720411300659
round 187: local lr = 0.005306331906467676, norm_avg_grad = 0.25827717781066895, avg_norm_grad = 3.6785600185394287
round 188: local lr = 0.005349484272301197, norm_avg_grad = 0.26065394282341003, avg_norm_grad = 3.682464599609375
round 189: local lr = 0.005022470373660326, norm_avg_grad = 0.24457761645317078, avg_norm_grad = 3.6803197860717773

>>> Round:  190 / Acc: 92.524% / Loss: 0.2534 /Time: 3.85s
======================================================================================================

= Test = round: 190 / acc: 93.120% / loss: 0.2313 / Time: 0.73s
======================================================================================================

round 190: local lr = 0.005711060017347336, norm_avg_grad = 0.27818891406059265, avg_norm_grad = 3.681368827819824
round 191: local lr = 0.006542254704982042, norm_avg_grad = 0.3172314167022705, avg_norm_grad = 3.664670705795288
round 192: local lr = 0.0059803556650877, norm_avg_grad = 0.28650474548339844, avg_norm_grad = 3.620687246322632
round 193: local lr = 0.006133037153631449, norm_avg_grad = 0.2919696867465973, avg_norm_grad = 3.5978941917419434
round 194: local lr = 0.006001274101436138, norm_avg_grad = 0.2849067151546478, avg_norm_grad = 3.587942361831665

>>> Round:  195 / Acc: 92.686% / Loss: 0.2507 /Time: 3.86s
======================================================================================================

= Test = round: 195 / acc: 93.200% / loss: 0.2292 / Time: 0.75s
======================================================================================================

round 195: local lr = 0.005964955780655146, norm_avg_grad = 0.28243938088417053, avg_norm_grad = 3.578526496887207
round 196: local lr = 0.006253257393836975, norm_avg_grad = 0.29564356803894043, avg_norm_grad = 3.5731260776519775
round 197: local lr = 0.006864235270768404, norm_avg_grad = 0.32205498218536377, avg_norm_grad = 3.5458805561065674
round 198: local lr = 0.0063785165548324585, norm_avg_grad = 0.2991756796836853, avg_norm_grad = 3.544808864593506
round 199: local lr = 0.006180910859256983, norm_avg_grad = 0.288601815700531, avg_norm_grad = 3.5288467407226562

>>> Round:  200 / Acc: 92.827% / Loss: 0.2465 /Time: 3.85s
======================================================================================================

= Test = round: 200 / acc: 93.350% / loss: 0.2250 / Time: 0.73s
======================================================================================================

>>> Training model_ft
Epoch: 001, Train_loss: 1.0775, Train_acc: 0.6709, Test_loss: 1.0514, Test_acc: 0.6799
Epoch: 006, Train_loss: 0.8501, Train_acc: 0.7361, Test_loss: 0.8412, Test_acc: 0.7389
Epoch: 011, Train_loss: 0.7958, Train_acc: 0.7490, Test_loss: 0.8047, Test_acc: 0.7495
Epoch: 016, Train_loss: 0.7748, Train_acc: 0.7545, Test_loss: 0.7912, Test_acc: 0.7521
Epoch: 021, Train_loss: 0.7261, Train_acc: 0.7697, Test_loss: 0.7559, Test_acc: 0.7617
Epoch: 026, Train_loss: 0.7100, Train_acc: 0.7733, Test_loss: 0.7485, Test_acc: 0.7624
Epoch: 031, Train_loss: 0.6965, Train_acc: 0.7769, Test_loss: 0.7407, Test_acc: 0.7642
Epoch: 036, Train_loss: 0.6973, Train_acc: 0.7736, Test_loss: 0.7507, Test_acc: 0.7615
Epoch: 041, Train_loss: 0.6651, Train_acc: 0.7851, Test_loss: 0.7236, Test_acc: 0.7664
Epoch: 046, Train_loss: 0.6713, Train_acc: 0.7826, Test_loss: 0.7420, Test_acc: 0.7627
Epoch: 051, Train_loss: 0.6561, Train_acc: 0.7865, Test_loss: 0.7280, Test_acc: 0.7662
Epoch: 056, Train_loss: 0.6595, Train_acc: 0.7854, Test_loss: 0.7379, Test_acc: 0.7646
Epoch: 061, Train_loss: 0.6359, Train_acc: 0.7937, Test_loss: 0.7235, Test_acc: 0.7675
Epoch: 066, Train_loss: 0.6340, Train_acc: 0.7940, Test_loss: 0.7256, Test_acc: 0.7647
Epoch: 071, Train_loss: 0.6247, Train_acc: 0.7968, Test_loss: 0.7201, Test_acc: 0.7674
Epoch: 076, Train_loss: 0.6263, Train_acc: 0.7951, Test_loss: 0.7281, Test_acc: 0.7665
Epoch: 081, Train_loss: 0.6165, Train_acc: 0.7971, Test_loss: 0.7151, Test_acc: 0.7646
Epoch: 086, Train_loss: 0.6118, Train_acc: 0.7996, Test_loss: 0.7215, Test_acc: 0.7636
Epoch: 091, Train_loss: 0.6102, Train_acc: 0.7984, Test_loss: 0.7170, Test_acc: 0.7668
Epoch: 096, Train_loss: 0.6066, Train_acc: 0.8018, Test_loss: 0.7204, Test_acc: 0.7685
Epoch: 101, Train_loss: 0.6039, Train_acc: 0.8021, Test_loss: 0.7194, Test_acc: 0.7677
Epoch: 106, Train_loss: 0.6019, Train_acc: 0.8025, Test_loss: 0.7249, Test_acc: 0.7691
Epoch: 111, Train_loss: 0.6073, Train_acc: 0.7994, Test_loss: 0.7266, Test_acc: 0.7681
Epoch: 116, Train_loss: 0.5892, Train_acc: 0.8067, Test_loss: 0.7230, Test_acc: 0.7661
Epoch: 121, Train_loss: 0.5887, Train_acc: 0.8057, Test_loss: 0.7162, Test_acc: 0.7696
Epoch: 126, Train_loss: 0.5968, Train_acc: 0.8026, Test_loss: 0.7302, Test_acc: 0.7662
Fine-tuning early stopped. Model saved at ./models/ft_checkpoints/20230927172317_model_ft.pt.
Model saved at ./models/ft_checkpoints/20230927172317_model_ft.pt.
>>> Fine-tuning done!
>>> Evaluating model_source_only
model_ft: [0.5839929903740175, 0.8070541177268182, 0.7131525059995777, 0.7682479724475059]
model_source_only: [2.0063615547669356, 0.4526702937238352, 2.0216086947670275, 0.4538384623930674]
fl_test_acc_mean 0.9328
model_source_only_test_acc_mean 0.4538384623930674
model_ft_test_acc_mean 0.7682479724475059
