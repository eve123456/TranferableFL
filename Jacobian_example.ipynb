{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichien3/miniconda3/envs/FLTransfer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Arguments:\n",
      "\t             algo : fedavg4\n",
      "\t       batch_size : 64\n",
      "\tclients_per_round : 10\n",
      "\t          dataset : mnist_all_data_1_random_niid\n",
      "\t           device : 0\n",
      "\t              dis : \n",
      "\t       eval_every : 5\n",
      "\t finetune_dataset : mnist-m\n",
      "\t  finetune_epochs : 100\n",
      "\t      finetune_lr : 0.01\n",
      "\t      finetune_wd : 0.0\n",
      "\t    ft_batch_size : 1024\n",
      "\t              gpu : True\n",
      "\t      input_shape : (1, 28, 28)\n",
      "\t               lr : 0.01\n",
      "\t            model : LeNet\n",
      "\t        noaverage : False\n",
      "\t          noprint : False\n",
      "\t        num_class : 10\n",
      "\t        num_epoch : 10\n",
      "\t        num_round : 200\n",
      "\t             seed : 0\n",
      "\t               wd : 0.0\n",
      ">>> Read data from:\n",
      "     ./data/mnist/data/train/all_data_1_random_niid.pkl\n",
      "     ./data/mnist/data/test/all_data_1_random_niid.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from src.utils.worker_utils import read_data\n",
    "from config import OPTIMIZERS, DATASETS, MODEL_PARAMS, TRAINERS\n",
    "\n",
    "from data.data_loader import GetLoader\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def read_options():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--algo',\n",
    "                        help='name of trainer;',\n",
    "                        type=str,\n",
    "                        choices=OPTIMIZERS,\n",
    "                        default='fedavg4')\n",
    "    parser.add_argument('--dataset',\n",
    "                        help='name of dataset;',\n",
    "                        type=str,\n",
    "                        default='mnist_all_data_1_random_niid')\n",
    "                        # default='svhn_all_data_1_random_niid')\n",
    "    parser.add_argument('--finetune_dataset',\n",
    "                        help='name of finetune dataset;',\n",
    "                        type=str,\n",
    "                        default='mnist-m')\n",
    "#                         default = 'mnist')\n",
    "    parser.add_argument('--finetune_lr',\n",
    "                        help='lr for finetune;',\n",
    "                        type=float,\n",
    "                        default=0.01)\n",
    "    parser.add_argument('--finetune_wd',\n",
    "                        help='weight decay for finetune;',\n",
    "                        type=float,\n",
    "                        default=0.0)\n",
    "    parser.add_argument('--finetune_epochs',\n",
    "                        help='epochs for finetune;',\n",
    "                        type=int,\n",
    "                        default=100)\n",
    "#                         default = 2)\n",
    "    parser.add_argument('--model',\n",
    "                        help='name of model;',\n",
    "                        type=str,\n",
    "                        default='LeNet')\n",
    "                        # default = 'cnn')\n",
    "    parser.add_argument('--wd',\n",
    "                        help='weight decay parameter;',\n",
    "                        type=float,\n",
    "                        default=0.0)\n",
    "    parser.add_argument('--gpu',\n",
    "                        action='store_true',\n",
    "                        default=True,\n",
    "                        help='use gpu (default: False)')\n",
    "    parser.add_argument('--noprint',\n",
    "                        action='store_true',\n",
    "                        default=False,\n",
    "                        help='whether to print inner result (default: False)')\n",
    "    parser.add_argument('--noaverage',\n",
    "                        action='store_true',\n",
    "                        default=False,\n",
    "                        help='whether to only average local solutions (default: True)')\n",
    "    parser.add_argument('--device',\n",
    "                        help='selected CUDA device',\n",
    "                        default=0,\n",
    "                        type=int)\n",
    "    parser.add_argument('--num_round',\n",
    "                        help='number of rounds to simulate;',\n",
    "                        type=int,\n",
    "                        # default=1000)\n",
    "                        default = 200)\n",
    "                        \n",
    "    parser.add_argument('--eval_every',\n",
    "                        help='evaluate every ____ rounds;',\n",
    "                        type=int,\n",
    "                        default=5)\n",
    "                        \n",
    "    parser.add_argument('--clients_per_round',\n",
    "                        help='number of clients trained per round;',\n",
    "                        type=int,\n",
    "                        default=10)\n",
    "                       \n",
    "    parser.add_argument('--batch_size',\n",
    "                        help='batch size when clients train on data;',\n",
    "                        type=int,\n",
    "                        default=64)\n",
    "    parser.add_argument('--ft_batch_size',\n",
    "                        help='batch size when clients train on data;',\n",
    "                        type=int,\n",
    "                        default=1024)\n",
    "    parser.add_argument('--num_epoch',\n",
    "                        help='number of epochs when clients train on data;',\n",
    "                        type=int,\n",
    "                        default=10)\n",
    "                        # default =1)\n",
    "    \n",
    "    parser.add_argument('--lr',\n",
    "                        help='learning rate for inner solver;',\n",
    "                        type=float,\n",
    "                        default=0.01)\n",
    "    parser.add_argument('--seed',\n",
    "                        help='seed for randomness;',\n",
    "                        type=int,\n",
    "                        default=0)\n",
    "    parser.add_argument('--dis',\n",
    "                        help='add more information;',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parsed = parser.parse_args([])\n",
    "    options = parsed.__dict__\n",
    "    options['gpu'] = options['gpu'] and torch.cuda.is_available()\n",
    "\n",
    "    # Set seeds\n",
    "    np.random.seed(1 + options['seed'])\n",
    "    torch.manual_seed(12 + options['seed'])\n",
    "    if options['gpu']:\n",
    "        torch.cuda.manual_seed_all(123 + options['seed'])\n",
    "\n",
    "    # read data\n",
    "    idx = options['dataset'].find(\"_\")\n",
    "    if idx != -1:\n",
    "        dataset_name, sub_data = options['dataset'][:idx], options['dataset'][idx+1:]\n",
    "    else:\n",
    "        dataset_name, sub_data = options['dataset'], None\n",
    "    assert dataset_name in DATASETS, \"{} not in dataset {}!\".format(dataset_name, DATASETS)\n",
    "\n",
    "    # Add model arguments\n",
    "    options.update(MODEL_PARAMS(dataset_name, options['model']))\n",
    "\n",
    "    # Load selected trainer\n",
    "    trainer_path = 'src.trainers.%s' % options['algo']\n",
    "    mod = importlib.import_module(trainer_path)\n",
    "    trainer_class = getattr(mod, TRAINERS[options['algo']])\n",
    "\n",
    "    # Print arguments and return\n",
    "    max_length = max([len(key) for key in options.keys()])\n",
    "    fmt_string = '\\t%' + str(max_length) + 's : %s'\n",
    "    print('>>> Arguments:')\n",
    "    for keyPair in sorted(options.items()):\n",
    "        print(fmt_string % keyPair)\n",
    "\n",
    "    return options, trainer_class, dataset_name, sub_data\n",
    "\n",
    "\n",
    "# Parse command line arguments\n",
    "options, trainer_class, dataset_name, sub_data = read_options()\n",
    "options[\"dataset_name\"] = dataset_name\n",
    "\n",
    "train_path = os.path.join('./data', dataset_name, 'data', 'train')\n",
    "test_path = os.path.join('./data', dataset_name, 'data', 'test')\n",
    "\n",
    "# `dataset` is a tuple like (cids, groups, train_data, test_data)\n",
    "all_data_info = read_data(train_path, test_path, dataset_name, sub_data)\n",
    "\n",
    "# # Call appropriate trainer\n",
    "# trainer = trainer_class(options, all_data_info)\n",
    "# trainer.train()\n",
    "\n",
    "# # FL training finish here, save the latest server model\n",
    "# flat_model_param = trainer.latest_model\n",
    "# PATH = f\"./models/{options['model']}_{dataset_name}_{options['algo']}_{options['seed']}\"\n",
    "# torch.save(flat_model_param, PATH)\n",
    "\n",
    "\n",
    "\n",
    "from src.utils.torch_utils import get_flat_grad, get_state_dict, get_flat_params_from, set_flat_params_to\n",
    "from src.models.model import choose_model\n",
    "from ft_functions import *\n",
    "\n",
    "# # This is the final flattened model params\n",
    "# flat_model_params = trainer.latest_model\n",
    "\n",
    "# Optional: load from the saved model.\n",
    "PATH = f\"./models/{options['model']}_{dataset_name}_{options['algo']}_{options['seed']}\"\n",
    "flat_model_params = torch.load(PATH)\n",
    "\n",
    "\n",
    "# Initialize new models\n",
    "model_source_only = choose_model(options) # baseline: lower bound\n",
    "model_ft = choose_model(options) # baseline: standard fine-tune\n",
    "model_target_only = choose_model(options) # baseline: upper bound\n",
    "model_flag = choose_model(options) # test if the model archetecture is proper for target domain\n",
    "# If performances of model_flag and model_target_only deviates, then the model is proper for target domain.\n",
    "\n",
    "# Assign model params\n",
    "set_flat_params_to(model_source_only, flat_model_params)\n",
    "set_flat_params_to(model_ft, flat_model_params)\n",
    "# Now model is set with flat_model_params\n",
    "\n",
    "# Start fine-tuning below\n",
    "# First, freeze all but last layer\n",
    "def freeze(model):\n",
    "    for mod in model.children():\n",
    "        for params in mod.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    for params in mod.parameters():\n",
    "        params.requires_grad = True\n",
    "\n",
    "freeze(model_flag)\n",
    "freeze(model_ft)\n",
    "\n",
    "\n",
    "# # Next fine-tune the model on mnist:\n",
    "# device = torch.device(f\"cuda:{options['device']}\")\n",
    "device = torch.device('cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "kwargs = {'num_workers': 32}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "# assert options['finetune_dataset'] == 'mnist-m' # now we only support transfer from svhn to mnist.\n",
    "if options['finetune_dataset'] == \"mnist-m\":\n",
    "    target_image_root = './data/TARGET/mnist-m/mnist_m'\n",
    "\n",
    "    tg_train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')\n",
    "    tg_test_list = os.path.join(target_image_root, 'mnist_m_test_labels.txt')\n",
    "    \n",
    "    img_transform_target = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x*255,\n",
    "    transforms.Grayscale()\n",
    "    ])\n",
    "\n",
    "    tg_trainset = GetLoader(\n",
    "        data_root=os.path.join(target_image_root, 'mnist_m_train'),\n",
    "        data_list=tg_train_list,\n",
    "        transform=img_transform_target\n",
    "    )\n",
    "\n",
    "    tg_testset = GetLoader(\n",
    "        data_root=os.path.join(target_image_root, 'mnist_m_test'),\n",
    "        data_list=tg_test_list,\n",
    "        transform=img_transform_target\n",
    "    )\n",
    "\n",
    "    tg_train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=tg_trainset,\n",
    "        batch_size=options['ft_batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        **kwargs)\n",
    "\n",
    "    tg_test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=tg_testset,\n",
    "        batch_size=options['ft_batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        **kwargs)\n",
    "\n",
    "elif options['finetune_dataset'] == \"mnist\":\n",
    "    data_path = './data/TARGET/mnist'\n",
    "    \n",
    "    img_transform_target = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x*255,\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.MNIST(root=data_path, train =True, download=False, transform=img_transform_target)\n",
    "    testset = torchvision.datasets.MNIST(root=data_path, train =False, download=False, transform=img_transform_target)\n",
    "    tg_train_loader = torch.utils.data.DataLoader(trainset, batch_size=options['ft_batch_size'], shuffle=True,pin_memory=True, **kwargs)\n",
    "    tg_test_loader = torch.utils.data.DataLoader(testset, batch_size=options['ft_batch_size'], shuffle=True,pin_memory=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet_MNIST(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_target_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use functorch to get Jacobian and see if it is compatible with loss.backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from functorch import make_functional_with_buffers, vmap, grad, jacfwd, jacrev\n",
    "\n",
    "\n",
    "# This is for computing loss with fmodel.\n",
    "def compute_loss_stateless_model(fmodel, criterion, buffers, sample, target,params):\n",
    "#     # Below two lines are only need if batchsize=1\n",
    "#     batch = sample.unsqueeze(0)\n",
    "#     targets = target.unsqueeze(0)\n",
    "    \n",
    "    predictions = fmodel(params, buffers, sample)\n",
    "    loss = criterion(predictions, target)\n",
    "    return loss\n",
    "\n",
    "def assign_params(model,params):\n",
    "    model.zero_grad() # prevent aggregate gradient.\n",
    "    for p_model, p_fmodel in zip(model.parameters(),params):\n",
    "        p_model.grad = p_fmodel.grad\n",
    "        p_model.data = p_fmodel.data\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample, target in tg_train_loader:\n",
    "    \n",
    "    # setup stateless model\n",
    "    fmodel, params, buffers = make_functional_with_buffers(model_target_only)\n",
    "    \n",
    "    # compute loss. it is now a fuction of params that will return loss.\n",
    "    compute_loss_stateless_model_partial = partial(compute_loss_stateless_model,fmodel,\n",
    "                                                   criterion, buffers, sample, target)\n",
    "    # compute jacobian with input: params\n",
    "    jacobian = jacrev(compute_loss_stateless_model_partial)(params)\n",
    "    \n",
    "    # In this example, we assume below operation is f(). You should replace it with your method.\n",
    "    temp = 0\n",
    "    for j in jacobian:\n",
    "        temp += j.view(-1).sum()\n",
    "    \n",
    "    # compute loss with params, sample, and target.\n",
    "    predictions = fmodel(params, buffers, sample)\n",
    "    loss = criterion(predictions, target) + temp\n",
    "\n",
    "    # the result is the per-batch jacobian. You can do whatever you want can \n",
    "    # (i.e., loss += f(jacobian)) and call loss.backward()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Note that now the gradient information are updated in \"params\"\n",
    "    # You have to put params back to the model\n",
    "    assign_params(model_target_only,params)\n",
    "    \n",
    "    # Then you can call the optimizer.\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes\n",
    "\n",
    "Note that the key idea of functorch and stateless model is to make fmodel as a pure function, with input (params, sample). I think you should use fmodel and params to compute the grad (w.r.t. your new loss) first and then update the resulting params to original model (via function assign_params) before calling the optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLT",
   "language": "python",
   "name": "flt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
